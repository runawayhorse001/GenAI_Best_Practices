%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,11pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{2}



\title{GenAI: Best Practices}
\date{December 12, 2024}
\release{1.0}
\author{Wenqiang Feng and Di Zhen}
\newcommand{\sphinxlogo}{\sphinxincludegraphics{logo.png}\par}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}\phantomsection\label{\detokenize{index:index}}\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{logo}.png}
\end{figure}
\end{quote}

\sphinxAtStartPar
Welcome to our \sphinxstylestrong{GenAI: Best Practices}!!! The PDF version
can be downloaded from \sphinxhref{../latex/GenAI.pdf}{HERE}.



\sphinxstepscope


\chapter{Preface}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
\begin{sphinxadmonition}{note}{Chinese proverb}

\sphinxAtStartPar
Good tools are prerequisite to the successful execution of a job. \textendash{} old Chinese proverb
\end{sphinxadmonition}


\section{About}
\label{\detokenize{preface:about}}

\subsection{About this book}
\label{\detokenize{preface:about-this-book}}
\sphinxAtStartPar
This is the book for our Generative AI: Best practics \sphinxcite{reference:autofeatures} API.
The PDF version can be downloaded from \sphinxhref{../latex/GenAI.pdf}{HERE}. \sphinxstylestrong{You may download and distribute it. Please beaware,
however, that the note contains typos as well as inaccurate or incorrect description.}

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{API}} assumes that the reader has a preliminary knowledge of \sphinxcode{\sphinxupquote{python}} programing and \sphinxcode{\sphinxupquote{Linux}}. And this
document is generated automatically by using \sphinxhref{http://sphinx.pocoo.org}{sphinx}.


\subsection{About the authors}
\label{\detokenize{preface:about-the-authors}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Wenqiang Feng}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sr. Data Scientist and PhD in Mathematics

\item {} 
\sphinxAtStartPar
University of Tennessee at Knoxville

\item {} 
\sphinxAtStartPar
Webpage: \sphinxurl{http://web.utk.edu/~wfeng1/}

\item {} 
\sphinxAtStartPar
Email: \sphinxhref{mailto:von198@gmail.com}{von198@gmail.com}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Biography}

\sphinxAtStartPar
Wenqiang Feng is Data Scientist within DST’s Applied Analytics Group. Dr. Feng’s responsibilities include providing
DST clients with access to cutting\sphinxhyphen{}edge skills and technologies, including Big Data analytic solutions, advanced
analytic and data enhancement techniques and modeling.

\sphinxAtStartPar
Dr. Feng has deep analytic expertise in data mining, analytic systems, machine learning algorithms, business
intelligence, and applying Big Data tools to strategically solve industry problems in a cross\sphinxhyphen{}functional business.
Before joining DST, Dr. Feng was an IMA Data Science Fellow at The Institute for Mathematics and its
Applications (IMA) at the University of Minnesota. While there, he helped startup companies make marketing
decisions based on deep predictive analytics.

\sphinxAtStartPar
Dr. Feng graduated from University of Tennessee, Knoxville, with Ph.D. in Computational Mathematics and Master’s
degree in Statistics. He also holds Master’s degree in Computational Mathematics from Missouri University of
Science and Technology (MST) and Master’s degree in Applied Mathematics from the University of Science and
Technology of China (USTC).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Declaration}

\sphinxAtStartPar
The work of Wenqiang Feng was supported by the IMA, while working at IMA. However, any opinion, finding,
and conclusions or recommendations expressed in this material are those of the author and do not necessarily
reflect the views of the IMA, UTK and DST.

\sphinxAtStartPar
ChatGPT has been extensively used in the creation of this book. If you notice that your work has not been
cited or has been cited incorrectly, please notify us.

\end{itemize}


\section{Feedback and suggestions}
\label{\detokenize{preface:feedback-and-suggestions}}
\sphinxAtStartPar
Your comments and suggestions are highly appreciated. I am more than happy to receive
corrections, suggestions or feedback through email (Wenqiang Feng: \sphinxhref{mailto:von198@gmail.com}{von198@gmail.com} and Di Zhen: \sphinxhref{mailto:dizhen318@gmail.com}{dizhen318@gmail.com}
) for improvements.

\sphinxstepscope


\chapter{Preliminary}
\label{\detokenize{prelim:preliminary}}\label{\detokenize{prelim:prelim}}\label{\detokenize{prelim::doc}}
\sphinxAtStartPar
In this chapter, we will introduce some math and NLP preliminaries which is highly
used in Generative AI.


\section{Math Preliminary}
\label{\detokenize{prelim:math-preliminary}}

\subsection{Vector}
\label{\detokenize{prelim:vector}}
\sphinxAtStartPar
A vector is a mathematical representation of data characterized by both magnitude and
direction. In this context, each data point is represented as a feature vector, with
each component corresponding to a specific feature or attribute of the data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{gensim}\PYG{n+nn}{.}\PYG{n+nn}{downloader} \PYG{k}{as} \PYG{n+nn}{api}
\PYG{c+c1}{\PYGZsh{} Download pre\PYGZhy{}trained GloVe model}
\PYG{n}{glove\PYGZus{}vectors} \PYG{o}{=} \PYG{n}{api}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{glove\PYGZhy{}twitter\PYGZhy{}25}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get word vectors (embeddings)}
\PYG{n}{word1} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{king}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{word2} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{queen}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} embedding}
\PYG{n}{king} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word1}\PYG{p}{]}
\PYG{n}{queen} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word2}\PYG{p}{]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{king}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{queen}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{king}\PYG{p}{:}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.74501}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11992}   \PYG{l+m+mf}{0.37329}   \PYG{l+m+mf}{0.36847}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4472}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2288}    \PYG{l+m+mf}{0.70118}
\PYG{l+m+mf}{0.82872}   \PYG{l+m+mf}{0.39486}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.58347}   \PYG{l+m+mf}{0.41488}   \PYG{l+m+mf}{0.37074}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.6906}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20101}
\PYG{l+m+mf}{0.11472}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34661}   \PYG{l+m+mf}{0.36208}   \PYG{l+m+mf}{0.095679} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01765}   \PYG{l+m+mf}{0.68498}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.049013}
\PYG{l+m+mf}{0.54049}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21005}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65397}   \PYG{l+m+mf}{0.64556} \PYG{p}{]}
\PYG{n}{queen}\PYG{p}{:}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.1266}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52064}   \PYG{l+m+mf}{0.45565}   \PYG{l+m+mf}{0.21079}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05081}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65158}   \PYG{l+m+mf}{1.1395}
\PYG{l+m+mf}{0.69897}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20612}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.71803}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02811}   \PYG{l+m+mf}{0.10977}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.3089}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.49299}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51375}   \PYG{l+m+mf}{0.10363}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11764}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.084972}  \PYG{l+m+mf}{0.02558}   \PYG{l+m+mf}{0.6859}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.29196}
\PYG{l+m+mf}{0.4594}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.39955}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40371}   \PYG{l+m+mf}{0.31828} \PYG{p}{]}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{vector}.png}
\caption{Vector}\label{\detokenize{prelim:id2}}\label{\detokenize{prelim:fig-logo}}\end{figure}


\subsection{Norm}
\label{\detokenize{prelim:norm}}
\sphinxAtStartPar
A norm is a function that maps a vector to a single positive value, representing its
magnitude. Norms are essential for calculating distances between vectors, which play
a crucial role in measuring prediction errors, performing feature selection, and
applying regularization techniques in models.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{1Bauo}.png}
\caption{Geometrical Interpretation of Norm (\sphinxhref{https://math.stackexchange.com/questions/805954/what-does-the-dot-product-of-two-vectors-represent}{source\_1})}\label{\detokenize{prelim:id3}}\label{\detokenize{prelim:id1}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Formula:
\begin{quote}

\sphinxAtStartPar
The \(\displaystyle \ell^p\) norm for \(\vec{v} = (v_1, v_2, \cdots, v_n)\) is
\begin{equation*}
\begin{split}||\vec{v}||_p = \sqrt[p]{|v_1|^p + |v_2|^p + \cdots +|v_n|^p }\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^1\) norm: Sum of absolute values of vector components, often used for feature selection due to its tendency to produce sparse solutions.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} l1 norm}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n+nb}{ord}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{}    max(sum(abs(x), axis=0))}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 13.188952}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^2\) norm: Square root of the sum of squared vector components, the most common norm used in many machine learning algorithms.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} l2 norm}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n+nb}{ord}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 4.3206835}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^\infty\) norm (Maximum norm): The largest absolute value of a vector component.

\end{itemize}


\subsection{Distances}
\label{\detokenize{prelim:distances}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Manhattan Distance (\(\displaystyle \ell^1\) Distance)
\begin{quote}

\sphinxAtStartPar
Also known as taxicab or city block distance, Manhattan distance measures the absolute differences
between the components of two vectors. It represents the distance a point would travel along grid
lines in a Cartesian plane, similar to navigating through city streets.

\sphinxAtStartPar
For two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\), the
Manhattan Distance distance \(d(\vec{u},\vec{v})\) is
\begin{equation*}
\begin{split}d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_1 = |u_1-v_1| + |u_2-v_2|+ \cdots +|u_n-v_n|\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
Euclidean Distance (\(\displaystyle \ell^2\) Distance)
\begin{quote}

\sphinxAtStartPar
Euclidean distance is the most common way to measure the distance between two points (vectors) in space.
It is essentially the straight\sphinxhyphen{}line distance between them, calculated using the Pythagorean theorem.

\sphinxAtStartPar
For two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\), the
Euclidean Distance distance \(d(\vec{u},\vec{v})\) is
\begin{equation*}
\begin{split}d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_2 = \sqrt{(u_1-v_1)^2 + (u_2-v_2)^2+ \cdots +(u_n-v_n)^2}\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
Minkowski Distance (\(\displaystyle \ell^p\) Distance)
\begin{quote}

\sphinxAtStartPar
Minkowski distance is a generalization of both Euclidean and Manhattan distances. It incorporates a parameter,
\(p\), which allows for adjusting the sensitivity of the distance metric.
\end{quote}

\item {} 
\sphinxAtStartPar
Cos Similarity
\begin{quote}

\sphinxAtStartPar
Cosine similarity measures the angle between two vectors rather than their straight\sphinxhyphen{}line distance.
It evaluates the similarity of two vectors by focusing on their orientation rather than their magnitude.
This makes it particularly useful for high\sphinxhyphen{}dimensional data, such as text, where the direction of the
vectors is often more significant than their magnitude.

\sphinxAtStartPar
The Cos similarity for two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\) is
\begin{equation*}
\begin{split}cos(\theta) = \frac{\vec{u}\cdot\vec{v}}{||\vec{u}|| ||\vec{v}||}\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
1 means the vectors point in exactly the same direction (perfect similarity).

\item {} 
\sphinxAtStartPar
0 means they are orthogonal (no similarity).

\item {} 
\sphinxAtStartPar
\sphinxhyphen{}1 means they point in opposite directions (complete dissimilarity).

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compute cosine similarity between the two word vectors}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n}{queen}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{)}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{queen}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 0.92024213}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compute cosine similarity between the two word vectors}
\PYG{n}{similarity} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{n}{word1}\PYG{p}{,} \PYG{n}{word2}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{king}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{queen}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine similarity between }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{similarity}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.74501}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11992}   \PYG{l+m+mf}{0.37329}   \PYG{l+m+mf}{0.36847}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4472}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2288}    \PYG{l+m+mf}{0.70118}
\PYG{l+m+mf}{0.82872}   \PYG{l+m+mf}{0.39486}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.58347}   \PYG{l+m+mf}{0.41488}   \PYG{l+m+mf}{0.37074}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.6906}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20101}
\PYG{l+m+mf}{0.11472}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34661}   \PYG{l+m+mf}{0.36208}   \PYG{l+m+mf}{0.095679} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01765}   \PYG{l+m+mf}{0.68498}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.049013}
\PYG{l+m+mf}{0.54049}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21005}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65397}   \PYG{l+m+mf}{0.64556} \PYG{p}{]}
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.1266}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52064}   \PYG{l+m+mf}{0.45565}   \PYG{l+m+mf}{0.21079}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05081}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65158}   \PYG{l+m+mf}{1.1395}
\PYG{l+m+mf}{0.69897}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20612}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.71803}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02811}   \PYG{l+m+mf}{0.10977}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.3089}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.49299}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51375}   \PYG{l+m+mf}{0.10363}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11764}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.084972}  \PYG{l+m+mf}{0.02558}   \PYG{l+m+mf}{0.6859}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.29196}
\PYG{l+m+mf}{0.4594}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.39955}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40371}   \PYG{l+m+mf}{0.31828} \PYG{p}{]}
\PYG{n}{Cosine} \PYG{n}{similarity} \PYG{n}{between} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{and} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.920242190361023}
\end{sphinxVerbatim}
\end{quote}

\end{itemize}


\section{NLP Preliminary}
\label{\detokenize{prelim:nlp-preliminary}}

\subsection{Vocabulary}
\label{\detokenize{prelim:vocabulary}}
\sphinxAtStartPar
In Natural Language Processing (NLP), \sphinxstylestrong{vocabulary} refers to the complete set of unique words or tokens
that a model recognizes or works with during training and inference. Vocabulary plays a critical role in
text processing and understanding, as it defines the scope of linguistic units a model can handle.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Types of Vocabulary in NLP
\begin{quote}

\sphinxAtStartPar
1. \sphinxstylestrong{Word\sphinxhyphen{}level Vocabulary}:
\sphinxhyphen{} Each word in the text is treated as a unique token.
\sphinxhyphen{} For example, the sentence “I love NLP” would generate the vocabulary: \sphinxcode{\sphinxupquote{\{I, love, NLP\}}}.

\sphinxAtStartPar
2. \sphinxstylestrong{Subword\sphinxhyphen{}level Vocabulary}:
\sphinxhyphen{} Text is broken down into smaller units like prefixes, suffixes, or character sequences.
\sphinxhyphen{} For example, the word “loving” might be split into \sphinxcode{\sphinxupquote{\{lov, ing\}}} using techniques like Byte Pair Encoding (BPE) or SentencePiece.
\sphinxhyphen{} Subword vocabularies handle rare or unseen words more effectively.

\sphinxAtStartPar
3. \sphinxstylestrong{Character\sphinxhyphen{}level Vocabulary}:
\sphinxhyphen{} Each character is treated as a token.
\sphinxhyphen{} For example, the word “love” would generate the vocabulary: \sphinxcode{\sphinxupquote{\{l, o, v, e\}}}.
\end{quote}

\item {} 
\sphinxAtStartPar
Importance of Vocabulary
\begin{quote}

\sphinxAtStartPar
1. \sphinxstylestrong{Text Representation}:
\sphinxhyphen{} Vocabulary is the basis for converting text into numerical representations like one\sphinxhyphen{}hot vectors, embeddings, or input IDs for machine learning models.

\sphinxAtStartPar
2. \sphinxstylestrong{Model Efficiency}:
\sphinxhyphen{} A larger vocabulary increases the model’s memory and computational requirements.
\sphinxhyphen{} A smaller vocabulary may lack the capacity to represent all words effectively, leading to a loss of meaning.

\sphinxAtStartPar
3. \sphinxstylestrong{Handling Out\sphinxhyphen{}of\sphinxhyphen{}Vocabulary (OOV) Words}:
\sphinxhyphen{} Words not present in the vocabulary are either replaced with a special token like \sphinxcode{\sphinxupquote{\textless{}UNK\textgreater{}}} or processed using subword/character\sphinxhyphen{}based techniques.
\end{quote}

\item {} 
\sphinxAtStartPar
Building a Vocabulary
\begin{quote}

\sphinxAtStartPar
Common practices include:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Tokenizing the text into words, subwords, or characters.

\item {} 
\sphinxAtStartPar
Counting the frequency of tokens.

\item {} 
\sphinxAtStartPar
Keeping only the most frequent tokens up to a predefined size (e.g., top 50,000 tokens).

\item {} 
\sphinxAtStartPar
Adding special tokens like \sphinxcode{\sphinxupquote{\textless{}PAD\textgreater{}}}, \sphinxcode{\sphinxupquote{\textless{}UNK\textgreater{}}}, \sphinxcode{\sphinxupquote{\textless{}BOS\textgreater{}}} (beginning of sentence), and \sphinxcode{\sphinxupquote{\textless{}EOS\textgreater{}}} (end of sentence).

\end{enumerate}
\end{quote}

\item {} 
\sphinxAtStartPar
Challenges

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Balancing Vocabulary Size}:
A larger vocabulary increases the richness of representation but requires more computational resources.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Domain\sphinxhyphen{}specific Vocabularies}:
In specialized fields like medicine or law, standard vocabularies may not be sufficient, requiring domain\sphinxhyphen{}specific tokenization strategies.

\end{itemize}


\subsection{Tagging}
\label{\detokenize{prelim:tagging}}
\sphinxAtStartPar
Tagging in NLP refers to the process of assigning labels or annotations
to words, phrases, or other linguistic units in a text. These labels provide additional information about
the syntactic, semantic, or structural role of the elements in the text.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Types of Tagging
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Part\sphinxhyphen{}of\sphinxhyphen{}Speech (POS) Tagging}:
\sphinxhyphen{} Assigns grammatical tags (e.g., noun, verb, adjective) to each word in a sentence.
\sphinxhyphen{} Example: For the sentence “The dog barks,” the tags might be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{The/DET}} (Determiner)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{dog/NOUN}} (Noun)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{barks/VERB}} (Verb).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Named Entity Recognition (NER) Tagging}:
\sphinxhyphen{} Identifies and classifies named entities in a text, such as names of people, organizations, locations, dates, or monetary values.
\sphinxhyphen{} Example: In the sentence “John works at Google in California,” the tags might be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{John/PERSON}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Google/ORGANIZATION}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{California/LOCATION}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chunking (Syntactic Tagging)}:
\sphinxhyphen{} Groups words into syntactic chunks like noun phrases (NP) or verb phrases (VP).
\sphinxhyphen{} Example: For the sentence “The quick brown fox jumps,” a chunking result might be:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{{[}NP The quick brown fox{]} {[}VP jumps{]}}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sentiment Tagging}:
\sphinxhyphen{} Assigns sentiment labels (e.g., positive, negative, neutral) to words, phrases, or entire documents.
\sphinxhyphen{} Example: The word “happy” might be tagged as \sphinxcode{\sphinxupquote{positive}}, while “sad” might be tagged as \sphinxcode{\sphinxupquote{negative}}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dependency Parsing Tags}:
\sphinxhyphen{} Identifies the grammatical relationships between words in a sentence, such as subject, object, or modifier.
\sphinxhyphen{} Example: In “She enjoys cooking,” the tags might show:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{She/nsubj}} (nominal subject)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{enjoys/ROOT}} (root of the sentence)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{cooking/dobj}} (direct object).

\end{itemize}

\end{enumerate}

\item {} 
\sphinxAtStartPar
Importance of Tagging
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Understanding Language Structure}: Tags help NLP models understand the grammatical and syntactic structure of text.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Improving Downstream Tasks}: Tagging is foundational for tasks like machine
translation, sentiment analysis, question answering, and summarization.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Feature Engineering}: Tags serve as features for training machine learning models in
text classification or sequence labeling tasks.

\end{itemize}

\item {} 
\sphinxAtStartPar
Tagging Techniques
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rule\sphinxhyphen{}based Tagging}: Relies on predefined linguistic rules to assign tags.
Example: Using dictionaries or regular expressions to match specific patterns.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistical Tagging}: Uses probabilistic models like Hidden Markov Models (HMMs)
to predict tags based on word sequences.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Neural Network\sphinxhyphen{}based Tagging}: Employs deep learning models like LSTMs, GRUs, or Transformers
to tag text with high accuracy.

\end{enumerate}

\item {} 
\sphinxAtStartPar
Challenges
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ambiguity}:Words with multiple meanings can lead to incorrect tagging.
Example: The word “bank” could mean a financial institution or a riverbank.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Domain\sphinxhyphen{}Specific Language}: General tagging models may fail to perform well on specialized text
like medical or legal documents.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data Sparsity}: Rare words or phrases may lack sufficient training data for accurate tagging.

\end{itemize}

\end{itemize}


\subsection{Lemmatization}
\label{\detokenize{prelim:lemmatization}}
\sphinxAtStartPar
Lemmatization in NLP is the process of reducing a word to its base or dictionary form, known as
the \sphinxstylestrong{lemma}. Unlike stemming, which simply removes word suffixes, lemmatization considers
the context and grammatical role of the word to produce a linguistically accurate root form.
\begin{itemize}
\item {} 
\sphinxAtStartPar
How Lemmatization Works
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Contextual Analysis}:
\sphinxhyphen{} Lemmatization relies on a vocabulary (lexicon) and morphological analysis to identify a word’s base form.
\sphinxhyphen{} For example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{running}} \(\rightarrow\) \sphinxcode{\sphinxupquote{run}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{better}} \(\rightarrow\) \sphinxcode{\sphinxupquote{good}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Part\sphinxhyphen{}of\sphinxhyphen{}Speech (POS) Tagging}:
\sphinxhyphen{} The process uses POS tags to determine the correct lemma for a word.
\sphinxhyphen{} Example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{barking}} (verb) \(\rightarrow\) \sphinxcode{\sphinxupquote{bark}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{barking}} (adjective, as in “barking dog”) \(\rightarrow\) \sphinxcode{\sphinxupquote{barking}}.

\end{itemize}

\end{enumerate}

\item {} 
\sphinxAtStartPar
Importance of Lemmatization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Improves Text Normalization}:
\sphinxhyphen{} Lemmatization helps normalize text by grouping different forms of a word into a single representation.
\sphinxhyphen{} Example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{run}}, \sphinxcode{\sphinxupquote{running}}, and \sphinxcode{\sphinxupquote{ran}} \(\rightarrow\) \sphinxcode{\sphinxupquote{run}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Enhances NLP Applications}:
\sphinxhyphen{} Lemmatized text improves the performance of tasks like information retrieval, text classification, and sentiment analysis.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reduces Vocabulary Size}:
\sphinxhyphen{} By mapping inflected forms to their base form, lemmatization reduces redundancy in text, resulting in a smaller vocabulary.

\end{enumerate}

\item {} 
\sphinxAtStartPar
Lemmatization vs. Stemming
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Lemmatization}:
\sphinxhyphen{} Produces linguistically accurate root forms.
\sphinxhyphen{} Considers the word’s context and POS.
\sphinxhyphen{} Example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{studies}} \(\rightarrow\) \sphinxcode{\sphinxupquote{study}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Stemming}:
\sphinxhyphen{} Applies heuristic rules to strip word suffixes without considering context.
\sphinxhyphen{} May produce non\sphinxhyphen{}dictionary forms.
\sphinxhyphen{} Example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{studies}} \(\rightarrow\) \sphinxcode{\sphinxupquote{studi}}.

\end{itemize}

\end{itemize}

\item {} 
\sphinxAtStartPar
Techniques for Lemmatization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rule\sphinxhyphen{}Based Lemmatization}:
\sphinxhyphen{} Relies on predefined linguistic rules and dictionaries.
\sphinxhyphen{} Example: WordNet\sphinxhyphen{}based lemmatizers.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistical Lemmatization}:
\sphinxhyphen{} Uses probabilistic models to predict lemmas based on the context.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep Learning\sphinxhyphen{}Based Lemmatization}:
\sphinxhyphen{} Employs neural networks and sequence\sphinxhyphen{}to\sphinxhyphen{}sequence models for highly accurate lemmatization in complex contexts.

\end{enumerate}

\item {} 
\sphinxAtStartPar
Challenges
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ambiguity}:
Words with multiple meanings may result in incorrect lemmatization without proper context.
\sphinxhyphen{} Example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{left}} (verb) \(\rightarrow\) \sphinxcode{\sphinxupquote{leave}}

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{left}} (noun/adjective) \(\rightarrow\) \sphinxcode{\sphinxupquote{left}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Language\sphinxhyphen{}Specific Complexity}:
Lemmatization rules vary widely across languages, requiring language\sphinxhyphen{}specific tools and resources.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Dependency}:
Lemmatizers require extensive lexicons and morphological rules, which can be resource\sphinxhyphen{}intensive to develop.

\end{itemize}

\end{itemize}


\subsection{Tokenization}
\label{\detokenize{prelim:tokenization}}
\sphinxAtStartPar
Tokenization in NLP refers to the process of splitting a text into smaller units, called \sphinxstylestrong{tokens}, which
can be words, subwords, sentences, or characters. These tokens serve as the basic building blocks for further
analysis in NLP tasks.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Types of Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Word Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Splits the text into individual words or terms.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Sentence: “I love NLP.”

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"I", "love", "NLP"{]}}}.

\end{itemize}

\end{description}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sentence Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Divides a text into sentences.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Text: “I love NLP. It’s amazing.”

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"I love NLP.", "It’s amazing."{]}}}.

\end{itemize}

\end{description}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Subword Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Breaks words into smaller units, often using methods like Byte Pair Encoding (BPE) or SentencePiece.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Word: \sphinxcode{\sphinxupquote{unhappiness}}.

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"un", "happiness"{]}}} (or subword units like \sphinxcode{\sphinxupquote{{[}"un", "happi", "ness"{]}}}).

\end{itemize}

\end{description}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{3}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Character Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Treats each character in a word as a separate token.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Word: \sphinxcode{\sphinxupquote{hello}}.

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"h", "e", "l", "l", "o"{]}}}.

\end{itemize}

\end{description}

\end{itemize}

\item {} 
\sphinxAtStartPar
Importance of Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Text Preprocessing}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tokenization is the first step in many NLP tasks like text classification, translation, and
summarization, as it converts text into manageable pieces.

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Text Representation}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tokens are converted into numerical representations (e.g., word embeddings) for model input
in tasks like sentiment analysis, named entity recognition (NER), or language modeling.

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Improving Accuracy}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Proper tokenization ensures that a model processes text at the correct granularity (e.g.,
words or subwords), improving accuracy for tasks like machine translation or text generation.

\end{itemize}

\item {} 
\sphinxAtStartPar
Challenges of Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ambiguity}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Certain words or phrases can be tokenized differently based on context.

\item {} 
\sphinxAtStartPar
Example: “New York” can be treated as one token (location) or two separate tokens (\sphinxcode{\sphinxupquote{{[}"New", "York"{]}}}).

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Handling Punctuation}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Deciding how to treat punctuation marks can be challenging. For example, should commas, periods,
or quotes be treated as separate tokens or grouped with adjacent words?

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}word Expressions (MWEs)}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Some expressions consist of multiple words that should be treated as a single token, such as “New York” or “machine learning.”

\end{itemize}

\item {} 
\sphinxAtStartPar
Techniques for Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rule\sphinxhyphen{}Based Tokenization}: Uses predefined rules to split text based on spaces, punctuation, and other delimiters.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistical and Machine Learning\sphinxhyphen{}Based Tokenization}: Uses trained models to predict token boundaries based on patterns learned from large corpora.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep Learning\sphinxhyphen{}Based Tokenization}: Modern tokenization models, such as those used in transformers (e.g., BERT, GPT), may rely on subword tokenization and neural networks to handle complex tokenization tasks.

\end{enumerate}

\end{itemize}


\section{Platform and Packages}
\label{\detokenize{prelim:platform-and-packages}}

\subsection{Google Colab}
\label{\detokenize{prelim:google-colab}}
\sphinxAtStartPar
\sphinxstylestrong{Google Colab} (short for Colaboratory) is a free, cloud\sphinxhyphen{}based platform that provides users with the ability to write
and execute Python code in an interactive notebook environment. It is based on Jupyter notebooks and is powered by
Google Cloud services, allowing for seamless integration with Google Drive and other Google services. We will primarily
use Google Colab with free T4 GPU runtime throughout this book.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Key Features

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Free Access to GPUs and TPUs}
Colab offers free access to Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), making it an ideal environment for machine learning, deep learning, and other computationally intensive tasks.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Integration with Google Drive}
You can store and access notebooks directly from your Google Drive, making it easy to collaborate with others and keep your projects organized.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{No Setup Required}
Since Colab is entirely cloud\sphinxhyphen{}based, you don’t need to worry about setting up an environment or managing dependencies. Everything is ready to go out of the box.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Support for Python Libraries}
Colab comes pre\sphinxhyphen{}installed with many popular Python libraries, including TensorFlow, PyTorch, Keras, and OpenCV, among others. You can also install any additional libraries using \sphinxtitleref{pip}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Collaborative Features}
Multiple users can work on the same notebook simultaneously, making it ideal for collaboration. Changes are synchronized in real\sphinxhyphen{}time.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rich Media Support}
Colab supports the inclusion of rich media, such as images, videos, and LaTeX equations, directly within the notebook. This makes it a great tool for data analysis, visualization, and educational purposes.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Easy Sharing}
Notebooks can be easily shared with others via a shareable link, just like Google Docs. Permissions can be set for viewing or editing the document.

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
GPU Activation
\sphinxcode{\sphinxupquote{Runtime \sphinxhyphen{}\sphinxhyphen{}\textgreater{} change runtime type \sphinxhyphen{}\sphinxhyphen{}\textgreater{} T4/A100 GPU}}

\end{itemize}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\sphinxthistablewithborderlessstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook
\noindent\sphinxincludegraphics[width=1.000\linewidth]{{runtime}.png}
&
\noindent\sphinxincludegraphics[width=1.000\linewidth]{{T4}.png}
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{HuggingFace}
\label{\detokenize{prelim:huggingface}}

\subsection{Ollama}
\label{\detokenize{prelim:ollama}}

\subsection{langchain}
\label{\detokenize{prelim:langchain}}
\sphinxstepscope


\chapter{Word and Sentence Embedding}
\label{\detokenize{embedding:word-and-sentence-embedding}}\label{\detokenize{embedding:embedding}}\label{\detokenize{embedding::doc}}
\sphinxAtStartPar
Word embedding is a method in natural language processing (NLP) to represent words as dense
vectors of real numbers, capturing semantic relationships between them. Instead of treating
words as discrete symbols (like one\sphinxhyphen{}hot encoding), word embeddings map words into a
continuous vector space where similar words are located closer together.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{embedding_diagram}.png}
\caption{Embedding Diagram}\label{\detokenize{embedding:id1}}\label{\detokenize{embedding:fig-embedding}}\end{figure}


\section{Bag\sphinxhyphen{}of\sphinxhyphen{}Word}
\label{\detokenize{embedding:bag-of-word}}
\sphinxAtStartPar
\sphinxstylestrong{Bag of Words (BoW)} is a simple and widely used text representation technique in natural language processing (NLP). It represents a text (e.g., a document or a sentence) as a collection of words, ignoring grammar, order, and context but keeping their frequency.

\sphinxAtStartPar
Key Features of Bag of Words:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Vocabulary Creation}:
\sphinxhyphen{} A list of all unique words in the dataset (the “vocabulary”) is created.
\sphinxhyphen{} Each word becomes a feature.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Representation}:
\sphinxhyphen{} Each document is represented as a vector or a frequency count of words from the vocabulary.
\sphinxhyphen{} If a word from the vocabulary is present in the document, its count is included in the vector.
\sphinxhyphen{} Words not present in the document are assigned a count of zero.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simplicity}:
\sphinxhyphen{} The method is computationally efficient and straightforward.
\sphinxhyphen{} However, it ignores the sequence and semantic meaning of the words.

\end{enumerate}

\sphinxAtStartPar
Applications:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Text Classification

\item {} 
\sphinxAtStartPar
Sentiment Analysis

\item {} 
\sphinxAtStartPar
Document Similarity

\end{itemize}

\sphinxAtStartPar
Limitations:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Context Ignorance}:
\sphinxhyphen{} BoW does not capture word order or semantics.
\sphinxhyphen{} For example, “not good” and “good” might appear similar in BoW.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dimensionality}:
\sphinxhyphen{} As the vocabulary size increases, the vector representation grows, leading to high\sphinxhyphen{}dimensional data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sparse Representations}:
\sphinxhyphen{} Many entries in the vectors might be zeros, leading to sparsity.

\end{enumerate}


\subsection{One Hot Encoder}
\label{\detokenize{embedding:one-hot-encoder}}

\subsection{CountVectorizer}
\label{\detokenize{embedding:countvectorizer}}
\sphinxAtStartPar
To overcome these limitations, advanced techniques like \sphinxstylestrong{TF\sphinxhyphen{}IDF}, \sphinxstylestrong{word embeddings} (e.g., Word2Vec, GloVe), and contextual embeddings (e.g., BERT) are often used.


\section{TF\sphinxhyphen{}IDF}
\label{\detokenize{embedding:tf-idf}}

\section{Word2Vec}
\label{\detokenize{embedding:word2vec}}

\section{GloVE}
\label{\detokenize{embedding:glove}}

\section{Fast Text}
\label{\detokenize{embedding:fast-text}}

\section{BERT}
\label{\detokenize{embedding:bert}}
\sphinxstepscope


\chapter{Prompt Engineering}
\label{\detokenize{prompt:prompt-engineering}}\label{\detokenize{prompt:prompt}}\label{\detokenize{prompt::doc}}

\section{Background about LLM and Prompt}
\label{\detokenize{prompt:background-about-llm-and-prompt}}

\section{Prompt Engineering Basics}
\label{\detokenize{prompt:prompt-engineering-basics}}

\subsection{Prompt Components}
\label{\detokenize{prompt:prompt-components}}

\subsection{Prompt Engineering Principles}
\label{\detokenize{prompt:prompt-engineering-principles}}

\section{Advanced Prompt Engineering}
\label{\detokenize{prompt:advanced-prompt-engineering}}
\sphinxstepscope


\chapter{Retrieval\sphinxhyphen{}Augmented Generation}
\label{\detokenize{rag:retrieval-augmented-generation}}\label{\detokenize{rag:rag}}\label{\detokenize{rag::doc}}

\section{Overview}
\label{\detokenize{rag:overview}}

\section{Indexing}
\label{\detokenize{rag:indexing}}

\section{Retrieval}
\label{\detokenize{rag:retrieval}}

\section{Generation}
\label{\detokenize{rag:generation}}
\sphinxstepscope


\chapter{Fine Tuning}
\label{\detokenize{finetuning:fine-tuning}}\label{\detokenize{finetuning:finetuning}}\label{\detokenize{finetuning::doc}}
\sphinxstepscope


\chapter{Pre\sphinxhyphen{}training}
\label{\detokenize{pretraining:pre-training}}\label{\detokenize{pretraining:pretraining}}\label{\detokenize{pretraining::doc}}
\sphinxstepscope


\chapter{LLM Evaluation Metrics}
\label{\detokenize{evaluation:llm-evaluation-metrics}}\label{\detokenize{evaluation:evaluation}}\label{\detokenize{evaluation::doc}}

\section{Statistical Scorers}
\label{\detokenize{evaluation:statistical-scorers}}

\section{Model\sphinxhyphen{}Based Scorers}
\label{\detokenize{evaluation:model-based-scorers}}
\sphinxstepscope


\chapter{Main Reference}
\label{\detokenize{reference:main-reference}}\label{\detokenize{reference:reference}}\label{\detokenize{reference::doc}}
\begin{sphinxthebibliography}{AutoFeat}
\bibitem[AutoFeatures]{reference:autofeatures}
\sphinxAtStartPar
Wenqiang Feng and Ming Chen.
\sphinxhref{https://runawayhorse001.github.io/AutoFeatures/}{Python Data Audit Library API}, 2019.
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}