%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,11pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{2}



\title{GenAI: Best Practices}
\date{December 11, 2024}
\release{1.0}
\author{Wenqiang Feng and Di Zhen}
\newcommand{\sphinxlogo}{\sphinxincludegraphics{logo.png}\par}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}\phantomsection\label{\detokenize{index:index}}\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{logo}.png}
\end{figure}
\end{quote}

\sphinxAtStartPar
Welcome to our \sphinxstylestrong{GenAI: Best Practices}!!! The PDF version
can be downloaded from \sphinxhref{../latex/GenAI.pdf}{HERE}.



\sphinxstepscope


\chapter{Preface}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
\begin{sphinxadmonition}{note}{Chinese proverb}

\sphinxAtStartPar
Good tools are prerequisite to the successful execution of a job. \textendash{} old Chinese proverb
\end{sphinxadmonition}


\section{About}
\label{\detokenize{preface:about}}

\subsection{About this book}
\label{\detokenize{preface:about-this-book}}
\sphinxAtStartPar
This is the book for our Generative AI: Best practics \sphinxcite{reference:autofeatures} API.
The PDF version can be downloaded from \sphinxhref{../latex/GenAI.pdf}{HERE}. \sphinxstylestrong{You may download and distribute it. Please beaware,
however, that the note contains typos as well as inaccurate or incorrect description.}

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{API}} assumes that the reader has a preliminary knowledge of \sphinxcode{\sphinxupquote{python}} programing and \sphinxcode{\sphinxupquote{Linux}}. And this
document is generated automatically by using \sphinxhref{http://sphinx.pocoo.org}{sphinx}.


\subsection{About the authors}
\label{\detokenize{preface:about-the-authors}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Wenqiang Feng}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sr. Data Scientist and PhD in Mathematics

\item {} 
\sphinxAtStartPar
University of Tennessee at Knoxville

\item {} 
\sphinxAtStartPar
Webpage: \sphinxurl{http://web.utk.edu/~wfeng1/}

\item {} 
\sphinxAtStartPar
Email: \sphinxhref{mailto:von198@gmail.com}{von198@gmail.com}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Biography}

\sphinxAtStartPar
Wenqiang Feng is Data Scientist within DST’s Applied Analytics Group. Dr. Feng’s responsibilities include providing
DST clients with access to cutting\sphinxhyphen{}edge skills and technologies, including Big Data analytic solutions, advanced
analytic and data enhancement techniques and modeling.

\sphinxAtStartPar
Dr. Feng has deep analytic expertise in data mining, analytic systems, machine learning algorithms, business
intelligence, and applying Big Data tools to strategically solve industry problems in a cross\sphinxhyphen{}functional business.
Before joining DST, Dr. Feng was an IMA Data Science Fellow at The Institute for Mathematics and its
Applications (IMA) at the University of Minnesota. While there, he helped startup companies make marketing
decisions based on deep predictive analytics.

\sphinxAtStartPar
Dr. Feng graduated from University of Tennessee, Knoxville, with Ph.D. in Computational Mathematics and Master’s
degree in Statistics. He also holds Master’s degree in Computational Mathematics from Missouri University of
Science and Technology (MST) and Master’s degree in Applied Mathematics from the University of Science and
Technology of China (USTC).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Declaration}

\sphinxAtStartPar
The work of Wenqiang Feng was supported by the IMA, while working at IMA. However, any opinion, finding,
and conclusions or recommendations expressed in this material are those of the author and do not necessarily
reflect the views of the IMA, UTK and DST.

\end{itemize}


\section{Feedback and suggestions}
\label{\detokenize{preface:feedback-and-suggestions}}
\sphinxAtStartPar
Your comments and suggestions are highly appreciated. I am more than happy to receive
corrections, suggestions or feedback through email (Wenqiang Feng: \sphinxhref{mailto:von198@gmail.com}{von198@gmail.com} and Di Zhen: \sphinxhref{mailto:dizhen318@gmail.com}{dizhen318@gmail.com}
) for improvements.

\sphinxstepscope


\chapter{Preliminary}
\label{\detokenize{prelim:preliminary}}\label{\detokenize{prelim:prelim}}\label{\detokenize{prelim::doc}}
\sphinxAtStartPar
In this chapter, we will introduce some math and NLP preliminaries which is highly
used in Generative AI.


\section{Math Preliminary}
\label{\detokenize{prelim:math-preliminary}}

\subsection{Vector}
\label{\detokenize{prelim:vector}}
\sphinxAtStartPar
A vector is a mathematical representation of data that has both magnitude and direction.
Each data point is represented as a feature vector, where each component of the vector
corresponds to a specific feature or attribute of the data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{gensim}\PYG{n+nn}{.}\PYG{n+nn}{downloader} \PYG{k}{as} \PYG{n+nn}{api}
\PYG{c+c1}{\PYGZsh{} Download pre\PYGZhy{}trained GloVe model}
\PYG{n}{glove\PYGZus{}vectors} \PYG{o}{=} \PYG{n}{api}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{glove\PYGZhy{}twitter\PYGZhy{}25}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get word vectors (embeddings)}
\PYG{n}{word1} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{king}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{word2} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{queen}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} embedding}
\PYG{n}{king} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word1}\PYG{p}{]}
\PYG{n}{queen} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word2}\PYG{p}{]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{king}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{queen}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{king}\PYG{p}{:}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.74501}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11992}   \PYG{l+m+mf}{0.37329}   \PYG{l+m+mf}{0.36847}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4472}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2288}    \PYG{l+m+mf}{0.70118}
\PYG{l+m+mf}{0.82872}   \PYG{l+m+mf}{0.39486}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.58347}   \PYG{l+m+mf}{0.41488}   \PYG{l+m+mf}{0.37074}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.6906}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20101}
\PYG{l+m+mf}{0.11472}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34661}   \PYG{l+m+mf}{0.36208}   \PYG{l+m+mf}{0.095679} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01765}   \PYG{l+m+mf}{0.68498}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.049013}
\PYG{l+m+mf}{0.54049}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21005}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65397}   \PYG{l+m+mf}{0.64556} \PYG{p}{]}
\PYG{n}{queen}\PYG{p}{:}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.1266}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52064}   \PYG{l+m+mf}{0.45565}   \PYG{l+m+mf}{0.21079}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05081}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65158}   \PYG{l+m+mf}{1.1395}
\PYG{l+m+mf}{0.69897}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20612}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.71803}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02811}   \PYG{l+m+mf}{0.10977}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.3089}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.49299}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51375}   \PYG{l+m+mf}{0.10363}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11764}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.084972}  \PYG{l+m+mf}{0.02558}   \PYG{l+m+mf}{0.6859}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.29196}
\PYG{l+m+mf}{0.4594}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.39955}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40371}   \PYG{l+m+mf}{0.31828} \PYG{p}{]}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{vector}.png}
\caption{Vector}\label{\detokenize{prelim:id2}}\label{\detokenize{prelim:fig-logo}}\end{figure}


\subsection{Norm}
\label{\detokenize{prelim:norm}}
\sphinxAtStartPar
Norm is a function that maps a vector to a single positive value, representing its
magnitude. Norms are used to calculate distances between vectors, which is vital
for measuring prediction errors in models, performing feature
selection, and applying regularization techniques.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{1Bauo}.png}
\caption{Geometrical Interpretation of Norm (\sphinxhref{https://math.stackexchange.com/questions/805954/what-does-the-dot-product-of-two-vectors-represent}{source\_1})}\label{\detokenize{prelim:id3}}\label{\detokenize{prelim:id1}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Formula:
\begin{quote}

\sphinxAtStartPar
The \(\displaystyle \ell^p\) norm for \(\vec{v} = (v_1, v_2, \cdots, v_n)\) is
\begin{equation*}
\begin{split}||\vec{v}||_p = \sqrt[p]{|v_1|^p + |v_2|^p + \cdots +|v_n|^p }\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^1\) norm: Sum of absolute values of vector components, often used for feature selection due to its tendency to produce sparse solutions.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} l1 norm}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n+nb}{ord}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{}    max(sum(abs(x), axis=0))}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 13.188952}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^2\) norm: Square root of the sum of squared vector components, the most common norm used in many machine learning algorithms.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} l2 norm}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n+nb}{ord}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 4.3206835}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^\infty\) norm (Maximum norm): The largest absolute value of a vector component.

\end{itemize}


\subsection{Distances}
\label{\detokenize{prelim:distances}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Manhattan Distance (\(\displaystyle \ell^1\) Distance)
\begin{quote}

\sphinxAtStartPar
Also known as taxicab or city block distance, Manhattan distance measures the absolute differences between the components of two vectors. It calculates the distance a point would travel along the grid lines in a Cartesian plane, as if navigating through a city.

\sphinxAtStartPar
For two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\), the
Manhattan Distance distance \(d(\vec{u},\vec{v})\) is
\begin{equation*}
\begin{split}d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_1 = |u_1-v_1| + |u_2-v_2|+ \cdots +|u_n-v_n|\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
Euclidean Distance (\(\displaystyle \ell^2\) Distance)
\begin{quote}

\sphinxAtStartPar
Euclidean distance is the most common way to measure the distance between two points (vectors) in space. It is essentially the straight\sphinxhyphen{}line distance between them, calculated using the Pythagorean theorem.

\sphinxAtStartPar
For two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\), the Euclidean Distance distance \(d(\vec{u},\vec{v})\) is
\begin{equation*}
\begin{split}d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_2 = \sqrt{(u_1-v_1)^2 + (u_2-v_2)^2+ \cdots +(u_n-v_n)^2}\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
Minkowski Distance (\(\displaystyle \ell^p\) Distance)
\begin{quote}

\sphinxAtStartPar
Minkowski distance is a generalization of both Euclidean and Manhattan distances. It introduces a parameter \(p\) that allows you to adjust the sensitivity of the distance metric.
\end{quote}

\item {} 
\sphinxAtStartPar
Cos Similarity
\begin{quote}

\sphinxAtStartPar
Cosine similarity measures the angle between two vectors rather than their straight\sphinxhyphen{}line distance. It is used to determine how similar two vectors are by focusing on their orientation rather than their magnitude. This makes it particularly useful for high\sphinxhyphen{}dimensional data, such as text, where the magnitude of the vectors may not be as important as the direction.

\sphinxAtStartPar
The Cos similarity for two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\) is
\begin{equation*}
\begin{split}cos(\theta) = \frac{\vec{u}\cdot\vec{v}}{||\vec{u}|| ||\vec{v}||}\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
1 means the vectors point in exactly the same direction (perfect similarity).

\item {} 
\sphinxAtStartPar
0 means they are orthogonal (no similarity).

\item {} 
\sphinxAtStartPar
\sphinxhyphen{}1 means they point in opposite directions (complete dissimilarity).

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compute cosine similarity between the two word vectors}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n}{queen}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{)}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{queen}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 0.92024213}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compute cosine similarity between the two word vectors}
\PYG{n}{similarity} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{n}{word1}\PYG{p}{,} \PYG{n}{word2}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{king}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{queen}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine similarity between }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{similarity}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.74501}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11992}   \PYG{l+m+mf}{0.37329}   \PYG{l+m+mf}{0.36847}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4472}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2288}    \PYG{l+m+mf}{0.70118}
\PYG{l+m+mf}{0.82872}   \PYG{l+m+mf}{0.39486}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.58347}   \PYG{l+m+mf}{0.41488}   \PYG{l+m+mf}{0.37074}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.6906}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20101}
\PYG{l+m+mf}{0.11472}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34661}   \PYG{l+m+mf}{0.36208}   \PYG{l+m+mf}{0.095679} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01765}   \PYG{l+m+mf}{0.68498}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.049013}
\PYG{l+m+mf}{0.54049}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21005}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65397}   \PYG{l+m+mf}{0.64556} \PYG{p}{]}
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.1266}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52064}   \PYG{l+m+mf}{0.45565}   \PYG{l+m+mf}{0.21079}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05081}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65158}   \PYG{l+m+mf}{1.1395}
\PYG{l+m+mf}{0.69897}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20612}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.71803}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02811}   \PYG{l+m+mf}{0.10977}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.3089}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.49299}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51375}   \PYG{l+m+mf}{0.10363}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11764}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.084972}  \PYG{l+m+mf}{0.02558}   \PYG{l+m+mf}{0.6859}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.29196}
\PYG{l+m+mf}{0.4594}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.39955}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40371}   \PYG{l+m+mf}{0.31828} \PYG{p}{]}
\PYG{n}{Cosine} \PYG{n}{similarity} \PYG{n}{between} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{and} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.920242190361023}
\end{sphinxVerbatim}
\end{quote}

\end{itemize}


\section{NLP Preliminary}
\label{\detokenize{prelim:nlp-preliminary}}

\subsection{Vocabulary}
\label{\detokenize{prelim:vocabulary}}

\subsection{Tagging}
\label{\detokenize{prelim:tagging}}

\subsection{Lemmatization}
\label{\detokenize{prelim:lemmatization}}

\subsection{Tokenization}
\label{\detokenize{prelim:tokenization}}
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}vocabulary tokens
Subword Tokenization


\section{Platform and Packages}
\label{\detokenize{prelim:platform-and-packages}}
\sphinxstepscope


\chapter{Word and Sentence Embedding}
\label{\detokenize{embedding:word-and-sentence-embedding}}\label{\detokenize{embedding:embedding}}\label{\detokenize{embedding::doc}}
\sphinxAtStartPar
Word embedding is a method in natural language processing (NLP) to represent words as dense
vectors of real numbers, capturing semantic relationships between them. Instead of treating
words as discrete symbols (like one\sphinxhyphen{}hot encoding), word embeddings map words into a
continuous vector space where similar words are located closer together.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{embedding_diagram}.png}
\caption{Embedding Diagram}\label{\detokenize{embedding:id1}}\label{\detokenize{embedding:fig-logo}}\end{figure}


\section{Bag\sphinxhyphen{}of\sphinxhyphen{}Word}
\label{\detokenize{embedding:bag-of-word}}
\sphinxAtStartPar
\sphinxstylestrong{Bag of Words (BoW)} is a simple and widely used text representation technique in natural language processing (NLP). It represents a text (e.g., a document or a sentence) as a collection of words, ignoring grammar, order, and context but keeping their frequency.

\sphinxAtStartPar
Key Features of Bag of Words:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Vocabulary Creation}:
\sphinxhyphen{} A list of all unique words in the dataset (the “vocabulary”) is created.
\sphinxhyphen{} Each word becomes a feature.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Representation}:
\sphinxhyphen{} Each document is represented as a vector or a frequency count of words from the vocabulary.
\sphinxhyphen{} If a word from the vocabulary is present in the document, its count is included in the vector.
\sphinxhyphen{} Words not present in the document are assigned a count of zero.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simplicity}:
\sphinxhyphen{} The method is computationally efficient and straightforward.
\sphinxhyphen{} However, it ignores the sequence and semantic meaning of the words.

\end{enumerate}

\sphinxAtStartPar
Applications:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Text Classification

\item {} 
\sphinxAtStartPar
Sentiment Analysis

\item {} 
\sphinxAtStartPar
Document Similarity

\end{itemize}

\sphinxAtStartPar
Limitations:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Context Ignorance}:
\sphinxhyphen{} BoW does not capture word order or semantics.
\sphinxhyphen{} For example, “not good” and “good” might appear similar in BoW.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dimensionality}:
\sphinxhyphen{} As the vocabulary size increases, the vector representation grows, leading to high\sphinxhyphen{}dimensional data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sparse Representations}:
\sphinxhyphen{} Many entries in the vectors might be zeros, leading to sparsity.

\end{enumerate}


\subsection{One Hot Encoder}
\label{\detokenize{embedding:one-hot-encoder}}

\subsection{CountVectorizer}
\label{\detokenize{embedding:countvectorizer}}
\sphinxAtStartPar
To overcome these limitations, advanced techniques like \sphinxstylestrong{TF\sphinxhyphen{}IDF}, \sphinxstylestrong{word embeddings} (e.g., Word2Vec, GloVe), and contextual embeddings (e.g., BERT) are often used.


\section{TF\sphinxhyphen{}IDF}
\label{\detokenize{embedding:tf-idf}}

\section{Word2Vec}
\label{\detokenize{embedding:word2vec}}

\section{GloVE}
\label{\detokenize{embedding:glove}}

\section{Fast Text}
\label{\detokenize{embedding:fast-text}}

\section{BERT}
\label{\detokenize{embedding:bert}}
\sphinxstepscope


\chapter{Prompt Engineering}
\label{\detokenize{prompt:prompt-engineering}}\label{\detokenize{prompt:prompt}}\label{\detokenize{prompt::doc}}

\section{Background about LLM and Prompt}
\label{\detokenize{prompt:background-about-llm-and-prompt}}

\section{Prompt Engineering Basics}
\label{\detokenize{prompt:prompt-engineering-basics}}

\subsection{Prompt Components}
\label{\detokenize{prompt:prompt-components}}

\subsection{Prompt Engineering Principles}
\label{\detokenize{prompt:prompt-engineering-principles}}

\section{Advanced Prompt Engineering}
\label{\detokenize{prompt:advanced-prompt-engineering}}
\sphinxstepscope


\chapter{Retrieval\sphinxhyphen{}Augmented Generation}
\label{\detokenize{rag:retrieval-augmented-generation}}\label{\detokenize{rag:rag}}\label{\detokenize{rag::doc}}

\section{Overview}
\label{\detokenize{rag:overview}}

\section{Indexing}
\label{\detokenize{rag:indexing}}

\section{Retrieval}
\label{\detokenize{rag:retrieval}}

\section{Generation}
\label{\detokenize{rag:generation}}
\sphinxstepscope


\chapter{Fine Tuning}
\label{\detokenize{finetuning:fine-tuning}}\label{\detokenize{finetuning:finetuning}}\label{\detokenize{finetuning::doc}}
\sphinxstepscope


\chapter{Pre\sphinxhyphen{}training}
\label{\detokenize{pretraining:pre-training}}\label{\detokenize{pretraining:pretraining}}\label{\detokenize{pretraining::doc}}
\sphinxstepscope


\chapter{Main Reference}
\label{\detokenize{reference:main-reference}}\label{\detokenize{reference:reference}}\label{\detokenize{reference::doc}}
\begin{sphinxthebibliography}{AutoFeat}
\bibitem[AutoFeatures]{reference:autofeatures}
\sphinxAtStartPar
Wenqiang Feng and Ming Chen.
\sphinxhref{https://runawayhorse001.github.io/AutoFeatures/}{Python Data Audit Library API}, 2019.
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}