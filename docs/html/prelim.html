

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>2. Preliminary &mdash; GenAI: Best Practices 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />

  
    <link rel="shortcut icon" href="_static/icon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3. Word and Sentence Embedding" href="embedding.html" />
    <link rel="prev" title="1. Preface" href="preface.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            GenAI: Best Practices
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preface.html">1. Preface</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">2. Preliminary</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#math-preliminary">2.1. Math Preliminary</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vector">2.1.1. Vector</a></li>
<li class="toctree-l3"><a class="reference internal" href="#norm">2.1.2. Norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distances">2.1.3. Distances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nlp-preliminary">2.2. NLP Preliminary</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vocabulary">2.2.1. Vocabulary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tagging">2.2.2. Tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lemmatization">2.2.3. Lemmatization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tokenization">2.2.4. Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bert-tokenization">2.2.5. BERT Tokenization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#platform-and-packages">2.3. Platform and Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#google-colab">2.3.1. Google Colab</a></li>
<li class="toctree-l3"><a class="reference internal" href="#huggingface">2.3.2. HuggingFace</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ollama">2.3.3. Ollama</a></li>
<li class="toctree-l3"><a class="reference internal" href="#langchain">2.3.4. langchain</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="embedding.html">3. Word and Sentence Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompt.html">4. Prompt Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">5. Retrieval-Augmented Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">6. Fine Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretraining.html">7. Pre-training</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">8. LLM Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">9. Main Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GenAI: Best Practices</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><span class="section-number">2. </span>Preliminary</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/prelim.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="preliminary">
<span id="prelim"></span><h1><span class="section-number">2. </span>Preliminary<a class="headerlink" href="#preliminary" title="Link to this heading"></a></h1>
<p>In this chapter, we will introduce some math and NLP preliminaries which is highly
used in Generative AI.</p>
<section id="math-preliminary">
<h2><span class="section-number">2.1. </span>Math Preliminary<a class="headerlink" href="#math-preliminary" title="Link to this heading"></a></h2>
<section id="vector">
<h3><span class="section-number">2.1.1. </span>Vector<a class="headerlink" href="#vector" title="Link to this heading"></a></h3>
<p>A vector is a mathematical representation of data characterized by both magnitude and
direction. In this context, each data point is represented as a feature vector, with
each component corresponding to a specific feature or attribute of the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>
<span class="c1"># Download pre-trained GloVe model</span>
<span class="n">glove_vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;glove-twitter-25&quot;</span><span class="p">)</span>

<span class="c1"># Get word vectors (embeddings)</span>
<span class="n">word1</span> <span class="o">=</span> <span class="s2">&quot;king&quot;</span>
<span class="n">word2</span> <span class="o">=</span> <span class="s2">&quot;queen&quot;</span>

<span class="c1"># embedding</span>
<span class="n">king</span> <span class="o">=</span> <span class="n">glove_vectors</span><span class="p">[</span><span class="n">word1</span><span class="p">]</span>
<span class="n">queen</span> <span class="o">=</span> <span class="n">glove_vectors</span><span class="p">[</span><span class="n">word2</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;king:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">king</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;queen:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">queen</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">king</span><span class="p">:</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.74501</span>  <span class="o">-</span><span class="mf">0.11992</span>   <span class="mf">0.37329</span>   <span class="mf">0.36847</span>  <span class="o">-</span><span class="mf">0.4472</span>   <span class="o">-</span><span class="mf">0.2288</span>    <span class="mf">0.70118</span>
<span class="mf">0.82872</span>   <span class="mf">0.39486</span>  <span class="o">-</span><span class="mf">0.58347</span>   <span class="mf">0.41488</span>   <span class="mf">0.37074</span>  <span class="o">-</span><span class="mf">3.6906</span>   <span class="o">-</span><span class="mf">0.20101</span>
<span class="mf">0.11472</span>  <span class="o">-</span><span class="mf">0.34661</span>   <span class="mf">0.36208</span>   <span class="mf">0.095679</span> <span class="o">-</span><span class="mf">0.01765</span>   <span class="mf">0.68498</span>  <span class="o">-</span><span class="mf">0.049013</span>
<span class="mf">0.54049</span>  <span class="o">-</span><span class="mf">0.21005</span>  <span class="o">-</span><span class="mf">0.65397</span>   <span class="mf">0.64556</span> <span class="p">]</span>
<span class="n">queen</span><span class="p">:</span>
<span class="p">[</span><span class="o">-</span><span class="mf">1.1266</span>   <span class="o">-</span><span class="mf">0.52064</span>   <span class="mf">0.45565</span>   <span class="mf">0.21079</span>  <span class="o">-</span><span class="mf">0.05081</span>  <span class="o">-</span><span class="mf">0.65158</span>   <span class="mf">1.1395</span>
<span class="mf">0.69897</span>  <span class="o">-</span><span class="mf">0.20612</span>  <span class="o">-</span><span class="mf">0.71803</span>  <span class="o">-</span><span class="mf">0.02811</span>   <span class="mf">0.10977</span>  <span class="o">-</span><span class="mf">3.3089</span>   <span class="o">-</span><span class="mf">0.49299</span>
<span class="o">-</span><span class="mf">0.51375</span>   <span class="mf">0.10363</span>  <span class="o">-</span><span class="mf">0.11764</span>  <span class="o">-</span><span class="mf">0.084972</span>  <span class="mf">0.02558</span>   <span class="mf">0.6859</span>   <span class="o">-</span><span class="mf">0.29196</span>
<span class="mf">0.4594</span>   <span class="o">-</span><span class="mf">0.39955</span>  <span class="o">-</span><span class="mf">0.40371</span>   <span class="mf">0.31828</span> <span class="p">]</span>
</pre></div>
</div>
<figure class="align-center" id="id2">
<span id="fig-logo"></span><img alt="_images/vector.png" src="_images/vector.png" />
<figcaption>
<p><span class="caption-text">Vector</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="norm">
<h3><span class="section-number">2.1.2. </span>Norm<a class="headerlink" href="#norm" title="Link to this heading"></a></h3>
<p>A norm is a function that maps a vector to a single positive value, representing its
magnitude. Norms are essential for calculating distances between vectors, which play
a crucial role in measuring prediction errors, performing feature selection, and
applying regularization techniques in models.</p>
<figure class="align-center" id="id3">
<span id="id1"></span><img alt="_images/1Bauo.png" src="_images/1Bauo.png" />
<figcaption>
<p><span class="caption-text">Geometrical Interpretation of Norm (<a class="reference external" href="https://math.stackexchange.com/questions/805954/what-does-the-dot-product-of-two-vectors-represent">source_1</a>)</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<ul>
<li><p>Formula:</p>
<blockquote>
<div><p>The <span class="math notranslate nohighlight">\(\displaystyle \ell^p\)</span> norm for <span class="math notranslate nohighlight">\(\vec{v} = (v_1, v_2, \cdots, v_n)\)</span> is</p>
<div class="math notranslate nohighlight">
\[||\vec{v}||_p = \sqrt[p]{|v_1|^p + |v_2|^p + \cdots +|v_n|^p }\]</div>
</div></blockquote>
</li>
<li><p><span class="math notranslate nohighlight">\(\displaystyle \ell^1\)</span> norm: Sum of absolute values of vector components, often used for feature selection due to its tendency to produce sparse solutions.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># l1 norm</span>
<span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">king</span><span class="p">,</span><span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#    max(sum(abs(x), axis=0))</span>

<span class="c1">### 13.188952</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><span class="math notranslate nohighlight">\(\displaystyle \ell^2\)</span> norm: Square root of the sum of squared vector components, the most common norm used in many machine learning algorithms.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># l2 norm</span>
<span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">king</span><span class="p">,</span><span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1">### 4.3206835</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><span class="math notranslate nohighlight">\(\displaystyle \ell^\infty\)</span> norm (Maximum norm): The largest absolute value of a vector component.</p></li>
</ul>
</section>
<section id="distances">
<h3><span class="section-number">2.1.3. </span>Distances<a class="headerlink" href="#distances" title="Link to this heading"></a></h3>
<ul>
<li><p>Manhattan Distance (<span class="math notranslate nohighlight">\(\displaystyle \ell^1\)</span> Distance)</p>
<blockquote>
<div><p>Also known as taxicab or city block distance, Manhattan distance measures the absolute differences
between the components of two vectors. It represents the distance a point would travel along grid
lines in a Cartesian plane, similar to navigating through city streets.</p>
<p>For two vector <span class="math notranslate nohighlight">\(\vec{u} = (u_1, u_2, \cdots, u_n)\)</span> and <span class="math notranslate nohighlight">\(\vec{v} = (v_1, v_2, \cdots, v_n)\)</span>, the
Manhattan Distance distance <span class="math notranslate nohighlight">\(d(\vec{u},\vec{v})\)</span> is</p>
<div class="math notranslate nohighlight">
\[d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_1 = |u_1-v_1| + |u_2-v_2|+ \cdots +|u_n-v_n|\]</div>
</div></blockquote>
</li>
<li><p>Euclidean Distance (<span class="math notranslate nohighlight">\(\displaystyle \ell^2\)</span> Distance)</p>
<blockquote>
<div><p>Euclidean distance is the most common way to measure the distance between two points (vectors) in space.
It is essentially the straight-line distance between them, calculated using the Pythagorean theorem.</p>
<p>For two vector <span class="math notranslate nohighlight">\(\vec{u} = (u_1, u_2, \cdots, u_n)\)</span> and <span class="math notranslate nohighlight">\(\vec{v} = (v_1, v_2, \cdots, v_n)\)</span>, the
Euclidean Distance distance <span class="math notranslate nohighlight">\(d(\vec{u},\vec{v})\)</span> is</p>
<div class="math notranslate nohighlight">
\[d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_2 = \sqrt{(u_1-v_1)^2 + (u_2-v_2)^2+ \cdots +(u_n-v_n)^2}\]</div>
</div></blockquote>
</li>
<li><p>Minkowski Distance (<span class="math notranslate nohighlight">\(\displaystyle \ell^p\)</span> Distance)</p>
<blockquote>
<div><p>Minkowski distance is a generalization of both Euclidean and Manhattan distances. It incorporates a parameter,
<span class="math notranslate nohighlight">\(p\)</span>, which allows for adjusting the sensitivity of the distance metric.</p>
</div></blockquote>
</li>
<li><p>Cos Similarity</p>
<blockquote>
<div><p>Cosine similarity measures the angle between two vectors rather than their straight-line distance.
It evaluates the similarity of two vectors by focusing on their orientation rather than their magnitude.
This makes it particularly useful for high-dimensional data, such as text, where the direction of the
vectors is often more significant than their magnitude.</p>
<p>The Cos similarity for two vector <span class="math notranslate nohighlight">\(\vec{u} = (u_1, u_2, \cdots, u_n)\)</span> and <span class="math notranslate nohighlight">\(\vec{v} = (v_1, v_2, \cdots, v_n)\)</span> is</p>
<div class="math notranslate nohighlight">
\[cos(\theta) = \frac{\vec{u}\cdot\vec{v}}{||\vec{u}|| ||\vec{v}||}\]</div>
<ul class="simple">
<li><p>1 means the vectors point in exactly the same direction (perfect similarity).</p></li>
<li><p>0 means they are orthogonal (no similarity).</p></li>
<li><p>-1 means they point in opposite directions (complete dissimilarity).</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute cosine similarity between the two word vectors</span>
<span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">king</span><span class="p">,</span><span class="n">queen</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">king</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">queen</span><span class="p">))</span>

<span class="c1">### 0.92024213</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute cosine similarity between the two word vectors</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">glove_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word vectors for &#39;</span><span class="si">{</span><span class="n">word1</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">king</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word vectors for &#39;</span><span class="si">{</span><span class="n">word2</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">queen</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cosine similarity between &#39;</span><span class="si">{</span><span class="n">word1</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">word2</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">similarity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Word</span> <span class="n">vectors</span> <span class="k">for</span> <span class="s1">&#39;king&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.74501</span>  <span class="o">-</span><span class="mf">0.11992</span>   <span class="mf">0.37329</span>   <span class="mf">0.36847</span>  <span class="o">-</span><span class="mf">0.4472</span>   <span class="o">-</span><span class="mf">0.2288</span>    <span class="mf">0.70118</span>
<span class="mf">0.82872</span>   <span class="mf">0.39486</span>  <span class="o">-</span><span class="mf">0.58347</span>   <span class="mf">0.41488</span>   <span class="mf">0.37074</span>  <span class="o">-</span><span class="mf">3.6906</span>   <span class="o">-</span><span class="mf">0.20101</span>
<span class="mf">0.11472</span>  <span class="o">-</span><span class="mf">0.34661</span>   <span class="mf">0.36208</span>   <span class="mf">0.095679</span> <span class="o">-</span><span class="mf">0.01765</span>   <span class="mf">0.68498</span>  <span class="o">-</span><span class="mf">0.049013</span>
<span class="mf">0.54049</span>  <span class="o">-</span><span class="mf">0.21005</span>  <span class="o">-</span><span class="mf">0.65397</span>   <span class="mf">0.64556</span> <span class="p">]</span>
<span class="n">Word</span> <span class="n">vectors</span> <span class="k">for</span> <span class="s1">&#39;queen&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1266</span>   <span class="o">-</span><span class="mf">0.52064</span>   <span class="mf">0.45565</span>   <span class="mf">0.21079</span>  <span class="o">-</span><span class="mf">0.05081</span>  <span class="o">-</span><span class="mf">0.65158</span>   <span class="mf">1.1395</span>
<span class="mf">0.69897</span>  <span class="o">-</span><span class="mf">0.20612</span>  <span class="o">-</span><span class="mf">0.71803</span>  <span class="o">-</span><span class="mf">0.02811</span>   <span class="mf">0.10977</span>  <span class="o">-</span><span class="mf">3.3089</span>   <span class="o">-</span><span class="mf">0.49299</span>
<span class="o">-</span><span class="mf">0.51375</span>   <span class="mf">0.10363</span>  <span class="o">-</span><span class="mf">0.11764</span>  <span class="o">-</span><span class="mf">0.084972</span>  <span class="mf">0.02558</span>   <span class="mf">0.6859</span>   <span class="o">-</span><span class="mf">0.29196</span>
<span class="mf">0.4594</span>   <span class="o">-</span><span class="mf">0.39955</span>  <span class="o">-</span><span class="mf">0.40371</span>   <span class="mf">0.31828</span> <span class="p">]</span>
<span class="n">Cosine</span> <span class="n">similarity</span> <span class="n">between</span> <span class="s1">&#39;king&#39;</span> <span class="ow">and</span> <span class="s1">&#39;queen&#39;</span><span class="p">:</span> <span class="mf">0.920242190361023</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
</section>
<section id="nlp-preliminary">
<h2><span class="section-number">2.2. </span>NLP Preliminary<a class="headerlink" href="#nlp-preliminary" title="Link to this heading"></a></h2>
<section id="vocabulary">
<h3><span class="section-number">2.2.1. </span>Vocabulary<a class="headerlink" href="#vocabulary" title="Link to this heading"></a></h3>
<p>In Natural Language Processing (NLP), <strong>vocabulary</strong> refers to the complete set of unique words or tokens
that a model recognizes or works with during training and inference. Vocabulary plays a critical role in
text processing and understanding, as it defines the scope of linguistic units a model can handle.</p>
<ul>
<li><p>Types of Vocabulary in NLP</p>
<blockquote>
<div><p>1. <strong>Word-level Vocabulary</strong>:
- Each word in the text is treated as a unique token.
- For example, the sentence “I love NLP” would generate the vocabulary: <code class="docutils literal notranslate"><span class="pre">{I,</span> <span class="pre">love,</span> <span class="pre">NLP}</span></code>.</p>
<p>2. <strong>Subword-level Vocabulary</strong>:
- Text is broken down into smaller units like prefixes, suffixes, or character sequences.
- For example, the word “loving” might be split into <code class="docutils literal notranslate"><span class="pre">{lov,</span> <span class="pre">ing}</span></code> using techniques like Byte Pair Encoding (BPE) or SentencePiece.
- Subword vocabularies handle rare or unseen words more effectively.</p>
<p>3. <strong>Character-level Vocabulary</strong>:
- Each character is treated as a token.
- For example, the word “love” would generate the vocabulary: <code class="docutils literal notranslate"><span class="pre">{l,</span> <span class="pre">o,</span> <span class="pre">v,</span> <span class="pre">e}</span></code>.</p>
</div></blockquote>
</li>
<li><p>Importance of Vocabulary</p>
<blockquote>
<div><p>1. <strong>Text Representation</strong>:
- Vocabulary is the basis for converting text into numerical representations like one-hot vectors, embeddings, or input IDs for machine learning models.</p>
<p>2. <strong>Model Efficiency</strong>:
- A larger vocabulary increases the model’s memory and computational requirements.
- A smaller vocabulary may lack the capacity to represent all words effectively, leading to a loss of meaning.</p>
<p>3. <strong>Handling Out-of-Vocabulary (OOV) Words</strong>:
- Words not present in the vocabulary are either replaced with a special token like <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code> or processed using subword/character-based techniques.</p>
</div></blockquote>
</li>
<li><p>Building a Vocabulary</p>
<blockquote>
<div><p>Common practices include:</p>
<ol class="arabic simple">
<li><p>Tokenizing the text into words, subwords, or characters.</p></li>
<li><p>Counting the frequency of tokens.</p></li>
<li><p>Keeping only the most frequent tokens up to a predefined size (e.g., top 50,000 tokens).</p></li>
<li><p>Adding special tokens like <code class="docutils literal notranslate"><span class="pre">&lt;PAD&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;</span></code> (beginning of sentence), and <code class="docutils literal notranslate"><span class="pre">&lt;EOS&gt;</span></code> (end of sentence).</p></li>
</ol>
</div></blockquote>
</li>
<li><p>Challenges</p></li>
<li><p><strong>Balancing Vocabulary Size</strong>:
A larger vocabulary increases the richness of representation but requires more computational resources.</p></li>
<li><p><strong>Domain-specific Vocabularies</strong>:
In specialized fields like medicine or law, standard vocabularies may not be sufficient, requiring domain-specific tokenization strategies.</p></li>
</ul>
</section>
<section id="tagging">
<h3><span class="section-number">2.2.2. </span>Tagging<a class="headerlink" href="#tagging" title="Link to this heading"></a></h3>
<p>Tagging in NLP refers to the process of assigning labels or annotations
to words, phrases, or other linguistic units in a text. These labels provide additional information about
the syntactic, semantic, or structural role of the elements in the text.</p>
<ul>
<li><p>Types of Tagging</p>
<blockquote>
<div><ol class="arabic">
<li><p><strong>Part-of-Speech (POS) Tagging</strong>:
- Assigns grammatical tags (e.g., noun, verb, adjective) to each word in a sentence.
- Example: For the sentence “The dog barks,” the tags might be:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">The/DET</span></code> (Determiner)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dog/NOUN</span></code> (Noun)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">barks/VERB</span></code> (Verb).</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Named Entity Recognition (NER) Tagging</strong>:
- Identifies and classifies named entities in a text, such as names of people, organizations, locations, dates, or monetary values.
- Example: In the sentence “John works at Google in California,” the tags might be:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">John/PERSON</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Google/ORGANIZATION</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">California/LOCATION</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Chunking (Syntactic Tagging)</strong>:
- Groups words into syntactic chunks like noun phrases (NP) or verb phrases (VP).
- Example: For the sentence “The quick brown fox jumps,” a chunking result might be:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">[NP</span> <span class="pre">The</span> <span class="pre">quick</span> <span class="pre">brown</span> <span class="pre">fox]</span> <span class="pre">[VP</span> <span class="pre">jumps]</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Sentiment Tagging</strong>:
- Assigns sentiment labels (e.g., positive, negative, neutral) to words, phrases, or entire documents.
- Example: The word “happy” might be tagged as <code class="docutils literal notranslate"><span class="pre">positive</span></code>, while “sad” might be tagged as <code class="docutils literal notranslate"><span class="pre">negative</span></code>.</p></li>
<li><p><strong>Dependency Parsing Tags</strong>:
- Identifies the grammatical relationships between words in a sentence, such as subject, object, or modifier.
- Example: In “She enjoys cooking,” the tags might show:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">She/nsubj</span></code> (nominal subject)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">enjoys/ROOT</span></code> (root of the sentence)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cooking/dobj</span></code> (direct object).</p></li>
</ul>
</div></blockquote>
</li>
</ol>
</div></blockquote>
</li>
<li><p>Importance of Tagging</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Understanding Language Structure</strong>: Tags help NLP models understand the grammatical and syntactic structure of text.</p></li>
<li><p><strong>Improving Downstream Tasks</strong>: Tagging is foundational for tasks like machine
translation, sentiment analysis, question answering, and summarization.</p></li>
<li><p><strong>Feature Engineering</strong>: Tags serve as features for training machine learning models in
text classification or sequence labeling tasks.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Tagging Techniques</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>Rule-based Tagging</strong>: Relies on predefined linguistic rules to assign tags.
Example: Using dictionaries or regular expressions to match specific patterns.</p></li>
<li><p><strong>Statistical Tagging</strong>: Uses probabilistic models like Hidden Markov Models (HMMs)
to predict tags based on word sequences.</p></li>
<li><p><strong>Neural Network-based Tagging</strong>: Employs deep learning models like LSTMs, GRUs, or Transformers
to tag text with high accuracy.</p></li>
</ol>
</div></blockquote>
</li>
<li><p>Challenges</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Ambiguity</strong>:Words with multiple meanings can lead to incorrect tagging.
Example: The word “bank” could mean a financial institution or a riverbank.</p></li>
<li><p><strong>Domain-Specific Language</strong>: General tagging models may fail to perform well on specialized text
like medical or legal documents.</p></li>
<li><p><strong>Data Sparsity</strong>: Rare words or phrases may lack sufficient training data for accurate tagging.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
<section id="lemmatization">
<h3><span class="section-number">2.2.3. </span>Lemmatization<a class="headerlink" href="#lemmatization" title="Link to this heading"></a></h3>
<p>Lemmatization in NLP is the process of reducing a word to its base or dictionary form, known as
the <strong>lemma</strong>. Unlike stemming, which simply removes word suffixes, lemmatization considers
the context and grammatical role of the word to produce a linguistically accurate root form.</p>
<ul>
<li><p>How Lemmatization Works</p>
<ol class="arabic">
<li><p><strong>Contextual Analysis</strong>:
- Lemmatization relies on a vocabulary (lexicon) and morphological analysis to identify a word’s base form.
- For example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">running</span></code> → <code class="docutils literal notranslate"><span class="pre">run</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">better</span></code> → <code class="docutils literal notranslate"><span class="pre">good</span></code></p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Part-of-Speech (POS) Tagging</strong>:
- The process uses POS tags to determine the correct lemma for a word.
- Example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">barking</span></code> (verb) → <code class="docutils literal notranslate"><span class="pre">bark</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">barking</span></code> (adjective, as in “barking dog”) → <code class="docutils literal notranslate"><span class="pre">barking</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
</ol>
</li>
<li><p>Importance of Lemmatization</p>
<ol class="arabic">
<li><p><strong>Improves Text Normalization</strong>:
- Lemmatization helps normalize text by grouping different forms of a word into a single representation.
- Example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">run</span></code>, <code class="docutils literal notranslate"><span class="pre">running</span></code>, and <code class="docutils literal notranslate"><span class="pre">ran</span></code> → <code class="docutils literal notranslate"><span class="pre">run</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Enhances NLP Applications</strong>:
- Lemmatized text improves the performance of tasks like information retrieval, text classification, and sentiment analysis.</p></li>
<li><p><strong>Reduces Vocabulary Size</strong>:
- By mapping inflected forms to their base form, lemmatization reduces redundancy in text, resulting in a smaller vocabulary.</p></li>
</ol>
</li>
<li><p>Lemmatization vs. Stemming</p>
<ul>
<li><p><strong>Lemmatization</strong>:
- Produces linguistically accurate root forms.
- Considers the word’s context and POS.
- Example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">studies</span></code> → <code class="docutils literal notranslate"><span class="pre">study</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Stemming</strong>:
- Applies heuristic rules to strip word suffixes without considering context.
- May produce non-dictionary forms.
- Example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">studies</span></code> → <code class="docutils literal notranslate"><span class="pre">studi</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</li>
<li><p>Techniques for Lemmatization</p>
<ol class="arabic simple">
<li><p><strong>Rule-Based Lemmatization</strong>:
- Relies on predefined linguistic rules and dictionaries.
- Example: WordNet-based lemmatizers.</p></li>
<li><p><strong>Statistical Lemmatization</strong>:
- Uses probabilistic models to predict lemmas based on the context.</p></li>
<li><p><strong>Deep Learning-Based Lemmatization</strong>:
- Employs neural networks and sequence-to-sequence models for highly accurate lemmatization in complex contexts.</p></li>
</ol>
</li>
<li><p>Challenges</p>
<ul>
<li><p><strong>Ambiguity</strong>:
Words with multiple meanings may result in incorrect lemmatization without proper context.
- Example:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">left</span></code> (verb) → <code class="docutils literal notranslate"><span class="pre">leave</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">left</span></code> (noun/adjective) → <code class="docutils literal notranslate"><span class="pre">left</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Language-Specific Complexity</strong>:
Lemmatization rules vary widely across languages, requiring language-specific tools and resources.</p></li>
<li><p><strong>Resource Dependency</strong>:
Lemmatizers require extensive lexicons and morphological rules, which can be resource-intensive to develop.</p></li>
</ul>
</li>
</ul>
</section>
<section id="tokenization">
<h3><span class="section-number">2.2.4. </span>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading"></a></h3>
<p>Tokenization in NLP refers to the process of splitting a text into smaller units, called <strong>tokens</strong>, which
can be words, subwords, sentences, or characters. These tokens serve as the basic building blocks for further
analysis in NLP tasks.</p>
<ul>
<li><p>Types of Tokenization</p>
<ol class="arabic simple">
<li><p><strong>Word Tokenization</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Splits the text into individual words or terms.</p></li>
<li><dl class="simple">
<dt>Example:</dt><dd><ul>
<li><p>Sentence: “I love NLP.”</p></li>
<li><p>Tokens: <code class="docutils literal notranslate"><span class="pre">[&quot;I&quot;,</span> <span class="pre">&quot;love&quot;,</span> <span class="pre">&quot;NLP&quot;]</span></code>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Sentence Tokenization</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Divides a text into sentences.</p></li>
<li><dl class="simple">
<dt>Example:</dt><dd><ul>
<li><p>Text: “I love NLP. It’s amazing.”</p></li>
<li><p>Tokens: <code class="docutils literal notranslate"><span class="pre">[&quot;I</span> <span class="pre">love</span> <span class="pre">NLP.&quot;,</span> <span class="pre">&quot;It’s</span> <span class="pre">amazing.&quot;]</span></code>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Subword Tokenization</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Breaks words into smaller units, often using methods like Byte Pair Encoding (BPE) or SentencePiece.</p></li>
<li><dl class="simple">
<dt>Example:</dt><dd><ul>
<li><p>Word: <code class="docutils literal notranslate"><span class="pre">unhappiness</span></code>.</p></li>
<li><p>Tokens: <code class="docutils literal notranslate"><span class="pre">[&quot;un&quot;,</span> <span class="pre">&quot;happiness&quot;]</span></code> (or subword units like <code class="docutils literal notranslate"><span class="pre">[&quot;un&quot;,</span> <span class="pre">&quot;happi&quot;,</span> <span class="pre">&quot;ness&quot;]</span></code>).</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Character Tokenization</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Treats each character in a word as a separate token.</p></li>
<li><dl class="simple">
<dt>Example:</dt><dd><ul>
<li><p>Word: <code class="docutils literal notranslate"><span class="pre">hello</span></code>.</p></li>
<li><p>Tokens: <code class="docutils literal notranslate"><span class="pre">[&quot;h&quot;,</span> <span class="pre">&quot;e&quot;,</span> <span class="pre">&quot;l&quot;,</span> <span class="pre">&quot;l&quot;,</span> <span class="pre">&quot;o&quot;]</span></code>.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
<li><p>Importance of Tokenization</p>
<ol class="arabic simple">
<li><p><strong>Text Preprocessing</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Tokenization is the first step in many NLP tasks like text classification, translation, and
summarization, as it converts text into manageable pieces.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Text Representation</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Tokens are converted into numerical representations (e.g., word embeddings) for model input
in tasks like sentiment analysis, named entity recognition (NER), or language modeling.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Improving Accuracy</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Proper tokenization ensures that a model processes text at the correct granularity (e.g.,
words or subwords), improving accuracy for tasks like machine translation or text generation.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Challenges of Tokenization</p>
<ol class="arabic simple">
<li><p><strong>Ambiguity</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Certain words or phrases can be tokenized differently based on context.</p></li>
<li><p>Example: “New York” can be treated as one token (location) or two separate tokens (<code class="docutils literal notranslate"><span class="pre">[&quot;New&quot;,</span> <span class="pre">&quot;York&quot;]</span></code>).</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Handling Punctuation</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Deciding how to treat punctuation marks can be challenging. For example, should commas, periods,
or quotes be treated as separate tokens or grouped with adjacent words?</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Multi-word Expressions (MWEs)</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Some expressions consist of multiple words that should be treated as a single token, such as “New York” or “machine learning.”</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Techniques for Tokenization</p>
<ol class="arabic simple">
<li><p><strong>Rule-Based Tokenization</strong>: Uses predefined rules to split text based on spaces, punctuation, and other delimiters.</p></li>
<li><p><strong>Statistical and Machine Learning-Based Tokenization</strong>: Uses trained models to predict token boundaries based on patterns learned from large corpora.</p></li>
<li><p><strong>Deep Learning-Based Tokenization</strong>: Modern tokenization models, such as those used in transformers (e.g., BERT, GPT), may rely on subword tokenization and neural networks to handle complex tokenization tasks.</p></li>
</ol>
</li>
</ul>
</section>
<section id="bert-tokenization">
<h3><span class="section-number">2.2.5. </span>BERT Tokenization<a class="headerlink" href="#bert-tokenization" title="Link to this heading"></a></h3>
<ul>
<li><p>Vocabulary: The BERT Tokenizer’s vocabulary contains 30,522 unique tokens.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="c1"># model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)</span>

<span class="c1"># vocabulary size</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>

<span class="c1"># vocabulary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># vocabulary size</span>
<span class="mi">30522</span>

<span class="c1"># vocabulary</span>
<span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;[PAD]&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;[unused0]&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
              <span class="o">...........</span><span class="p">,</span>
              <span class="p">(</span><span class="s1">&#39;writing&#39;</span><span class="p">,</span> <span class="mi">3015</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;bay&#39;</span><span class="p">,</span> <span class="mi">3016</span><span class="p">),</span>
              <span class="o">...........</span><span class="p">,</span>
              <span class="p">(</span><span class="s1">&#39;##？&#39;</span><span class="p">,</span> <span class="mi">30520</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;##～&#39;</span><span class="p">,</span> <span class="mi">30521</span><span class="p">)])</span>
</pre></div>
</div>
</li>
<li><p>Tokens and IDs</p>
<ul>
<li><p>Tokens to IDs</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Gen AI is awesome&quot;</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="c1"># tokens to ids</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>

<span class="c1"># output</span>
<span class="p">{</span><span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span>  <span class="mi">101</span><span class="p">,</span>  <span class="mi">8991</span><span class="p">,</span>  <span class="mi">9932</span><span class="p">,</span>  <span class="mi">2003</span><span class="p">,</span> <span class="mi">12476</span><span class="p">,</span>   <span class="mi">102</span><span class="p">]]),</span> \
<span class="s1">&#39;token_type_ids&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> \
<span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])}</span>
</pre></div>
</div>
<p>You might notice that there are only four words, yet we have six token IDs.
This is due to the inclusion of two additional special tokens <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">({</span><span class="n">x</span> <span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">]</span><span class="o">+</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span><span class="o">+</span> <span class="p">[</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">]})</span>

<span class="c1">### output</span>
<span class="p">{</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">101</span><span class="p">],</span> <span class="s1">&#39;Gen&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8991</span><span class="p">],</span> <span class="s1">&#39;AI&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">9932</span><span class="p">],</span> <span class="s1">&#39;is&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2003</span><span class="p">],</span> <span class="s1">&#39;awesome&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">12476</span><span class="p">],</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">102</span><span class="p">]}</span>
</pre></div>
</div>
</li>
<li><p>Special Tokens</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Special tokens</span>
<span class="nb">print</span><span class="p">({</span><span class="n">x</span> <span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">,</span> <span class="s1">&#39;[MASK]&#39;</span><span class="p">,</span> <span class="s1">&#39;[EOS]&#39;</span><span class="p">]})</span>

<span class="c1"># tokens to ids</span>
<span class="p">{</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">101</span><span class="p">],</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">102</span><span class="p">],</span> <span class="s1">&#39;[MASK]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">103</span><span class="p">],</span> <span class="s1">&#39;[EOS]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1031</span><span class="p">,</span> <span class="mi">1041</span><span class="p">,</span> <span class="mi">2891</span><span class="p">,</span> <span class="mi">1033</span><span class="p">]}</span>
</pre></div>
</div>
</li>
<li><p>IDs to tokens</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ids to tokens</span>
<span class="n">token_id</span> <span class="o">=</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">({</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="nb">id</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span><span class="nb">id</span> \
      <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">token_id</span><span class="p">})</span>

<span class="c1">### output</span>
<span class="p">{</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="mi">101</span><span class="p">,</span> <span class="s1">&#39;gen&#39;</span><span class="p">:</span> <span class="mi">8991</span><span class="p">,</span> <span class="s1">&#39;ai&#39;</span><span class="p">:</span> <span class="mi">9932</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">:</span> <span class="mi">2003</span><span class="p">,</span> <span class="s1">&#39;awesome&#39;</span><span class="p">:</span> <span class="mi">12476</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="mi">102</span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Out-of-vocabulary tokens</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Gen AI is awesome 👍&quot;</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">({</span><span class="n">x</span> <span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">]</span><span class="o">+</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span><span class="o">+</span> <span class="p">[</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">]})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="c1">### output</span>
<span class="p">{</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">101</span><span class="p">],</span> <span class="s1">&#39;Gen&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8991</span><span class="p">],</span> <span class="s1">&#39;AI&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">9932</span><span class="p">],</span> <span class="s1">&#39;is&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2003</span><span class="p">],</span> <span class="s1">&#39;awesome&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">12476</span><span class="p">],</span> <span class="s1">&#39;👍&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">],</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">102</span><span class="p">]}</span>
<span class="p">[</span><span class="n">UNK</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>Subword Tokenization</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Subword Tokenization</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;GenAI is awesome 👍&quot;</span>
<span class="nb">print</span><span class="p">({</span><span class="n">x</span> <span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">]</span><span class="o">+</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span><span class="o">+</span> <span class="p">[</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">]})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="c1"># output</span>
<span class="p">{</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">101</span><span class="p">],</span> <span class="s1">&#39;GenAI&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8991</span><span class="p">,</span> <span class="mi">4886</span><span class="p">],</span> <span class="s1">&#39;is&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2003</span><span class="p">],</span> <span class="s1">&#39;awesome&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">12476</span><span class="p">],</span> <span class="s1">&#39;👍&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">],</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">102</span><span class="p">]}</span>
<span class="p">[</span><span class="n">UNK</span><span class="p">]</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="platform-and-packages">
<h2><span class="section-number">2.3. </span>Platform and Packages<a class="headerlink" href="#platform-and-packages" title="Link to this heading"></a></h2>
<section id="google-colab">
<h3><span class="section-number">2.3.1. </span>Google Colab<a class="headerlink" href="#google-colab" title="Link to this heading"></a></h3>
<p><strong>Google Colab</strong> (short for Colaboratory) is a free, cloud-based platform that provides users with the ability to write
and execute Python code in an interactive notebook environment. It is based on Jupyter notebooks and is powered by
Google Cloud services, allowing for seamless integration with Google Drive and other Google services. We will primarily
use Google Colab with free T4 GPU runtime throughout this book.</p>
<ul class="simple">
<li><p>Key Features</p></li>
</ul>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>Free Access to GPUs and TPUs</strong>
Colab offers free access to Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), making it an ideal environment for machine learning, deep learning, and other computationally intensive tasks.</p></li>
<li><p><strong>Integration with Google Drive</strong>
You can store and access notebooks directly from your Google Drive, making it easy to collaborate with others and keep your projects organized.</p></li>
<li><p><strong>No Setup Required</strong>
Since Colab is entirely cloud-based, you don’t need to worry about setting up an environment or managing dependencies. Everything is ready to go out of the box.</p></li>
<li><p><strong>Support for Python Libraries</strong>
Colab comes pre-installed with many popular Python libraries, including TensorFlow, PyTorch, Keras, and OpenCV, among others. You can also install any additional libraries using <cite>pip</cite>.</p></li>
<li><p><strong>Collaborative Features</strong>
Multiple users can work on the same notebook simultaneously, making it ideal for collaboration. Changes are synchronized in real-time.</p></li>
<li><p><strong>Rich Media Support</strong>
Colab supports the inclusion of rich media, such as images, videos, and LaTeX equations, directly within the notebook. This makes it a great tool for data analysis, visualization, and educational purposes.</p></li>
<li><p><strong>Easy Sharing</strong>
Notebooks can be easily shared with others via a shareable link, just like Google Docs. Permissions can be set for viewing or editing the document.</p></li>
</ol>
</div></blockquote>
<ul class="simple">
<li><p>GPU Activation
<code class="docutils literal notranslate"><span class="pre">Runtime</span> <span class="pre">--&gt;</span> <span class="pre">change</span> <span class="pre">runtime</span> <span class="pre">type</span> <span class="pre">--&gt;</span> <span class="pre">T4/A100</span> <span class="pre">GPU</span></code></p></li>
</ul>
<table class="borderless docutils align-default" style="width: 100%">
<tbody>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/runtime.png"><img alt="_images/runtime.png" src="_images/runtime.png" style="width: 100%;" />
</a>
</td>
<td><a class="reference internal image-reference" href="_images/T4.png"><img alt="_images/T4.png" src="_images/T4.png" style="width: 100%;" />
</a>
</td>
</tr>
</tbody>
</table>
</section>
<section id="huggingface">
<h3><span class="section-number">2.3.2. </span>HuggingFace<a class="headerlink" href="#huggingface" title="Link to this heading"></a></h3>
<p><strong>Hugging Face</strong> is a company and open-source community focused on providing tools and resources for NLP
and machine learning. It is best known for its popular <strong>Transformers</strong> library, which allows easy access
to pre-trained models for a wide variety of NLP tasks. MOreover,  Hugging Face’s libraries provide simple
Python APIs that make it easy to load models, preprocess data, and run inference. This simplicity allows
both beginners and advanced users to leverage cutting-edge NLP models. We will mainly use the embedding models
and Large Language Models (LLMs) from <strong>Hugging Face Model Hub</strong> central repository.</p>
</section>
<section id="ollama">
<h3><span class="section-number">2.3.3. </span>Ollama<a class="headerlink" href="#ollama" title="Link to this heading"></a></h3>
<p>Ollama is a package designed to run LLMs locally on your personal device or
server, rather than relying on external cloud services. It provides a simple
interface to download and use AI models tailored for various tasks, ensuring
privacy and control over data while still leveraging the power of LLMs.</p>
<ul class="simple">
<li><p>Key features of Ollama:</p>
<ul>
<li><p>Local Execution: Models run entirely on your hardware, making it ideal for users who prioritize data privacy.</p></li>
<li><p>Pre-trained Models: Offers a curated set of LLMs optimized for local usage.</p></li>
<li><p>Cross-Platform: Compatible with macOS, Linux, and other operating systems, depending on hardware specifications.</p></li>
<li><p>Ease of Use: Designed to make setting up and using local AI models simple for non-technical users.</p></li>
<li><p>Efficiency: Focused on lightweight models optimized for local performance without needing extensive computational resources.</p></li>
</ul>
</li>
</ul>
<p>To simplify the management of access tokens for various LLMs, we will use Ollama in Google Colab.</p>
<ul>
<li><p>Ollama installation in Google Colab</p>
<ol class="arabic simple">
<li><p>colab-xterm</p></li>
</ol>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>!pip<span class="w"> </span>install<span class="w"> </span>colab-xterm
%load_ext<span class="w"> </span>colabxterm
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>download ollama</p></li>
</ol>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/content#<span class="w"> </span>curl<span class="w"> </span>https://ollama.ai/install.sh<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh
</pre></div>
</div>
<figure class="align-center" id="fig-ollama-download">
<img alt="_images/ollama_download.png" src="_images/ollama_download.png" />
</figure>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>launch Ollama serve</p></li>
</ol>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/content#<span class="w"> </span>ollama<span class="w"> </span>serve
</pre></div>
</div>
<figure class="align-center" id="fig-ollama-serve">
<img alt="_images/ollama_serve.png" src="_images/ollama_serve.png" />
</figure>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>download models</p></li>
</ol>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/content#<span class="w"> </span>ollama<span class="w"> </span>pull<span class="w"> </span>mistral<span class="w"> </span><span class="c1">#llama3.2 #bge-m3</span>
</pre></div>
</div>
<figure class="align-center" id="fig-ollama-pull">
<img alt="_images/pull_models.png" src="_images/pull_models.png" />
</figure>
</div></blockquote>
<ol class="arabic simple" start="5">
<li><p>check</p></li>
</ol>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>!ollama<span class="w"> </span>list

<span class="c1">####</span>
NAME<span class="w">               </span>ID<span class="w">              </span>SIZE<span class="w">      </span>MODIFIED
llama3.2:latest<span class="w">    </span>a80c4f17acd5<span class="w">    </span><span class="m">2</span>.0<span class="w"> </span>GB<span class="w">    </span><span class="m">14</span><span class="w"> </span>seconds<span class="w"> </span>ago
mistral:latest<span class="w">     </span>f974a74358d6<span class="w">    </span><span class="m">4</span>.1<span class="w"> </span>GB<span class="w">    </span>About<span class="w"> </span>a<span class="w"> </span>minute<span class="w"> </span>ago
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="langchain">
<h3><span class="section-number">2.3.4. </span>langchain<a class="headerlink" href="#langchain" title="Link to this heading"></a></h3>
<p>LangChain is a powerful framework for building AI applications that combine the
capabilities of large language models with external tools, memory, and custom
workflows. It enables developers to create intelligent, context-aware,
and dynamic applications with ease.</p>
<p>It has widely applied in:</p>
<ol class="arabic simple">
<li><p><strong>Conversational AI</strong>
Create chatbots or virtual assistants that maintain context, integrate with APIs, and provide intelligent responses.</p></li>
<li><p><strong>Knowledge Management</strong>
Combine LLMs with external knowledge bases or databases to answer complex questions or summarize documents.</p></li>
<li><p><strong>Automation</strong>
Automate workflows by chaining LLMs with tools for decision-making, data extraction, or content generation.</p></li>
<li><p><strong>Creative Applications</strong>
Use LangChain for generating stories, crafting marketing copy, or producing artistic content.</p></li>
</ol>
<p>We will primarily use LangChain in this book. For instance, to work with downloaded Ollama LLMs, the <code class="docutils literal notranslate"><span class="pre">langchain_ollama</span></code>
package is required.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># chain of thought prompting</span>
<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">CommaSeparatedListOutputParser</span>


<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Question: </span><span class="si">{question}</span>

<span class="s2">Answer: Let&#39;s think step by step.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;mistral&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>
<span class="n">output_parser</span> <span class="o">=</span> <span class="n">CommaSeparatedListOutputParser</span><span class="p">()</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="n">output_parser</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is Mixture of Experts(MoE) in AI?&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;{&quot;answer&quot;:&quot;MoE&#39;</span><span class="p">,</span> <span class="s1">&#39;or Mixture of Experts&#39;</span><span class="p">,</span> <span class="s2">&quot;is a neural network architecture that allows for </span><span class="se">\</span>
<span class="s2">    efficient computation and model parallelism. It consists of multiple &#39;experts&#39;&quot;</span><span class="p">,</span> <span class="s1">&#39;each of </span><span class="se">\</span>
<span class="s1">    which is a smaller neural network that specializes in handling different parts of the input </span><span class="se">\</span>
<span class="s1">    data. The final output is obtained by combining the outputs of these experts based on their </span><span class="se">\</span>
<span class="s1">    expertise relevance to the input. This architecture is particularly useful in tasks where </span><span class="se">\</span>
<span class="s1">    the data exhibits complex and diverse patterns.&quot;}&#39;</span><span class="p">,</span>
    <span class="s1">&#39;</span><span class="se">\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span><span class="s1">&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="preface.html" class="btn btn-neutral float-left" title="1. Preface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="embedding.html" class="btn btn-neutral float-right" title="3. Word and Sentence Embedding" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Wenqiang Feng and Di Zhen.
      <span class="lastupdated">Last updated on Dec 13, 2024.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>