

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>5. Retrieval-Augmented Generation &mdash; GenAI: Best Practices 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />

  
    <link rel="shortcut icon" href="_static/icon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6. Fine Tuning" href="finetuning.html" />
    <link rel="prev" title="4. Prompt Engineering" href="prompt.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            GenAI: Best Practices
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preface.html">1. Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="prelim.html">2. Preliminary</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedding.html">3. Word and Sentence Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompt.html">4. Prompt Engineering</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">5. Retrieval-Augmented Generation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">5.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#indexing">5.2. Indexing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#naive-chunking">5.2.1. Naive Chunking</a></li>
<li class="toctree-l3"><a class="reference internal" href="#late-chunking">5.2.2. Late Chunking</a></li>
<li class="toctree-l3"><a class="reference internal" href="#types-of-indexing">5.2.3. Types of Indexing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vector-database">5.2.4. Vector Database</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#retrieval">5.3. Retrieval</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-retrieval-methods">5.3.1. Common retrieval methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reciprocal-rank-fusion">5.3.2. Reciprocal Rank Fusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generation">5.4. Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-topic">5.5. Advanced Topic</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#self-rag">5.5.1. Self-RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#corrective-rag">5.5.2. Corrective RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-rag">5.5.3. Adaptive RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="#agentic-rag">5.5.4. Agentic RAG</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">6. Fine Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretraining.html">7. Pre-training</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">8. LLM Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">9. Main Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GenAI: Best Practices</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><span class="section-number">5. </span>Retrieval-Augmented Generation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/rag.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="retrieval-augmented-generation">
<span id="rag"></span><h1><span class="section-number">5. </span>Retrieval-Augmented Generation<a class="headerlink" href="#retrieval-augmented-generation" title="Link to this heading"></a></h1>
<div class="admonition-colab-notebook-for-this-chapter admonition">
<p class="admonition-title">Colab Notebook for This Chapter</p>
<ul class="simple">
<li><p>Naive Chunking: <a class="reference external" href="https://colab.research.google.com/drive/1r89QGmEDAS-ZJcfgYv6GKRrNDs4s_lir?usp=drive_link"><img alt="Naive Chunking" src="_images/colab-badge.png" /></a></p></li>
<li><p>Late Chunking: <a class="reference external" href="https://colab.research.google.com/drive/1ZqxmK1YuvPcpJap1psscMTih2jnQVINT?usp=drive_link"><img alt="Late Chunking" src="_images/colab-badge.png" /></a></p></li>
<li><p>Reciprocal Rank Fusion: <a class="reference external" href="https://colab.research.google.com/drive/1Vg7C5Z3Y7OlilnfQlQqkKyNrcnUR6Xry?usp=drive_link"><img alt="RRF" src="_images/colab-badge.png" /></a></p></li>
<li><p>RAG in colab: <a class="reference external" href="https://colab.research.google.com/drive/1jTkoER5eYQqBZs8RgOiCmTl8_WDt67go?usp=drive_link"><img alt="n-rag" src="_images/colab-badge.png" /></a></p></li>
</ul>
<ul class="simple">
<li><p>Self-RAG: <a class="reference external" href="https://colab.research.google.com/drive/1KbC4Wu-JUj6zbVuFkSNvtz3oVzWhEAZG?usp=drive_link"><img alt="S-RAG" src="_images/colab-badge.png" /></a></p></li>
<li><p>Corrective RAG: <a class="reference external" href="https://colab.research.google.com/drive/1Sb7w1cTRZGkFs2LeF3F6gL8WNhjCdWyR?usp=drive_link"><img alt="C-RAG" src="_images/colab-badge.png" /></a></p></li>
<li><p>Adaptive RAG: <a class="reference external" href="https://colab.research.google.com/drive/1tzihPk9Ln96RYcwn6WaHHQjlMdFvCviS?usp=drive_link"><img alt="A-RAG" src="_images/colab-badge.png" /></a></p></li>
<li><p>Agentic RAG: <a class="reference external" href="https://colab.research.google.com/drive/1-eZ61ux0QSCNx2jUDq_vL-6OKLW8Hdr4?usp=drive_link"><img alt="Agentic-RAG" src="_images/colab-badge.png" /></a></p></li>
</ul>
</div>
<figure class="align-center" id="id4">
<span id="fig-rag"></span><img alt="_images/RAG_diagram.png" src="_images/RAG_diagram.png" />
<figcaption>
<p><span class="caption-text">Retrieval-Augmented Generation Diagram</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The naive chunking strategy was used in the diagram above. More advanced strategies,
such as Late Chunking <a class="reference internal" href="reference.html#latechunking" id="id1"><span>[lateChunking]</span></a> (or Chunked Pooling), are discussed later in this chapter.</p>
</div>
<section id="overview">
<h2><span class="section-number">5.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>Retrieval-Augmented Generation (RAG) is a framework that enhances large language models (LLMs)
by combining their generative capabilities with external knowledge retrieval. The goal of RAG
is to improve accuracy, relevance, and factuality by providing the LLM with specific, up-to-date,
or domain-specific context from a knowledge base or database during the generation process.</p>
<p>As you can see in <a class="reference internal" href="#fig-rag"><span class="std std-ref">Retrieval-Augmented Generation Diagram</span></a>, the RAG has there main  components</p>
<ul class="simple">
<li><p>Indexer: The indexer processes raw text or other forms of unstructured data and creates an
efficient structure (called an index) that allows for fast and accurate retrieval
by the retriever when a query is made.</p></li>
<li><p>Retriever: Responsible for finding relevant information from an external knowledge source,
such as a document database, a vector database, or the web.</p></li>
<li><p>Generator: An LLM (like GPT-4, T5, or similar) that uses the retrieved context to generate a response.
The model is “augmented” with the retrieved information, which reduces hallucination and enhances factual accuracy.</p></li>
</ul>
</section>
<section id="indexing">
<h2><span class="section-number">5.2. </span>Indexing<a class="headerlink" href="#indexing" title="Link to this heading"></a></h2>
<p>The indexing processes raw text or other forms of unstructured data and creates an efficient structure
(called an index) that allows for fast and accurate retrieval by the retriever when a query is made.</p>
<figure class="align-center" id="fig-indexing">
<img alt="_images/indexer.png" src="_images/indexer.png" />
</figure>
<section id="naive-chunking">
<h3><span class="section-number">5.2.1. </span>Naive Chunking<a class="headerlink" href="#naive-chunking" title="Link to this heading"></a></h3>
<p>Chunking in Retrieval-Augmented Generation (RAG) involves splitting documents or knowledge bases
into smaller, manageable pieces (chunks) that can be efficiently retrieved and used by a language model (LLM).</p>
<p>Below are the common chunking strategies used in RAG workflows:</p>
<ol class="arabic">
<li><p><strong>Fixed-Length Chunking</strong></p>
<ul class="simple">
<li><p>Chunks are created with a predefined, fixed length (e.g., 200 words or 512 tokens).</p></li>
<li><p>Simple and easy to implement but might split content mid-sentence or lose semantic coherence.</p></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fixed_length_chunking</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">)</span>
    <span class="p">]</span>

<span class="c1"># Example Usage</span>
<span class="n">document</span> <span class="o">=</span> <span class="s2">&quot;This is a sample document with multiple sentences to demonstrate fixed-length chunking.&quot;</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">fixed_length_chunking</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chunk </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>Chunk 1: This is a sample document with multiple sentences to demonstrate</p></li>
<li><p>Chunk 2: fixed-length chunking.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Sliding Window Chunking</strong></p>
<ul class="simple">
<li><p>Creates overlapping chunks to preserve context across splits.</p></li>
<li><p>Ensures important information in overlapping regions is retained.</p></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sliding_window_chunking</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">overlap_size</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="n">chunk_size</span> <span class="o">-</span> <span class="n">overlap_size</span><span class="p">):</span>
        <span class="n">chunk</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">])</span>
        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">chunks</span>

<span class="c1"># Example Usage</span>
<span class="n">document</span> <span class="o">=</span> <span class="s2">&quot;This is a sample document with multiple sentences to demonstrate sliding window chunking.&quot;</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">sliding_window_chunking</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">overlap_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chunk </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt><strong>Output</strong>:</dt><dd><ul class="simple">
<li><p>Chunk 1: This is a sample document with multiple sentences to demonstrate</p></li>
<li><p>Chunk 2: with multiple sentences to demonstrate sliding window chunking.</p></li>
<li><p>Chunk 3: sliding window chunking.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>Semantic Chunking</strong></p>
<ul class="simple">
<li><p>Splits text based on natural language boundaries such as paragraphs, sentences, or specific delimiters (e.g., headings).</p></li>
<li><p>Retains semantic coherence, ideal for better retrieval and generation accuracy.</p></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt_tab&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">semantic_chunking</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">sentence_len</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">chunk</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="n">sentence_len</span><span class="p">:</span>
            <span class="n">chunk</span> <span class="o">+=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">sentence</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
            <span class="n">chunk</span> <span class="o">=</span> <span class="n">sentence</span>
    <span class="k">if</span> <span class="n">chunk</span><span class="p">:</span>
        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">chunks</span>

<span class="c1"># Example Usage</span>
<span class="n">document</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;This is a sample document. It is split based on semantic boundaries. &quot;</span>
            <span class="s2">&quot;Each chunk will have coherent meaning for better retrieval.&quot;</span><span class="p">)</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">semantic_chunking</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chunk </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Output</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>Chunk 1: This is a sample document.</p></li>
<li><p>Chunk 2: It is split based on semantic boundaries.</p></li>
<li><p>Chunk 3: Each chunk will have coherent meaning for better retrieval.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Dynamic Chunking</strong></p>
<ul class="simple">
<li><p>Adapts chunk sizes based on content properties such as token count, content density, or specific criteria.</p></li>
<li><p>Useful when handling diverse document types with varying information density.</p></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">dynamic_chunking</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">tokenizer_name</span><span class="o">=</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">):</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">tokenizer_name</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">max_tokens</span><span class="p">):</span>
        <span class="n">chunk</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">max_tokens</span><span class="p">]</span>
        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">chunk</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">chunks</span>

<span class="c1"># Example Usage</span>
<span class="n">document</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;This is a sample document to demonstrate dynamic chunking. &quot;</span>
            <span class="s2">&quot;The tokenizer adapts the chunks based on token limits.&quot;</span><span class="p">)</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">dynamic_chunking</span><span class="p">(</span><span class="n">document</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chunk </span><span class="si">{</span><span class="n">idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt><strong>Output</strong>:</dt><dd><ul class="simple">
<li><p>Chunk 1: this is a sample document to demonstrate dynamic chunking</p></li>
<li><p>Chunk 2: . the tokenizer adapts the chunks based on</p></li>
<li><p>Chunk 3: token limits.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
<p>Comparison of Strategies</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Strategy</p></th>
<th class="head"><p>Pros</p></th>
<th class="head"><p>Cons</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Fixed-Length Chunking</p></td>
<td><p>Simple, fast</p></td>
<td><p>May split text mid-sentence
or lose coherence.</p></td>
</tr>
<tr class="row-odd"><td><p>Sliding Window Chunking</p></td>
<td><p>Preserves context</p></td>
<td><p>Overlapping increases redundancy.</p></td>
</tr>
<tr class="row-even"><td><p>Semantic Chunking</p></td>
<td><p>Coherent chunks</p></td>
<td><p>Requires NLP preprocessing.</p></td>
</tr>
<tr class="row-odd"><td><p>Dynamic Chunking</p></td>
<td><p>Adapts to content</p></td>
<td><p>Computationally intensive.</p></td>
</tr>
</tbody>
</table>
<p>Each strategy has its strengths and weaknesses. Select based on the task requirements, context, and available computational resources.</p>
<p>The optimal chunk length depends on the type of content being processed and the intended use case.
Below are recommendations for chunk lengths based on different context types, along with their rationale:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Context Type</p></th>
<th class="head"><p>Chunk Length (Tokens)</p></th>
<th class="head"><p>Rationale</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FAQs or Short Texts</p></td>
<td><p>100-200</p></td>
<td><p>Short enough to handle specific queries.</p></td>
</tr>
<tr class="row-odd"><td><p>Articles or Blog Posts</p></td>
<td><p>300-500</p></td>
<td><p>Covers logical sections while fitting multiple
chunks in the LLM context.</p></td>
</tr>
<tr class="row-even"><td><p>Research Papers or Reports</p></td>
<td><p>500-700</p></td>
<td><p>Captures detailed sections like methodology
or results.</p></td>
</tr>
<tr class="row-odd"><td><p>Legal or Technical Texts</p></td>
<td><p>200-300</p></td>
<td><p>Maintains precision due to dense information.</p></td>
</tr>
</tbody>
</table>
<p>The valuating Chunking Strategies for Retrieval can be found at: <a class="reference external" href="https://research.trychroma.com/evaluating-chunking">https://research.trychroma.com/evaluating-chunking</a></p>
</section>
<section id="late-chunking">
<h3><span class="section-number">5.2.2. </span>Late Chunking<a class="headerlink" href="#late-chunking" title="Link to this heading"></a></h3>
<figure class="align-center" id="id5">
<span id="fig-late-chunk"></span><img alt="_images/late_chunking.png" src="_images/late_chunking.png" />
<figcaption>
<p><span class="caption-text">An illustration of the naive chunking strategy (left) and the late chunking strategy (right). (Souce <a class="reference external" href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/">Jina AI</a>)</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Late Chunking</strong> refers to a strategy in Retrieval-Augmented Generation (RAG) where chunking of
data is <strong>deferred until query time</strong>. Unlike pre-chunking, where documents are split into chunks
during preprocessing, late chunking dynamically extracts relevant content when a query is made.</p>
<ul class="simple">
<li><p>Key Concepts of Late Chunking</p>
<ul>
<li><p><strong>Dynamic Chunk Creation</strong>:</p>
<ul>
<li><p>Full documents or large sections are stored in the vector database.</p></li>
<li><p>Relevant chunks are dynamically extracted at query time based on the query and similarity match.</p></li>
</ul>
</li>
<li><p><strong>Query-Time Optimization</strong>:</p>
<ul>
<li><p>The system identifies relevant content using similarity search or semantic analysis.</p></li>
<li><p>Only the most relevant content is chunked and passed to the language model.</p></li>
</ul>
</li>
<li><p><strong>Reduced Preprocessing Time</strong>:</p>
<ul>
<li><p>Eliminates extensive preprocessing and fixed chunking during data ingestion.</p></li>
<li><p>Higher computational cost occurs during query-time retrieval.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The folloing implementations are from <a class="reference external" href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/">Jina AI</a>, and the copyright belongs to the original author.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">chunk_by_sentences</span><span class="p">(</span><span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="nb">callable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split the input text into sentences using the tokenizer</span>
<span class="sd">    :param input_text: The text snippet to split into sentences</span>
<span class="sd">    :param tokenizer: The tokenizer to use</span>
<span class="sd">    :return: A tuple containing the list of text chunks and their corresponding token spans</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">punctuation_mark_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
    <span class="n">sep_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="s1">&#39;[SEP]&#39;</span><span class="p">)</span>
    <span class="n">token_offsets</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;offset_mapping&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">chunk_positions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">token_offsets</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">token_id</span> <span class="o">==</span> <span class="n">punctuation_mark_id</span>
        <span class="ow">and</span> <span class="p">(</span>
            <span class="n">token_offsets</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">token_offsets</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="n">token_ids</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">sep_id</span>
        <span class="p">)</span>
    <span class="p">]</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">input_text</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">+</span> <span class="n">chunk_positions</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">chunk_positions</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">span_annotations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">+</span> <span class="n">chunk_positions</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">chunk_positions</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">chunks</span><span class="p">,</span> <span class="n">span_annotations</span>

<span class="k">def</span> <span class="nf">late_chunking</span><span class="p">(</span>
    <span class="n">model_output</span><span class="p">:</span> <span class="s1">&#39;BatchEncoding&#39;</span><span class="p">,</span> <span class="n">span_annotation</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">model_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">annotations</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">token_embeddings</span><span class="p">,</span> <span class="n">span_annotation</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>  <span class="c1"># remove annotations which go bejond the max-length of the model</span>
            <span class="n">annotations</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="n">annotations</span>
                <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">embeddings</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">annotations</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span>
        <span class="p">]</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">embedding</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">pooled_embeddings</span>
        <span class="p">]</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union&#39;s most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.&quot;</span>

<span class="c1"># determine chunks</span>
<span class="n">chunks</span><span class="p">,</span> <span class="n">span_annotations</span> <span class="o">=</span> <span class="n">chunk_by_sentences</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Chunks:</span><span class="se">\n</span><span class="s1">- &quot;&#39;</span> <span class="o">+</span> <span class="s1">&#39;&quot;</span><span class="se">\n</span><span class="s1">- &quot;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">)</span>


<span class="c1"># chunk before</span>
<span class="n">embeddings_traditional_chunking</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

<span class="c1"># chunk afterwards (context-sensitive chunked pooling)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">model_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">late_chunking</span><span class="p">(</span><span class="n">model_output</span><span class="p">,</span> <span class="p">[</span><span class="n">span_annotations</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">cos_sim</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="n">berlin_embedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;Berlin&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">new_embedding</span><span class="p">,</span> <span class="n">trad_embeddings</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">embeddings_traditional_chunking</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;similarity_new(&quot;Berlin&quot;, &quot;</span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="s1">&quot;):&#39;</span><span class="p">,</span> <span class="n">cos_sim</span><span class="p">(</span><span class="n">berlin_embedding</span><span class="p">,</span> <span class="n">new_embedding</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;similarity_trad(&quot;Berlin&quot;, &quot;</span><span class="si">{</span><span class="n">chunk</span><span class="si">}</span><span class="s1">&quot;):&#39;</span><span class="p">,</span> <span class="n">cos_sim</span><span class="p">(</span><span class="n">berlin_embedding</span><span class="p">,</span> <span class="n">trad_embeddings</span><span class="p">))</span>
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>Query</p></td>
<td><p>Chunk</p></td>
<td><p><strong>similarity_new</strong></p></td>
<td><p><strong>similarity_trad</strong></p></td>
</tr>
<tr class="row-even"><td><p>Berlin</p></td>
<td><p>Berlin is the capital and largest city of Germany, both by area and by population.</p></td>
<td><p>0.849546</p></td>
<td><p>0.8486219</p></td>
</tr>
<tr class="row-odd"><td><p>Berlin</p></td>
<td><p>Its more than 3.85 million inhabitants make it the European Union’s most populous city,
as measured by population within city limits.”</p></td>
<td><p>0.82489026</p></td>
<td><p>0.70843387</p></td>
</tr>
<tr class="row-even"><td><p>Berlin</p></td>
<td><p>The city is also one of the states of Germany, and is the third smallest state in the
country in terms of area.”</p></td>
<td><p>0.8498009</p></td>
<td><p>0.75345534</p></td>
</tr>
</tbody>
</table>
</section>
<section id="types-of-indexing">
<h3><span class="section-number">5.2.3. </span>Types of Indexing<a class="headerlink" href="#types-of-indexing" title="Link to this heading"></a></h3>
<p>The embedding methods we introduced in Chapter <a class="reference internal" href="embedding.html#embedding"><span class="std std-ref">Word and Sentence Embedding</span></a> can be applied here to convert each chunk
into embeddings and create indexing. These indexings(embeddings) will be used to retrieve relevant documents or information.</p>
<ul>
<li><p>Sparse Indexing:</p>
<p>Uses traditional keyword-based methods (e.g., TF-IDF, BM25).
Index stores the frequency of terms and their associations with documents.</p>
<ul class="simple">
<li><p>Advantages:Easy to understand and deploy and works well for exact matches or keyword-heavy queries.</p></li>
<li><p>Disadvantages: Struggles with semantic understanding or paraphrased queries.</p></li>
</ul>
</li>
<li><p>Dense Indexing:</p>
<p>Uses vector embeddings to capture semantic meaning. Documents are represented as vectors in a
high-dimensional space, enabling similarity search.</p>
<ul class="simple">
<li><p>Advantages: Excellent for semantic search, handling synonyms, and paraphrasing.</p></li>
<li><p>Disadvantages: Requires more computational resources for storage and retrieval.</p></li>
</ul>
</li>
<li><p>Hybrid Indexing:</p>
<p>Combines sparse and dense indexing for more robust search capabilities. For example, Elasticsearch
can integrate BM25 with vector search.</p>
</li>
</ul>
</section>
<section id="vector-database">
<h3><span class="section-number">5.2.4. </span>Vector Database<a class="headerlink" href="#vector-database" title="Link to this heading"></a></h3>
<p>Vector databases are essential for Retrieval-Augmented Generation (RAG) systems, enabling
efficient similarity search on dense vector embeddings. Below is a comprehensive overview
of popular vector databases for RAG workflows:</p>
<ol class="arabic simple">
<li><p><strong>FAISS (Facebook AI Similarity Search)</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- An open-source library developed by Facebook AI for efficient similarity search and clustering of dense vectors.</p></li>
<li><p><strong>Features</strong>:
- High performance and scalability.
- Supports various indexing methods like <code class="docutils literal notranslate"><span class="pre">Flat</span></code>, <code class="docutils literal notranslate"><span class="pre">IVF</span></code>, and <code class="docutils literal notranslate"><span class="pre">HNSW</span></code>.
- GPU acceleration for faster searches.</p></li>
<li><p><strong>Use Cases</strong>:
- Research and prototyping.
- Scenarios requiring custom implementations.</p></li>
<li><p><strong>Limitations</strong>:
- File-based storage; lacks a built-in distributed or managed cloud solution.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://github.com/facebookresearch/faiss">FAISS GitHub</a></p></li>
</ul>
</li>
<li><p><strong>Pinecone</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- A fully managed vector database designed for production-scale workloads.</p></li>
<li><p><strong>Features</strong>:
- Scalable and serverless architecture.
- Automatic scaling and optimization of indexes.
- Hybrid search (combining vector and keyword search).
- Integrates with popular frameworks like LangChain and OpenAI.</p></li>
<li><p><strong>Use Cases</strong>:
- Enterprise-grade applications.
- Handling large datasets with minimal operational overhead.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://www.pinecone.io/">Pinecone</a></p></li>
</ul>
</li>
<li><p><strong>Weaviate</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- An open-source vector search engine with a strong focus on modularity and customization.</p></li>
<li><p><strong>Features</strong>:
- Supports hybrid search and symbolic reasoning.
- Schema-based data organization.
- Plugin support for pre-built and custom vectorization modules.
- Cloud-managed and self-hosted options.</p></li>
<li><p><strong>Use Cases</strong>:
- Applications requiring hybrid search capabilities.
- Knowledge graphs and semantically rich data.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://weaviate.io/">Weaviate</a></p></li>
</ul>
</li>
<li><p><strong>Milvus</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- An open-source, high-performance vector database designed for similarity search on large datasets.</p></li>
<li><p><strong>Features</strong>:
- Distributed and scalable architecture.
- Integration with FAISS, Annoy, and HNSW indexing techniques.
- Built-in support for time travel queries (searching historical data).</p></li>
<li><p><strong>Use Cases</strong>:
- Video, audio, and image search applications.
- Large-scale datasets requiring real-time indexing and retrieval.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://milvus.io/">Milvus</a></p></li>
</ul>
</li>
<li><p><strong>Qdrant</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- An open-source, lightweight vector database focused on ease of use and modern developer needs.</p></li>
<li><p><strong>Features</strong>:
- Supports HNSW for efficient vector search.
- Advanced filtering capabilities for combining metadata with vector queries.
- REST and gRPC APIs for integration.
- Docker-ready deployment.</p></li>
<li><p><strong>Use Cases</strong>:
- Scenarios requiring metadata-rich search.
- Lightweight deployments with simplicity in mind.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://qdrant.tech/">Qdrant</a></p></li>
</ul>
</li>
<li><p><strong>Redis (with Vector Similarity Search Module)</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- A popular in-memory database with a module for vector similarity search.</p></li>
<li><p><strong>Features</strong>:
- Combines vector search with traditional key-value storage.
- Supports hybrid search and metadata filtering.
- High throughput and low latency due to in-memory architecture.</p></li>
<li><p><strong>Use Cases</strong>:
- Applications requiring real-time, low-latency search.
- Integrating vector search with existing Redis-based systems.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://redis.io/docs/stack/search/">Redis Vector Search</a></p></li>
</ul>
</li>
<li><p><strong>Zilliz</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- A cloud-native vector database built on Milvus for scalable and managed vector storage.</p></li>
<li><p><strong>Features</strong>:
- Fully managed service for vector data.
- Seamless scaling and distributed indexing.
- Integration with machine learning pipelines.</p></li>
<li><p><strong>Use Cases</strong>:
- Large-scale enterprise deployments.
- Cloud-native solutions with minimal infrastructure management.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://zilliz.com/">Zilliz</a></p></li>
</ul>
</li>
<li><p><strong>Vespa</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- A real-time serving engine supporting vector and hybrid search.</p></li>
<li><p><strong>Features</strong>:
- Combines vector search with advanced ranking and filtering.
- Scales to large datasets with support for distributed clusters.
- Powerful query configuration options.</p></li>
<li><p><strong>Use Cases</strong>:
- E-commerce and recommendation systems.
- Applications with complex ranking requirements.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://vespa.ai/">Vespa</a></p></li>
</ul>
</li>
<li><p><strong>Chroma</strong></p>
<ul class="simple">
<li><p><strong>Description</strong>:
- An open-source, user-friendly vector database built for LLMs and embedding-based applications.</p></li>
<li><p><strong>Features</strong>:
- Designed specifically for RAG workflows.
- Simple Python API for seamless integration with AI models.
- Efficient and customizable vector storage for embedding data.</p></li>
<li><p><strong>Use Cases</strong>:
- Prototyping and experimentation for LLM-based applications.
- Lightweight deployments for small to medium-scale RAG systems.</p></li>
<li><p><strong>Official Website</strong>: <a class="reference external" href="https://www.trychroma.com/">Chroma</a></p></li>
</ul>
</li>
</ol>
<p>Comparison of Vector Databases:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Database</strong></p></th>
<th class="head"><p><strong>Open Source</strong></p></th>
<th class="head"><p><strong>Managed Service</strong></p></th>
<th class="head"><p><strong>Key Features</strong></p></th>
<th class="head"><p><strong>Best For</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FAISS</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>High performance, GPU acceleration</p></td>
<td><p>Research, prototyping</p></td>
</tr>
<tr class="row-odd"><td><p>Pinecone</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>Serverless, automatic scaling</p></td>
<td><p>Enterprise-scale applications</p></td>
</tr>
<tr class="row-even"><td><p>Weaviate</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Hybrid search, modularity</p></td>
<td><p>Knowledge graphs</p></td>
</tr>
<tr class="row-odd"><td><p>Milvus</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>Distributed, high performance</p></td>
<td><p>Large-scale datasets</p></td>
</tr>
<tr class="row-even"><td><p>Qdrant</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>Lightweight, metadata filtering</p></td>
<td><p>Small to medium-scale apps</p></td>
</tr>
<tr class="row-odd"><td><p>Redis</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>In-memory performance, hybrid search</p></td>
<td><p>Real-time apps</p></td>
</tr>
<tr class="row-even"><td><p>Zilliz</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>Fully managed Milvus</p></td>
<td><p>Enterprise cloud solutions</p></td>
</tr>
<tr class="row-odd"><td><p>Vespa</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>Hybrid search, real-time ranking</p></td>
<td><p>E-commerce, recommendations</p></td>
</tr>
<tr class="row-even"><td><p>Chroma</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>LLM-focused, simple API</p></td>
<td><p>Prototyping, lightweight apps</p></td>
</tr>
</tbody>
</table>
<p>Choosing a Vector Database</p>
<ul class="simple">
<li><p><strong>For Research or Small Projects</strong>: FAISS, Qdrant, Milvus, or Chroma.</p></li>
<li><p><strong>For Enterprise or Cloud-Native Workflows</strong>: Pinecone, Zilliz, or Weaviate.</p></li>
<li><p><strong>For Real-Time Use Cases</strong>: Redis or Vespa.</p></li>
</ul>
<p>Each database has unique strengths and is suited for specific RAG use cases. The choice depends on scalability, integration needs, and budget.</p>
</section>
</section>
<section id="retrieval">
<h2><span class="section-number">5.3. </span>Retrieval<a class="headerlink" href="#retrieval" title="Link to this heading"></a></h2>
<p>The retriever selects “chunks” of text (e.g., paragraphs or sections) relevant to the user’s query.</p>
<figure class="align-center" id="id6">
<span id="fig-retriever"></span><a class="reference internal image-reference" href="_images/retriever.png"><img alt="_images/retriever.png" src="_images/retriever.png" style="width: 369.0px; height: 515.5px;" />
</a>
<figcaption>
<p><span class="caption-text">Reciprocal Rank Fusion</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="common-retrieval-methods">
<h3><span class="section-number">5.3.1. </span>Common retrieval methods<a class="headerlink" href="#common-retrieval-methods" title="Link to this heading"></a></h3>
<ul>
<li><p><strong>Sparse Vector Search</strong>: Traditional keyword-based retrieval (e.g., TF-IDF, BM25).</p></li>
<li><p><strong>Dense Vector Search</strong>: Vector-based search using embeddings e.g.</p>
<ul>
<li><p><strong>Approximate Nearest Neighbor (ANN) Search</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>HNSW (Hierarchical Navigable Small World): Graph-based approach</p></li>
<li><p>IVF (Inverted File Index): Clusters embeddings into groups and searches within relevant clusters.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Exact Nearest Neighbor Search</strong>: Computes similarities exhaustively for all vectors in the corpus</p></li>
</ul>
</li>
<li><p><strong>Hybrid Search</strong> (Fig <a class="reference internal" href="#fig-retriever"><span class="std std-ref">Reciprocal Rank Fusion</span></a>): the combination of Sparse and Dense vector search.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>BM25 (Best Matching 25) is a popular ranking function used by search engines
and information retrieval systems to rank documents based on their relevance
to a given query. It belongs to the family of <strong>bag-of-words retrieval models</strong>
and is an enhancement of the <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> approach.</p>
<ul>
<li><p><strong>Key Features of BM25</strong></p>
<ol class="arabic simple">
<li><p><strong>Relevance Scoring</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>BM25 scores documents by measuring how well the query terms match the terms in the document.</p></li>
<li><p>It incorporates term frequency, inverse document frequency, and document length normalization.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Formula</strong>:</p></li>
</ol>
<blockquote>
<div><p>The BM25 score for a document <code class="docutils literal notranslate"><span class="pre">D</span></code> given a query <code class="docutils literal notranslate"><span class="pre">Q</span></code> is calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{BM25}(D, Q) = \sum_{t \in Q} \text{IDF}(t) \cdot \frac{\text{f}(t, D) \cdot (k_1 + 1)}{\text{f}(t, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}\]</div>
<p>Where:
- <code class="docutils literal notranslate"><span class="pre">t</span></code>: Query term.
- <code class="docutils literal notranslate"><span class="pre">f(t,</span> <span class="pre">D)</span></code>: Frequency of term <code class="docutils literal notranslate"><span class="pre">t</span></code> in document <code class="docutils literal notranslate"><span class="pre">D</span></code>.
- <code class="docutils literal notranslate"><span class="pre">|D|</span></code>: Length of document <code class="docutils literal notranslate"><span class="pre">D</span></code> (number of terms).
- <code class="docutils literal notranslate"><span class="pre">avgdl</span></code>: Average document length in the corpus.
- <code class="docutils literal notranslate"><span class="pre">k1</span></code>: Tuning parameter that controls term frequency saturation (usually set between 1.2 and 2.0).
- <code class="docutils literal notranslate"><span class="pre">b</span></code>: Tuning parameter that controls length normalization (usually set to 0.75).
- <code class="docutils literal notranslate"><span class="pre">IDF(t)</span></code>: Inverse Document Frequency of term <code class="docutils literal notranslate"><span class="pre">t</span></code>, calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{IDF}(t) = \log \frac{N - n_t + 0.5}{n_t + 0.5}\]</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of documents in the corpus, and <code class="docutils literal notranslate"><span class="pre">n_t</span></code> is the number of documents containing <code class="docutils literal notranslate"><span class="pre">t</span></code>.</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Improvements Over TF-IDF</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Document Length Normalization: BM25 adjusts for the length of documents, addressing the bias of TF-IDF toward longer documents.</p></li>
<li><p>Saturation of Term Frequency: BM25 avoids the overemphasis of excessively high term frequencies by using a non-linear saturation function controlled by <code class="docutils literal notranslate"><span class="pre">k1</span></code>.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Applications</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p><strong>Information Retrieval</strong>: Ranking search results by relevance.</p></li>
<li><p><strong>Question Answering</strong>: Identifying relevant documents or passages for a query.</p></li>
<li><p><strong>Document Matching</strong>: Comparing similarities between textual content.</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="5">
<li><p><strong>Limitations</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>BM25 does not consider semantic meanings or relationships between words, relying solely on exact term matches.</p></li>
<li><p>It may struggle with queries or documents that require contextual understanding.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<p>Summary of Common Algorithms:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Metric/Algorithm</strong></p></th>
<th class="head"><p><strong>Purpose</strong></p></th>
<th class="head"><p><strong>Common Use</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>TF-IDF</strong></p></td>
<td><p>Keyword matching with term weighting.</p></td>
<td><p>Effective for small-scale or structured corpora.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>BM25</strong></p></td>
<td><p>Advanced keyword matching with term frequency
saturation and document length normalization.</p></td>
<td><p>Widely used in sparse search; default in tools like
Elasticsearch and Solr.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Cosine Similarity</strong></p></td>
<td><p>Measures orientation (ignores magnitude).</p></td>
<td><p>Widely used; works well with normalized vectors.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Dot Product Similarity</strong></p></td>
<td><p>Measures magnitude and direction.</p></td>
<td><p>Preferred in embeddings like OpenAI’s models.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Euclidean Distance</strong></p></td>
<td><p>Measures absolute distance between vectors.</p></td>
<td><p>Less common but used in some specific cases.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>HNSW (ANN)</strong></p></td>
<td><p>Fast and scalable nearest neighbor search.</p></td>
<td><p>Default for large-scale systems (e.g., FAISS).</p></td>
</tr>
<tr class="row-even"><td><p><strong>IVF (ANN)</strong></p></td>
<td><p>Efficient clustering-based search.</p></td>
<td><p>Often combined with product quantization.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reciprocal-rank-fusion">
<h3><span class="section-number">5.3.2. </span>Reciprocal Rank Fusion<a class="headerlink" href="#reciprocal-rank-fusion" title="Link to this heading"></a></h3>
<p>Reciprocal Rank Fusion (RRF) is a ranking technique commonly used in information retrieval
and ensemble learning. Although it is not specific to large language models (LLMs), it can
be applied to scenarios where multiple ranking systems (or scoring mechanisms) produce
different rankings, and you want to combine them into a single, unified ranking.</p>
<dl class="simple">
<dt>The reciprocal rank of an item in a ranked list is calculated as <span class="math notranslate nohighlight">\(\frac{1}{k+r}\)</span>, where</dt><dd><ul class="simple">
<li><p>r is the rank of the item (1 for the top rank, 2 for the second rank, etc.).</p></li>
<li><p>k is a small constant (often set to 60 or another fixed value) to control how much weight is given to higher ranks.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<p>Suppose two retrieval models give ranked lists for query responses:</p>
<ul class="simple">
<li><p>Model 1 ranks documents as: [A,B,C,D]</p></li>
<li><p>Model 2 ranks documents as: [B,A,D,C]</p></li>
</ul>
<p>RRF combines these rankings by assigning each document a combined score:</p>
<ul class="simple">
<li><p>Document A: <span class="math notranslate nohighlight">\(\frac{1}{60+1} +\frac{1}{60+2}=0.03252247488101534\)</span></p></li>
<li><p>Document B: <span class="math notranslate nohighlight">\(\frac{1}{60+2} +\frac{1}{60+1}=0.03252247488101534\)</span></p></li>
<li><p>Document C: <span class="math notranslate nohighlight">\(\frac{1}{60+3} +\frac{1}{60+4}=0.03149801587301587\)</span></p></li>
<li><p>Document D: <span class="math notranslate nohighlight">\(\frac{1}{60+4} +\frac{1}{60+3}=0.03149801587301587\)</span></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="k">def</span> <span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">ranked_results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuse rank from multiple retrieval systems using Reciprocal Rank Fusion.</span>

<span class="sd">    Args:</span>
<span class="sd">    ranked_results: Ranked results from different retrieval system.</span>
<span class="sd">    k (int): A constant used in the RRF formula (default is 60).</span>

<span class="sd">    Returns:</span>
<span class="sd">    Tuple of list of sorted documents by score and sorted documents</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Dictionary to store RRF mapping</span>
    <span class="n">rrf_map</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># Calculate RRF score for each result in each list</span>
    <span class="k">for</span> <span class="n">rank_list</span> <span class="ow">in</span> <span class="n">ranked_results</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rank_list</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">rrf_map</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>

    <span class="c1"># Sort items based on their RRF scores in descending order</span>
    <span class="n">sorted_items</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">rrf_map</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Return tuple of list of sorted documents by score and sorted documents</span>
    <span class="k">return</span> <span class="n">sorted_items</span><span class="p">,</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">sorted_items</span><span class="p">]</span>

<span class="c1"># Example ranked lists from different sources</span>
<span class="n">ranked_a</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">]</span>
<span class="n">ranked_b</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">]</span>


<span class="c1"># Combine the lists using RRF</span>
<span class="n">combined_list</span> <span class="o">=</span> <span class="n">reciprocal_rank_fusion</span><span class="p">([</span><span class="n">ranked_a</span><span class="p">,</span> <span class="n">ranked_b</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">combined_list</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">([(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="mf">0.03252247488101534</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="mf">0.03252247488101534</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="mf">0.03149801587301587</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="mf">0.03149801587301587</span><span class="p">)],</span> <span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="s1">&#39;D&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="generation">
<h2><span class="section-number">5.4. </span>Generation<a class="headerlink" href="#generation" title="Link to this heading"></a></h2>
<p>Finally, the retrieved relevant information will be feed back into the LLMs to generate responses.</p>
<figure class="align-center" id="fig-generator">
<img alt="_images/generator.png" src="_images/generator.png" />
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the remainder of this implementation, we will use the following components:</p>
<ul class="simple">
<li><p>Vector database: <code class="docutils literal notranslate"><span class="pre">Chroma</span></code></p></li>
<li><p>Embedding model: <code class="docutils literal notranslate"><span class="pre">BAAI/bge-m3</span></code></p></li>
<li><p>LLM: <code class="docutils literal notranslate"><span class="pre">mistral</span></code></p></li>
<li><p>Web search engine: <code class="docutils literal notranslate"><span class="pre">Google</span></code></p></li>
</ul>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load models</span>
<span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>

<span class="c1">## embedding model</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">)</span>

<span class="c1">## LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;mistral&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>

<span class="c1"># Indexing</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>  <span class="c1"># Import OllamaEmbeddings instead</span>


<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;https://python.langchain.com/v0.1/docs/get_started/introduction/&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">WebBaseLoader</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]</span>
<span class="n">docs_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">doc_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs_list</span><span class="p">)</span>

<span class="c1"># Add to vectorDB</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">doc_splits</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;rag-chroma&quot;</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Retriever</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Generation</span>
<span class="n">questions</span> <span class="o">=</span> <span class="p">[</span>
  <span class="s2">&quot;what is LangChain?&quot;</span><span class="p">,</span>
  <span class="p">]</span>

<span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">questions</span><span class="p">:</span>
    <span class="n">retrieved_context</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">retrieved_context</span><span class="p">,</span> <span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">)</span>
    <span class="n">response_from_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">)</span>
    <span class="n">parsed_response</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">response_from_model</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Answer: </span><span class="si">{</span><span class="n">parsed_response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Answer</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;answer&quot;</span><span class="p">:</span> <span class="s2">&quot;LangChain refers to chains, agents, and retrieval strategies that make up an application&#39;s cognitive architecture.&quot;</span>
  <span class="p">}</span>
</pre></div>
</div>
</section>
<section id="advanced-topic">
<h2><span class="section-number">5.5. </span>Advanced Topic<a class="headerlink" href="#advanced-topic" title="Link to this heading"></a></h2>
<section id="self-rag">
<h3><span class="section-number">5.5.1. </span>Self-RAG<a class="headerlink" href="#self-rag" title="Link to this heading"></a></h3>
<figure class="align-center" id="id7">
<span id="fig-self-rag-paper"></span><img alt="_images/self_rag_paper.png" src="_images/self_rag_paper.png" />
<figcaption>
<p><span class="caption-text">Overview of SELF-RAG. (Source <a class="reference internal" href="reference.html#selfrag" id="id2"><span>[selfRAG]</span></a>)</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>In the paper <a class="reference internal" href="reference.html#selfrag" id="id3"><span>[selfRAG]</span></a>, Four types decisions are made:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>Should I retrieve from retriever, R</strong></p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p><strong>Input</strong>:
- <cite>x</cite> (question)
- OR <cite>x</cite> (question), <cite>y</cite> (generation)</p></li>
<li><p><strong>Description</strong>:
Decides when to retrieve <cite>D</cite> chunks with <cite>R</cite>.</p></li>
<li><p><strong>Output</strong>:
- <cite>yes</cite>
- <cite>no</cite>
- <cite>continue</cite></p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Are the retrieved passages D relevant to the question x</strong></p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p><strong>Input</strong>:
- (<cite>x</cite> (question), <cite>d</cite> (chunk)) for <cite>d</cite> in <cite>D</cite></p></li>
<li><p><strong>Description</strong>:
Determines if <cite>d</cite> provides useful information to solve <cite>x</cite>.</p></li>
<li><p><strong>Output</strong>:
- <cite>relevant</cite>
- <cite>irrelevant</cite></p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Are the LLM generations from each chunk in D relevant to the chunk (hallucinations, etc.)</strong></p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p><strong>Input</strong>:
- <cite>x</cite> (question), <cite>d</cite> (chunk), <cite>y</cite> (generation) for <cite>d</cite> in <cite>D</cite></p></li>
<li><p><strong>Description</strong>:
Verifies if all statements in <cite>y</cite> (generation) are supported by <cite>d</cite>.</p></li>
<li><p><strong>Output</strong>:
- <cite>fully supported</cite>
- <cite>partially supported</cite>
- <cite>no support</cite></p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Is the LLM generation from each chunk in D a useful response to x (question)</strong></p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p><strong>Input</strong>:
- <cite>x</cite> (question), <cite>y</cite> (generation) for <cite>d</cite> in <cite>D</cite></p></li>
<li><p><strong>Description</strong>:
Assesses if <cite>y</cite> (generation) is a useful response to <cite>x</cite> (question).</p></li>
<li><p><strong>Output</strong>:
- <cite>{5, 4, 3, 2, 1}</cite></p></li>
</ul>
</div></blockquote>
</div></blockquote>
<figure class="align-center" id="id8">
<span id="fig-self-rag"></span><img alt="_images/self_rag.png" src="_images/self_rag.png" />
<figcaption>
<p><span class="caption-text">Self-RAG langgraph diagram (source <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag_local/">Langgraph self-rag</a>)</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<ul>
<li><p>Load Models</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>

<span class="c1"># embedding model</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">)</span>

<span class="c1"># LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;mistral&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You need to specify <code class="docutils literal notranslate"><span class="pre">format='json'</span></code> when Initializing <code class="docutils literal notranslate"><span class="pre">OllamaLLM</span></code>. otherwise
you will get error:</p>
<blockquote>
<div><figure class="align-center">
<img alt="_images/gemini.png" src="_images/gemini.png" />
</figure>
</div></blockquote>
</div>
</li>
<li><p>Create Index</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>  <span class="c1"># Import OllamaEmbeddings instead</span>


<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">WebBaseLoader</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]</span>
<span class="n">docs_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">doc_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs_list</span><span class="p">)</span>

<span class="c1"># Add to vectorDB</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">doc_splits</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;rag-chroma&quot;</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
<ul>
<li><p>Retrieval Grader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Retrieval Grader</span>

<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">JsonOutputParser</span>

<span class="kn">from</span> <span class="nn">langchain_core.pydantic_v1</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">PydanticOutputParser</span>

<span class="c1"># Data model</span>
<span class="k">class</span> <span class="nc">GradeDocuments</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary score for relevance check on retrieved documents.&quot;&quot;&quot;</span>

    <span class="n">score</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>  <span class="c1"># Changed field name to &#39;score&#39;</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Documents are relevant to the question, &#39;yes&#39; or &#39;no&#39;&quot;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">GradeDocuments</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are a grader assessing relevance of a retrieved</span>
<span class="s2">    document to a user question. </span><span class="se">\n</span>
<span class="s2">    Here is the retrieved document: </span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{document}</span><span class="s2"> </span><span class="se">\n\n</span>
<span class="s2">    Here is the user question: </span><span class="si">{question}</span><span class="s2"> </span><span class="se">\n</span>
<span class="s2">    If the document contains keywords related to the user question,</span>
<span class="s2">    grade it as relevant. </span><span class="se">\n</span>
<span class="s2">    It does not need to be a stringent test. The goal is to filter out</span>
<span class="s2">    erroneous retrievals. </span><span class="se">\n</span>
<span class="s2">    Give a binary score &#39;yes&#39; or &#39;no&#39; score to indicate whether the document</span>
<span class="s2">    is relevant to the question. </span><span class="se">\n</span>
<span class="s2">    Provide the binary score as a JSON with a single key &#39;score&#39; and no</span>
<span class="s2">    premable or explanation.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">,</span> <span class="s2">&quot;document&quot;</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
<span class="p">)</span>

<span class="n">retrieval_grader</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;agent memory&quot;</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="n">doc_txt</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span>
<span class="n">retrieval_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;document&quot;</span><span class="p">:</span> <span class="n">doc_txt</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GradeDocuments</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">OllamaLLM</span></code> does not have <code class="docutils literal notranslate"><span class="pre">with_structured_output(GradeDocuments)</span></code>. You need to use</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PydanticOutputParser(pydantic_object=GradeDocuments)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">partial_variables={&quot;format_instructions&quot;:</span> <span class="pre">parser.get_format_instructions()}</span></code></p></li>
</ul>
<p>to format the structured output.</p>
</div>
</li>
<li><p>Generate</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Generate</span>

<span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">hub</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>

<span class="c1"># Prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="s2">&quot;rlm/rag-prompt&quot;</span><span class="p">)</span>

<span class="c1"># LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;mistral&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>


<span class="c1"># Post-processing</span>
<span class="k">def</span> <span class="nf">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>


<span class="c1"># Chain</span>
<span class="n">rag_chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

<span class="c1"># Run</span>
<span class="n">generation</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;Component Two: Memory&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="s2">&quot;Types of Memory&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
              <span class="s2">&quot;Sensory Memory&quot;</span><span class="p">:</span> <span class="p">[</span>
                  <span class="s2">&quot;This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).&quot;</span>
              <span class="p">],</span>
              <span class="s2">&quot;Short-term Memory&quot;</span><span class="p">:</span> <span class="p">[</span>
                  <span class="s2">&quot;Short-term memory as learning embedding representations for raw inputs, including text, image or other modalities;&quot;</span>
                  <span class="p">,</span>
                  <span class="s2">&quot;Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.&quot;</span>
              <span class="p">],</span>
              <span class="s2">&quot;Long-term Memory&quot;</span><span class="p">:</span> <span class="p">[</span>
                  <span class="s2">&quot;Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.&quot;</span>
              <span class="p">]</span>
            <span class="p">},</span>
            <span class="p">{</span>
              <span class="s2">&quot;Maximum Inner Product Search (MIPS)&quot;</span><span class="p">:</span> <span class="p">[</span>
                  <span class="s2">&quot;The external memory can alleviate the restriction of finite attention span. A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)</span><span class="se">\u200b</span><span class="s2"> algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.&quot;</span>
                  <span class="p">,</span>
                  <span class="s2">&quot;A couple common choices of ANN algorithms for fast MIPS:&quot;</span>
              <span class="p">]</span>
            <span class="p">}</span>
        <span class="p">]</span>
      <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Hallucination Grader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Hallucination Grader</span>

<span class="c1"># Data model</span>
<span class="k">class</span> <span class="nc">GradeHallucinations</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary score for relevance check on retrieved documents.&quot;&quot;&quot;</span>

    <span class="n">score</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>  <span class="c1"># Changed field name to &#39;score&#39;</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Documents are relevant to the question, &#39;yes&#39; or &#39;no&#39;&quot;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">GradeHallucinations</span><span class="p">)</span>

<span class="c1"># Prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are a grader assessing whether an answer is grounded in /</span>
<span class="s2">                supported by a set of facts. </span><span class="se">\n</span>
<span class="s2">                Here are the facts:</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                </span><span class="si">{documents}</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                Here is the answer: </span><span class="si">{generation}</span>
<span class="s2">                Give a binary score &#39;yes&#39; or &#39;no&#39; score to indicate whether</span>
<span class="s2">                the answer is grounded in / supported by a set of facts. </span><span class="se">\n</span>
<span class="s2">                Provide the binary score as a JSON with a single key &#39;score&#39;</span>
<span class="s2">                and no preamble or explanation.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">,</span> <span class="s2">&quot;documents&quot;</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
<span class="p">)</span>

<span class="n">hallucination_grader</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">hallucination_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GradeHallucinations</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Answer Grader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Answer Grader</span>

<span class="c1"># Data model</span>
<span class="k">class</span> <span class="nc">GradeAnswer</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary score for relevance check on retrieved documents.&quot;&quot;&quot;</span>

    <span class="n">score</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>  <span class="c1"># Changed field name to &#39;score&#39;</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Documents are relevant to the question, &#39;yes&#39; or &#39;no&#39;&quot;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">GradeAnswer</span><span class="p">)</span>

<span class="c1"># Prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are a grader assessing whether an answer is useful to</span>
<span class="s2">                resolve a question. </span><span class="se">\n</span>
<span class="s2">                Here is the answer:</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                </span><span class="si">{generation}</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                Here is the question: </span><span class="si">{question}</span>
<span class="s2">                Give a binary score &#39;yes&#39; or &#39;no&#39; to indicate whether</span>
<span class="s2">                the answer is useful to resolve a question. </span><span class="se">\n</span>
<span class="s2">                Provide the binary score as a JSON with a single key</span>
<span class="s2">                &#39;score&#39; and no preamble or explanation.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
<span class="p">)</span>

<span class="n">answer_grader</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">answer_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GradeAnswer</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Question Re-writer</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Question Re-writer</span>

<span class="c1"># Prompt</span>
<span class="n">re_write_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You a question re-writer that converts an input question</span>
<span class="s2">                to a better version that is optimized </span><span class="se">\n</span><span class="s2"> for vectorstore</span>
<span class="s2">                retrieval. Look at the input and try to reason about the</span>
<span class="s2">                underlying semantic intent / meaning. </span><span class="se">\n</span>
<span class="s2">                Here is the initial question: </span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{question}</span><span class="s2">.</span>
<span class="s2">                Formulate an improved question.</span><span class="se">\n</span><span class="s2"> &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">question_rewriter</span> <span class="o">=</span> <span class="n">re_write_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="n">question_rewriter</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the function or purpose of an agent&#39;s memory in a given context?&quot;</span> <span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Create the Graph</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>


<span class="k">class</span> <span class="nc">GraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents the state of our graph.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        question: question</span>
<span class="sd">        generation: LLM generation</span>
<span class="sd">        documents: list of documents</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">generation</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

<span class="c1">### Nodes</span>


<span class="k">def</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve documents</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): New key added to state, documents, that contains retrieved documents</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---RETRIEVE---&quot;</span><span class="p">)</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>

    <span class="c1"># Retrieval</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate answer</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): New key added to state, generation, that contains LLM generation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---GENERATE---&quot;</span><span class="p">)</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]</span>

    <span class="c1"># RAG generation</span>
    <span class="n">generation</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">grade_documents</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the retrieved documents are relevant to the question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): Updates documents key with only filtered relevant documents</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---CHECK DOCUMENT RELEVANCE TO QUESTION---&quot;</span><span class="p">)</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]</span>

    <span class="c1"># Score each doc</span>
    <span class="n">filtered_docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">retrieval_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;document&quot;</span><span class="p">:</span> <span class="n">d</span><span class="o">.</span><span class="n">page_content</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="n">grade</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">score</span>
        <span class="k">if</span> <span class="n">grade</span> <span class="o">==</span> <span class="s2">&quot;yes&quot;</span> <span class="ow">or</span> <span class="n">grade</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---GRADE: DOCUMENT RELEVANT---&quot;</span><span class="p">)</span>
            <span class="n">filtered_docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---GRADE: DOCUMENT NOT RELEVANT---&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">filtered_docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">transform_query</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform the query to produce a better question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): Updates question key with a re-phrased question</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---TRANSFORM QUERY---&quot;</span><span class="p">)</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]</span>

    <span class="c1"># Re-write question</span>
    <span class="n">better_question</span> <span class="o">=</span> <span class="n">question_rewriter</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">better_question</span><span class="p">}</span>


<span class="c1">### Edges</span>


<span class="k">def</span> <span class="nf">decide_to_generate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether to generate an answer, or re-generate a question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: Binary decision for next node to call</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---ASSESS GRADED DOCUMENTS---&quot;</span><span class="p">)</span>
    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">filtered_documents</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">filtered_documents</span><span class="p">:</span>
        <span class="c1"># All documents have been filtered check_relevance</span>
        <span class="c1"># We will re-generate a new query</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;transform_query&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We have relevant documents, so generate answer</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---DECISION: GENERATE---&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;generate&quot;</span>


<span class="k">def</span> <span class="nf">grade_generation_v_documents_and_question</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the generation is grounded in the document and answers question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: Decision for next node to call</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---CHECK HALLUCINATIONS---&quot;</span><span class="p">)</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]</span>
    <span class="n">generation</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">]</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">hallucination_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">}</span>
    <span class="p">)</span>
    <span class="n">grade</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">score</span>

    <span class="c1"># Check hallucination</span>
    <span class="k">if</span> <span class="n">grade</span> <span class="o">==</span> <span class="s2">&quot;yes&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---&quot;</span><span class="p">)</span>
        <span class="c1"># Check question-answering</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---GRADE GENERATION vs QUESTION---&quot;</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">answer_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">})</span>
        <span class="n">grade</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">score</span>
        <span class="k">if</span> <span class="n">grade</span> <span class="o">==</span> <span class="s2">&quot;yes&quot;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---DECISION: GENERATION ADDRESSES QUESTION---&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;useful&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---DECISION: GENERATION DOES NOT ADDRESS QUESTION---&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;not useful&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;not supported&quot;</span>

<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span>

<span class="n">workflow</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">GraphState</span><span class="p">)</span>

<span class="c1"># Define the nodes</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="n">retrieve</span><span class="p">)</span>  <span class="c1"># retrieve</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;grade_documents&quot;</span><span class="p">,</span> <span class="n">grade_documents</span><span class="p">)</span>  <span class="c1"># grade documents</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="n">generate</span><span class="p">)</span>  <span class="c1"># generatae</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;transform_query&quot;</span><span class="p">,</span> <span class="n">transform_query</span><span class="p">)</span>  <span class="c1"># transform_query</span>

<span class="c1"># Build graph</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="s2">&quot;grade_documents&quot;</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
    <span class="s2">&quot;grade_documents&quot;</span><span class="p">,</span>
    <span class="n">decide_to_generate</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="s2">&quot;transform_query&quot;</span><span class="p">:</span> <span class="s2">&quot;transform_query&quot;</span><span class="p">,</span>
        <span class="s2">&quot;generate&quot;</span><span class="p">:</span> <span class="s2">&quot;generate&quot;</span><span class="p">,</span>
        <span class="s2">&quot;out of context&quot;</span><span class="p">:</span> <span class="s2">&quot;generate&quot;</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;transform_query&quot;</span><span class="p">,</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
    <span class="s2">&quot;generate&quot;</span><span class="p">,</span>
    <span class="n">grade_generation_v_documents_and_question</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="s2">&quot;not supported&quot;</span><span class="p">:</span> <span class="n">END</span><span class="p">,</span>
        <span class="s2">&quot;useful&quot;</span><span class="p">:</span> <span class="n">END</span><span class="p">,</span>
        <span class="s2">&quot;not useful&quot;</span><span class="p">:</span> <span class="s2">&quot;transform_query&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Compile</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>Graph visualization</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">get_graph</span><span class="p">(</span><span class="n">xray</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>Ouput</p>
<figure class="align-center" id="id9">
<span id="fig-self-rag-graph"></span><img alt="_images/self_rag_graph.png" src="_images/self_rag_graph.png" />
<figcaption>
<p><span class="caption-text">Self-RAG Graph</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
<li><p>Test</p>
<ul>
<li><p>Relevant retrieval</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="c1"># Run</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is prompt engineering?&quot;</span><span class="p">}</span>
<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">app</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Node</span>
        <span class="n">pprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Node &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
        <span class="c1"># Optional: print full state at each node</span>
        <span class="c1"># pprint.pprint(value[&quot;keys&quot;], indent=2, width=80, depth=None)</span>
    <span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Final generation</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">---</span><span class="n">RETRIEVE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;retrieve&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANCE</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">ASSESS</span> <span class="n">GRADED</span> <span class="n">DOCUMENTS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">GENERATE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;grade_documents&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">GENERATE</span><span class="o">---</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">HALLUCINATIONS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">GENERATION</span> <span class="n">IS</span> <span class="n">GROUNDED</span> <span class="n">IN</span> <span class="n">DOCUMENTS</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span> <span class="n">GENERATION</span> <span class="n">vs</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">GENERATION</span> <span class="n">ADDRESSES</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;generate&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="p">(</span><span class="s1">&#39;{</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="s1">&#39;    &quot;Prompt Engineering&quot; : &quot;A method for communicating with language models &#39;</span>
<span class="s1">&#39;(LLMs) to steer their behavior towards desired outcomes without updating &#39;</span>
<span class="s1">&#39;model weights. It involves alignment and model steerability, and requires &#39;</span>
<span class="s1">&#39;heavy experimentation and heuristics.&quot;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="s1">&#39;}&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Irrelevant retrieval</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="c1"># Run</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;SegRNN?&quot;</span><span class="p">}</span>
<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">app</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Node</span>
        <span class="n">pprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Node &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
        <span class="c1"># Optional: print full state at each node</span>
        <span class="c1"># pprint.pprint(value[&quot;keys&quot;], indent=2, width=80, depth=None)</span>
    <span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Final generation</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">---</span><span class="n">RETRIEVE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;retrieve&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANCE</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">ASSESS</span> <span class="n">GRADED</span> <span class="n">DOCUMENTS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">ALL</span> <span class="n">DOCUMENTS</span> <span class="n">ARE</span> <span class="n">NOT</span> <span class="n">RELEVANT</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="p">,</span> <span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;grade_documents&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;transform_query&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">RETRIEVE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;retrieve&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANCE</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">ASSESS</span> <span class="n">GRADED</span> <span class="n">DOCUMENTS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">ALL</span> <span class="n">DOCUMENTS</span> <span class="n">ARE</span> <span class="n">NOT</span> <span class="n">RELEVANT</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="p">,</span> <span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;grade_documents&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;transform_query&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">RETRIEVE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;retrieve&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANCE</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">ASSESS</span> <span class="n">GRADED</span> <span class="n">DOCUMENTS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">ALL</span> <span class="n">DOCUMENTS</span> <span class="n">ARE</span> <span class="n">NOT</span> <span class="n">RELEVANT</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="p">,</span> <span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;grade_documents&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;transform_query&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">RETRIEVE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;retrieve&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANCE</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">ASSESS</span> <span class="n">GRADED</span> <span class="n">DOCUMENTS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">ALL</span> <span class="n">DOCUMENTS</span> <span class="n">ARE</span> <span class="n">NOT</span> <span class="n">RELEVANT</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="p">,</span> <span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;grade_documents&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">TRANSFORM</span> <span class="n">QUERY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;transform_query&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">RETRIEVE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;retrieve&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANCE</span> <span class="n">TO</span> <span class="n">QUESTION</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">NOT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">ASSESS</span> <span class="n">GRADED</span> <span class="n">DOCUMENTS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">GENERATE</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;grade_documents&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">GENERATE</span><span class="o">---</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">HALLUCINATIONS</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">GENERATION</span> <span class="n">IS</span> <span class="n">NOT</span> <span class="n">GROUNDED</span> <span class="n">IN</span> <span class="n">DOCUMENTS</span><span class="p">,</span> <span class="n">RE</span><span class="o">-</span><span class="n">TRY</span><span class="o">---</span>
<span class="s2">&quot;Node &#39;generate&#39;:&quot;</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="p">(</span><span class="s1">&#39;{</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="s1">&#39;      &quot;Question&quot;: &quot;Define and provide an explanation for a Sequential &#39;</span>
<span class="s1">&#39;Recurrent Neural Network (SegRNN)&quot;,</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="s1">&#39;      &quot;Answer&quot;: &quot;A Sequential Recurrent Neural Network (SegRNN) is a type of &#39;</span>
<span class="s1">&#39;artificial neural network used in machine learning. It processes input data &#39;</span>
<span class="s1">&#39;sequentially, allowing it to maintain internal state over time and use this &#39;</span>
<span class="s1">&#39;context when processing new data points. This makes SegRNNs particularly &#39;</span>
<span class="s1">&#39;useful for tasks such as speech recognition, language modeling, and time &#39;</span>
<span class="s1">&#39;series analysis.&quot;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="s1">&#39;   }&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="corrective-rag">
<h3><span class="section-number">5.5.2. </span>Corrective RAG<a class="headerlink" href="#corrective-rag" title="Link to this heading"></a></h3>
<figure class="align-center" id="id10">
<span id="fig-c-rag"></span><img alt="_images/corrective_rag.png" src="_images/corrective_rag.png" />
<figcaption>
<p><span class="caption-text">Corrective-RAG langgraph diagram (source <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag_local/">Langgraph c-rag</a>)</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<ul>
<li><p>Load Models</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>

<span class="c1"># embedding model</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">)</span>

<span class="c1"># LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;mistral&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You need to specify <code class="docutils literal notranslate"><span class="pre">format='json'</span></code> when Initializing <code class="docutils literal notranslate"><span class="pre">OllamaLLM</span></code>. otherwise
you will get error:</p>
<blockquote>
<div><figure class="align-center">
<img alt="_images/gemini.png" src="_images/gemini.png" />
</figure>
</div></blockquote>
</div>
</li>
<li><p>Create Index</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>  <span class="c1"># Import OllamaEmbeddings instead</span>


<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">WebBaseLoader</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]</span>
<span class="n">docs_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">doc_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs_list</span><span class="p">)</span>

<span class="c1"># Add to vectorDB</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">doc_splits</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;rag-chroma&quot;</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
<ul>
<li><p>Retrieval Grader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Retrieval Grader</span>

<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">JsonOutputParser</span>

<span class="kn">from</span> <span class="nn">langchain_core.pydantic_v1</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">PydanticOutputParser</span>

<span class="c1"># Data model</span>
<span class="k">class</span> <span class="nc">GradeDocuments</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary score for relevance check on retrieved documents.&quot;&quot;&quot;</span>

    <span class="n">score</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>  <span class="c1"># Changed field name to &#39;score&#39;</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Documents are relevant to the question, &#39;yes&#39; or &#39;no&#39;&quot;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">GradeDocuments</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are a grader assessing relevance of a retrieved</span>
<span class="s2">    document to a user question. </span><span class="se">\n</span>
<span class="s2">    Here is the retrieved document: </span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{document}</span><span class="s2"> </span><span class="se">\n\n</span>
<span class="s2">    Here is the user question: </span><span class="si">{question}</span><span class="s2"> </span><span class="se">\n</span>
<span class="s2">    If the document contains keywords related to the user question,</span>
<span class="s2">    grade it as relevant. </span><span class="se">\n</span>
<span class="s2">    It does not need to be a stringent test. The goal is to filter out</span>
<span class="s2">    erroneous retrievals. </span><span class="se">\n</span>
<span class="s2">    Give a binary score &#39;yes&#39; or &#39;no&#39; score to indicate whether the document</span>
<span class="s2">    is relevant to the question. </span><span class="se">\n</span>
<span class="s2">    Provide the binary score as a JSON with a single key &#39;score&#39; and no</span>
<span class="s2">    premable or explanation.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">,</span> <span class="s2">&quot;document&quot;</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
<span class="p">)</span>

<span class="n">retrieval_grader</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;agent memory&quot;</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="n">doc_txt</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span>
<span class="n">retrieval_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;document&quot;</span><span class="p">:</span> <span class="n">doc_txt</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GradeDocuments</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The output from LangChain Official tutorials (<a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag_local/">Langgraph c-rag</a>) is
<code class="docutils literal notranslate"><span class="pre">{'score':</span> <span class="pre">1}</span></code>. If you use that implementation, you need to add
the <code class="docutils literal notranslate"><span class="pre">or</span> <span class="pre">grade</span> <span class="pre">==</span> <span class="pre">1</span></code> in <code class="docutils literal notranslate"><span class="pre">grade_documents</span></code>. Otherwise, it will always
use web search.</p>
</div>
</li>
<li><p>Generate</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Generate</span>

<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>

<span class="c1"># Prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are an assistant for question-answering tasks.</span>

<span class="s2">    Use the following documents to answer the question.</span>

<span class="s2">    If you don&#39;t know the answer, just say that you don&#39;t know.</span>

<span class="s2">    Use three sentences maximum and keep the answer concise:</span>
<span class="s2">    Question: </span><span class="si">{question}</span>
<span class="s2">    Documents: </span><span class="si">{documents}</span>
<span class="s2">    Answer:</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">,</span> <span class="s2">&quot;documents&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Chain</span>
<span class="n">rag_chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

<span class="c1"># Run</span>
<span class="n">generation</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;short_term_memory&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;They discussed the risks, especially with illicit drugs and bioweapons.&quot;</span><span class="p">,</span> <span class="s2">&quot;They developed a test set containing a list of known chemical weapon agents&quot;</span><span class="p">,</span> <span class="s2">&quot;4 out of 11 requests (36%) were accepted to obtain a synthesis solution&quot;</span><span class="p">,</span> <span class="s2">&quot;The agent attempted to consult documentation to execute the procedure&quot;</span><span class="p">,</span> <span class="s2">&quot;7 out of 11 were rejected&quot;</span><span class="p">,</span> <span class="s2">&quot;5 happened after a Web search&quot;</span><span class="p">,</span> <span class="s2">&quot;2 were rejected based on prompt only&quot;</span><span class="p">,</span> <span class="s2">&quot;Generative Agents Simulation#&quot;</span><span class="p">,</span> <span class="s2">&quot;Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters&quot;</span><span class="p">],</span>
  <span class="s2">&quot;long_term_memory&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience&quot;</span><span class="p">,</span> <span class="s2">&quot;The memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language&quot;</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Router</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Router</span>

<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">JsonOutputParser</span>


<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are an expert at routing a</span>
<span class="s2">    user question to a vectorstore or web search. Use the vectorstore for</span>
<span class="s2">    questions on LLM agents, prompt engineering, prompting, and adversarial</span>
<span class="s2">    attacks. You can also use words that are similar to those,</span>
<span class="s2">    no need to have exactly those words. Otherwise, use web-search.</span>

<span class="s2">    Give a binary choice &#39;web_search&#39; or &#39;vectorstore&#39; based on the question.</span>
<span class="s2">    Return the a JSON with a single key &#39;datasource&#39; and</span>
<span class="s2">    no preamble or explanation.</span>

<span class="s2">    Examples:</span>
<span class="s2">    Question: When will the Euro of Football take place?</span>
<span class="s2">    Answer: {{&quot;datasource&quot;: &quot;web_search&quot;}}</span>

<span class="s2">    Question: What are the types of agent memory?</span>
<span class="s2">    Answer: {{&quot;datasource&quot;: &quot;vectorstore&quot;}}</span>

<span class="s2">    Question: What are the basic approaches for prompt engineering?</span>
<span class="s2">    Answer: {{&quot;datasource&quot;: &quot;vectorstore&quot;}}</span>

<span class="s2">    Question: What is prompt engineering?</span>
<span class="s2">    Answer: {{&quot;datasource&quot;: &quot;vectorstore&quot;}}</span>

<span class="s2">    Question to route:</span>
<span class="s2">    </span><span class="si">{question}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">],</span>
<span class="p">)</span>


<span class="n">question_router</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">JsonOutputParser</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">question_router</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;When will the Euro of Football </span><span class="se">\</span>
<span class="s2">                                          take place?&quot;</span><span class="p">}))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">question_router</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What are the types of agent </span><span class="se">\</span>
<span class="s2">                                          memory?&quot;</span><span class="p">}))</span> <span class="c1">### Index</span>

<span class="nb">print</span><span class="p">(</span><span class="n">question_router</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What are the basic approaches for </span><span class="se">\</span>
<span class="s2">                                          prompt engineering?&quot;</span><span class="p">}))</span> <span class="c1">### Index</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;datasource&#39;</span><span class="p">:</span> <span class="s1">&#39;web_search&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;datasource&#39;</span><span class="p">:</span> <span class="s1">&#39;vectorstore&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;datasource&#39;</span><span class="p">:</span> <span class="s1">&#39;vectorstore&#39;</span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Hallucination Grader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Hallucination Grader</span>

<span class="c1"># Data model</span>
<span class="k">class</span> <span class="nc">GradeHallucinations</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary score for relevance check on retrieved documents.&quot;&quot;&quot;</span>

    <span class="n">score</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>  <span class="c1"># Changed field name to &#39;score&#39;</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Documents are relevant to the question, &#39;yes&#39; or &#39;no&#39;&quot;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">GradeHallucinations</span><span class="p">)</span>

<span class="c1"># Prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are a grader assessing whether an answer is grounded in /</span>
<span class="s2">                supported by a set of facts. </span><span class="se">\n</span>
<span class="s2">                Here are the facts:</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                </span><span class="si">{documents}</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                Here is the answer: </span><span class="si">{generation}</span>
<span class="s2">                Give a binary score &#39;yes&#39; or &#39;no&#39; score to indicate whether</span>
<span class="s2">                the answer is grounded in / supported by a set of facts. </span><span class="se">\n</span>
<span class="s2">                Provide the binary score as a JSON with a single key &#39;score&#39;</span>
<span class="s2">                and no preamble or explanation.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">,</span> <span class="s2">&quot;documents&quot;</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
<span class="p">)</span>

<span class="n">hallucination_grader</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">hallucination_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GradeHallucinations</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Answer Grader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Answer Grader</span>

<span class="c1"># Data model</span>
<span class="k">class</span> <span class="nc">GradeAnswer</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary score for relevance check on retrieved documents.&quot;&quot;&quot;</span>

    <span class="n">score</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>  <span class="c1"># Changed field name to &#39;score&#39;</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Documents are relevant to the question, &#39;yes&#39; or &#39;no&#39;&quot;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">GradeAnswer</span><span class="p">)</span>

<span class="c1"># Prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are a grader assessing whether an answer is useful to</span>
<span class="s2">                resolve a question. </span><span class="se">\n</span>
<span class="s2">                Here is the answer:</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                </span><span class="si">{generation}</span>
<span class="s2">                </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">                Here is the question: </span><span class="si">{question}</span>
<span class="s2">                Give a binary score &#39;yes&#39; or &#39;no&#39; to indicate whether</span>
<span class="s2">                the answer is useful to resolve a question. </span><span class="se">\n</span>
<span class="s2">                Provide the binary score as a JSON with a single key</span>
<span class="s2">                &#39;score&#39; and no preamble or explanation.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
<span class="p">)</span>

<span class="n">answer_grader</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">answer_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GradeAnswer</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Question Re-writer</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Question Re-writer</span>

<span class="c1"># Prompt</span>
<span class="n">re_write_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You a question re-writer that converts an input question</span>
<span class="s2">                to a better version that is optimized </span><span class="se">\n</span><span class="s2"> for vectorstore</span>
<span class="s2">                retrieval. Look at the input and try to reason about the</span>
<span class="s2">                underlying semantic intent / meaning. </span><span class="se">\n</span>
<span class="s2">                Here is the initial question: </span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{question}</span><span class="s2">.</span>
<span class="s2">                Formulate an improved question.</span><span class="se">\n</span><span class="s2"> &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;generation&quot;</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">question_rewriter</span> <span class="o">=</span> <span class="n">re_write_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="n">question_rewriter</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the function or purpose of an agent&#39;s memory in a given context?&quot;</span> <span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Web Search Tool (Google)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_google_community</span> <span class="kn">import</span> <span class="n">GoogleSearchAPIWrapper</span><span class="p">,</span> <span class="n">GoogleSearchResults</span>

<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">userdata</span>
<span class="n">api_key</span> <span class="o">=</span> <span class="n">userdata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;GOOGLE_API_KEY&#39;</span><span class="p">)</span>
<span class="n">cx</span> <span class="o">=</span>  <span class="n">userdata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;GOOGLE_CSE_ID&#39;</span><span class="p">)</span>
<span class="c1"># Replace with your actual API key and CX ID</span>

<span class="c1"># Create an instance of the GoogleSearchAPIWrapper</span>
<span class="n">google_search_wrapper</span> <span class="o">=</span> <span class="n">GoogleSearchAPIWrapper</span><span class="p">(</span><span class="n">google_api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">google_cse_id</span><span class="o">=</span><span class="n">cx</span><span class="p">)</span>

<span class="c1"># Pass the api_wrapper to GoogleSearchResults</span>
<span class="n">web_search_tool</span> <span class="o">=</span> <span class="n">GoogleSearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">google_search_wrapper</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># web_results = web_search_tool.invoke({&quot;query&quot;: question})</span>
</pre></div>
</div>
</li>
<li><p>Create the Graph</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">langchain.schema</span> <span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span>


<span class="k">class</span> <span class="nc">GraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Represents the state of our graph.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        question: question</span>
<span class="sd">        generation: LLM generation</span>
<span class="sd">        search: whether to add search</span>
<span class="sd">        documents: list of documents</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">generation</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">search</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">documents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">steps</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retrieve documents</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): New key added to state, documents, that contains retrieved documents</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;retrieve_documents&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="n">steps</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate answer</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): New key added to state, generation, that contains LLM generation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]</span>
    <span class="n">generation</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;generate_answer&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span>
        <span class="s2">&quot;generation&quot;</span><span class="p">:</span> <span class="n">generation</span><span class="p">,</span>
        <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="n">steps</span><span class="p">,</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">grade_documents</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the retrieved documents are relevant to the question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): Updates documents key with only filtered relevant documents</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">]</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;grade_document_retrieval&quot;</span><span class="p">)</span>
    <span class="n">filtered_docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">search</span> <span class="o">=</span> <span class="s2">&quot;No&quot;</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">retrieval_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">d</span><span class="o">.</span><span class="n">page_content</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="n">grade</span> <span class="o">=</span> <span class="n">score</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">grade</span> <span class="o">==</span> <span class="s2">&quot;yes&quot;</span> <span class="ow">or</span> <span class="n">grade</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;---GRADE: DOCUMENT </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> RELEVANT---&quot;</span><span class="p">)</span>
            <span class="n">filtered_docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;---GRADE: DOCUMENT </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> ISN&#39;T RELEVANT---&quot;</span><span class="p">)</span>
            <span class="n">search</span> <span class="o">=</span> <span class="s2">&quot;Yes&quot;</span>
            <span class="k">continue</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">filtered_docs</span><span class="p">,</span>
        <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span>
        <span class="s2">&quot;search&quot;</span><span class="p">:</span> <span class="n">search</span><span class="p">,</span>
        <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="n">steps</span><span class="p">,</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">web_search</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Web search based on the re-phrased question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        state (dict): Updates documents key with appended web results</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">question</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;question&quot;</span><span class="p">]</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;documents&quot;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span>
    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;web_search&quot;</span><span class="p">)</span>
    <span class="n">web_results</span> <span class="o">=</span> <span class="n">web_search_tool</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
    <span class="n">documents</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;snippet&quot;</span><span class="p">],</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="n">d</span><span class="p">[</span><span class="s2">&quot;link&quot;</span><span class="p">]})</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">eval</span><span class="p">(</span><span class="n">web_results</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;documents&quot;</span><span class="p">:</span> <span class="n">documents</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="n">steps</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">decide_to_generate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether to generate an answer, or re-generate a question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (dict): The current graph state</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: Binary decision for next node to call</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">search</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;search&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">search</span> <span class="o">==</span> <span class="s2">&quot;Yes&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;search&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;generate&quot;</span>


<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span>


<span class="n">workflow</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">GraphState</span><span class="p">)</span>

<span class="c1"># Define the nodes</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="n">retrieve</span><span class="p">)</span>  <span class="c1"># retrieve</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;grade_documents&quot;</span><span class="p">,</span> <span class="n">grade_documents</span><span class="p">)</span>  <span class="c1"># grade documents</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="n">generate</span><span class="p">)</span>  <span class="c1"># generatae</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;web_search&quot;</span><span class="p">,</span> <span class="n">web_search</span><span class="p">)</span>  <span class="c1"># web search</span>

<span class="c1"># Build graph</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="s2">&quot;grade_documents&quot;</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
    <span class="s2">&quot;grade_documents&quot;</span><span class="p">,</span>
    <span class="n">decide_to_generate</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="s2">&quot;search&quot;</span><span class="p">:</span> <span class="s2">&quot;web_search&quot;</span><span class="p">,</span>
        <span class="s2">&quot;generate&quot;</span><span class="p">:</span> <span class="s2">&quot;generate&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;web_search&quot;</span><span class="p">,</span> <span class="s2">&quot;generate&quot;</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>

<span class="c1"># Compile</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>Graph visualization</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">get_graph</span><span class="p">(</span><span class="n">xray</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>Ouput</p>
<figure class="align-center" id="id11">
<span id="fig-c-rag-graph"></span><img alt="_images/c_rag_graph.png" src="_images/c_rag_graph.png" />
<figcaption>
<p><span class="caption-text">Corrective-RAG Graph</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
<li><p>Test</p>
<ul>
<li><p>Relevant retrieval</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">uuid</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())}}</span>
<span class="n">example</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;What are the basic approaches for </span><span class="se">\</span>
<span class="s2">                    prompt engineering?&quot;</span><span class="p">}</span>

<span class="n">state_dict</span> <span class="o">=</span> <span class="n">app</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="p">[]},</span> <span class="n">config</span><span class="p">)</span>
<span class="n">state_dict</span>
</pre></div>
</div>
<p>Ouput:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">0</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">1</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">2</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">3</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="p">{</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="s1">&#39;What are the basic approaches for                      prompt engineering?&#39;</span><span class="p">,</span>
<span class="s1">&#39;generation&#39;</span><span class="p">:</span> <span class="s1">&#39;{</span><span class="se">\n</span><span class="s1">       &quot;Basic Prompting&quot; : &quot;A basic approach for prompt engineering is to provide clear and concise instructions to the language model, guiding it towards the desired output.&quot;</span><span class="se">\n</span><span class="s1">    }&#39;</span><span class="p">,</span>
<span class="s1">&#39;search&#39;</span><span class="p">:</span> <span class="s1">&#39;No&#39;</span><span class="p">,</span>
<span class="s1">&#39;documents&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;description&#39;</span><span class="p">:</span> <span class="s1">&#39;Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.</span><span class="se">\n</span><span class="s1">This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span> <span class="s1">&#39;source&#39;</span><span class="p">:</span> <span class="s1">&#39;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s2">&quot;Prompt Engineering | Lil&#39;Log&quot;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s1">&#39;Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.</span><span class="se">\n</span><span class="s1">This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.</span><span class="se">\n</span><span class="s1">[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.</span><span class="se">\n</span><span class="s1">Basic Prompting#&#39;</span><span class="p">),</span>
  <span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;description&#39;</span><span class="p">:</span> <span class="s1">&#39;Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.</span><span class="se">\n</span><span class="s1">This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span> <span class="s1">&#39;source&#39;</span><span class="p">:</span> <span class="s1">&#39;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s2">&quot;Prompt Engineering | Lil&#39;Log&quot;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s1">&#39;Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.</span><span class="se">\n</span><span class="s1">This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.</span><span class="se">\n</span><span class="s1">[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.</span><span class="se">\n</span><span class="s1">Basic Prompting#&#39;</span><span class="p">),</span>
  <span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;description&#39;</span><span class="p">:</span> <span class="s1">&#39;Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.</span><span class="se">\n</span><span class="s1">This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span> <span class="s1">&#39;source&#39;</span><span class="p">:</span> <span class="s1">&#39;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s2">&quot;Prompt Engineering | Lil&#39;Log&quot;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s1">&#39;Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.</span><span class="se">\n</span><span class="s1">This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.</span><span class="se">\n</span><span class="s1">[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.</span><span class="se">\n</span><span class="s1">Basic Prompting#&#39;</span><span class="p">),</span>
  <span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;description&#39;</span><span class="p">:</span> <span class="s1">&#39;Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.</span><span class="se">\n</span><span class="s1">This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span> <span class="s1">&#39;source&#39;</span><span class="p">:</span> <span class="s1">&#39;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s2">&quot;Prompt Engineering | Lil&#39;Log&quot;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Prompt Engineering | Lil&#39;Log</span><span class="se">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</span><span class="s2">Lil&#39;Log</span><span class="se">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</span><span class="s2">|</span><span class="se">\n\n\n\n\n\n\n</span><span class="s2">Posts</span><span class="se">\n\n\n\n\n</span><span class="s2">Archive</span><span class="se">\n\n\n\n\n</span><span class="s2">Search</span><span class="se">\n\n\n\n\n</span><span class="s2">Tags</span><span class="se">\n\n\n\n\n</span><span class="s2">FAQ</span><span class="se">\n\n\n\n\n</span><span class="s2">emojisearch.app</span><span class="se">\n\n\n\n\n\n\n\n\n\n</span><span class="s2">      Prompt Engineering</span><span class="se">\n</span><span class="s2">    </span><span class="se">\n</span><span class="s2">Date: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng</span><span class="se">\n\n\n</span><span class="s2"> </span><span class="se">\n\n\n</span><span class="s2">Table of Contents</span><span class="se">\n\n\n\n</span><span class="s2">Basic Prompting</span><span class="se">\n\n</span><span class="s2">Zero-Shot</span><span class="se">\n\n</span><span class="s2">Few-shot</span><span class="se">\n\n</span><span class="s2">Tips for Example Selection</span><span class="se">\n\n</span><span class="s2">Tips for Example Ordering</span><span class="se">\n\n\n\n</span><span class="s2">Instruction Prompting</span><span class="se">\n\n</span><span class="s2">Self-Consistency Sampling</span><span class="se">\n\n</span><span class="s2">Chain-of-Thought (CoT)</span><span class="se">\n\n</span><span class="s2">Types of CoT prompts</span><span class="se">\n\n</span><span class="s2">Tips and Extensions</span><span class="se">\n\n\n</span><span class="s2">Automatic Prompt Design</span><span class="se">\n\n</span><span class="s2">Augmented Language Models</span><span class="se">\n\n</span><span class="s2">Retrieval</span><span class="se">\n\n</span><span class="s2">Programming Language</span><span class="se">\n\n</span><span class="s2">External APIs</span><span class="se">\n\n\n</span><span class="s2">Citation</span><span class="se">\n\n</span><span class="s2">Useful Resources</span><span class="se">\n\n</span><span class="s2">References&quot;</span><span class="p">)],</span>
<span class="s1">&#39;steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;retrieve_documents&#39;</span><span class="p">,</span>
  <span class="s1">&#39;grade_document_retrieval&#39;</span><span class="p">,</span>
  <span class="s1">&#39;generate_answer&#39;</span><span class="p">]}</span>
</pre></div>
</div>
</li>
<li><p>Irrelevant retrieval</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">example</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of China?&quot;</span><span class="p">}</span>
<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())}}</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">app</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">example</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span> <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="p">[]},</span> <span class="n">config</span><span class="p">)</span>
<span class="n">state_dict</span>
</pre></div>
</div>
<p>Ouput:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">0</span> <span class="n">ISN</span><span class="s1">&#39;T RELEVANT---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">1</span> <span class="n">ISN</span><span class="s1">&#39;T RELEVANT---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">2</span> <span class="n">ISN</span><span class="s1">&#39;T RELEVANT---</span>
<span class="o">---</span><span class="n">GRADE</span><span class="p">:</span> <span class="n">DOCUMENT</span> <span class="mi">3</span> <span class="n">ISN</span><span class="s1">&#39;T RELEVANT---</span>
<span class="p">{</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="s1">&#39;What is the capital of China?&#39;</span><span class="p">,</span>
<span class="s1">&#39;generation&#39;</span><span class="p">:</span> <span class="s1">&#39;{</span><span class="se">\n</span><span class="s1">      &quot;answer&quot;: &quot;Beijing is the capital of China.&quot;</span><span class="se">\n</span><span class="s1">    }&#39;</span><span class="p">,</span>
<span class="s1">&#39;search&#39;</span><span class="p">:</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span>
<span class="s1">&#39;documents&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="s1">&#39;https://clintonwhitehouse3.archives.gov/WH/New/China/beijing.html&#39;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s1">&#39;The modern day capital of China is Beijing (literally &quot;Northern Capital&quot;), which first served as China</span><span class="se">\&#39;</span><span class="s1">s capital city in 1261, when the Mongol ruler Kublai</span><span class="se">\xa0</span><span class="s1">...&#39;</span><span class="p">),</span>
  <span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="s1">&#39;https://en.wikipedia.org/wiki/Beijing&#39;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Beijing, previously romanized as Peking, is the capital city of China. With more than 22 million residents, it is the world&#39;s most populous national capital</span><span class="se">\xa0</span><span class="s2">...&quot;</span><span class="p">),</span>
  <span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="s1">&#39;https://pubmed.ncbi.nlm.nih.gov/38294063/&#39;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s1">&#39;Supercritical and homogenous transmission of monkeypox in the capital of China. J Med Virol. 2024 Feb;96(2):e29442. doi: 10.1002/jmv.29442. Authors. Yunjun</span><span class="se">\xa0</span><span class="s1">...&#39;</span><span class="p">),</span>
  <span class="n">Document</span><span class="p">(</span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="s1">&#39;https://www.sciencedirect.com/science/article/pii/S0304387820301358&#39;</span><span class="p">},</span> <span class="n">page_content</span><span class="o">=</span><span class="s1">&#39;This paper investigates the impacts of fires on cognitive performance. We find that a one-standard-deviation increase in the difference between upwind and</span><span class="se">\xa0</span><span class="s1">...&#39;</span><span class="p">)],</span>
<span class="s1">&#39;steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;retrieve_documents&#39;</span><span class="p">,</span>
  <span class="s1">&#39;grade_document_retrieval&#39;</span><span class="p">,</span>
  <span class="s1">&#39;web_search&#39;</span><span class="p">,</span>
  <span class="s1">&#39;generate_answer&#39;</span><span class="p">]}</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="adaptive-rag">
<h3><span class="section-number">5.5.3. </span>Adaptive RAG<a class="headerlink" href="#adaptive-rag" title="Link to this heading"></a></h3>
<figure class="align-center" id="id12">
<span id="fig-a-rag"></span><img alt="_images/adaptive_rag.png" src="_images/adaptive_rag.png" />
<figcaption>
<p><span class="caption-text">Adaptive-RAG langgraph diagram (source <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/">Langgraph A-rag</a>)</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="agentic-rag">
<h3><span class="section-number">5.5.4. </span>Agentic RAG<a class="headerlink" href="#agentic-rag" title="Link to this heading"></a></h3>
<figure class="align-center" id="id13">
<span id="fig-agentic-rag"></span><img alt="_images/agentic_rag.png" src="_images/agentic_rag.png" />
<figcaption>
<p><span class="caption-text">Agentic-RAG langgraph diagram (source <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/">Langgraph Agentic-rag</a>)</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<ul>
<li><p>Load Models</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>

<span class="c1"># embedding model</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">)</span>

<span class="c1"># LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;mistral&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You need to specify <code class="docutils literal notranslate"><span class="pre">format='json'</span></code> when Initializing <code class="docutils literal notranslate"><span class="pre">OllamaLLM</span></code>. otherwise
you will get error:</p>
<blockquote>
<div><figure class="align-center">
<img alt="_images/gemini.png" src="_images/gemini.png" />
</figure>
</div></blockquote>
</div>
</li>
<li><p>Create Index</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>  <span class="c1"># Import OllamaEmbeddings instead</span>


<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">WebBaseLoader</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]</span>
<span class="n">docs_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">doc_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs_list</span><span class="p">)</span>

<span class="c1"># Add to vectorDB</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">doc_splits</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;rag-chroma&quot;</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;bge-m3&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<ul>
<li><p>Retrieval Grader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Retrieval Grader</span>

<span class="kn">from</span> <span class="nn">langchain_ollama.llms</span> <span class="kn">import</span> <span class="n">OllamaLLM</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># Import from pydantic directly instead of langchain_core.pydantic_v1</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">PydanticOutputParser</span>

<span class="c1"># Data model</span>
<span class="k">class</span> <span class="nc">GradeDocuments</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary score for relevance check on retrieved documents.&quot;&quot;&quot;</span>

    <span class="n">score</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>  <span class="c1"># Changed field name to &#39;score&#39;</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Documents are relevant to the question, &#39;yes&#39; or &#39;no&#39;&quot;</span><span class="p">)</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">PydanticOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">GradeDocuments</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;You are a grader assessing relevance of a retrieved</span>
<span class="s2">    document to a user question. </span><span class="se">\n</span>
<span class="s2">    Here is the retrieved document: </span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{context}</span><span class="s2"> </span><span class="se">\n\n</span>
<span class="s2">    Here is the user question: </span><span class="si">{question}</span><span class="s2"> </span><span class="se">\n</span>
<span class="s2">    If the document contains keywords related to the user question,</span>
<span class="s2">    grade it as relevant. </span><span class="se">\n</span>
<span class="s2">    It does not need to be a stringent test. The goal is to filter out</span>
<span class="s2">    erroneous retrievals. </span><span class="se">\n</span>
<span class="s2">    Give a binary score &#39;yes&#39; or &#39;no&#39; score to indicate whether the document</span>
<span class="s2">    is relevant to the question. </span><span class="se">\n</span>
<span class="s2">    Provide the binary score as a JSON with a single key &#39;score&#39; and no</span>
<span class="s2">    premable or explanation.&quot;&quot;&quot;</span><span class="p">,</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">],</span>
    <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format_instructions&quot;</span><span class="p">:</span> <span class="n">parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()}</span>
<span class="p">)</span>

<span class="n">retrieval_grader</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>
<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;agent memory&quot;</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="n">doc_txt</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span>
<span class="n">retrieval_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">doc_txt</span><span class="p">})</span>
</pre></div>
</div>
<p>Ouput:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GradeDocuments</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Agent State</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>

<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">BaseMessage</span>

<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>


<span class="k">class</span> <span class="nc">AgentState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="c1"># The add_messages function defines how an update should be processed</span>
    <span class="c1"># Default is to replace. add_messages says &quot;append&quot;</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span> <span class="n">add_messages</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>Create the Graph</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>

<span class="kn">from</span> <span class="nn">langchain</span> <span class="kn">import</span> <span class="n">hub</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">BaseMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>


<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">langchain_experimental.llms.ollama_functions</span> <span class="kn">import</span> <span class="n">OllamaFunctions</span>



<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">tools_condition</span>

<span class="c1">### Edges</span>


<span class="k">def</span> <span class="nf">grade_documents</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="s2">&quot;rewrite&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the retrieved documents are relevant to the question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (messages): The current state</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: A decision for whether the documents are relevant or not</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---CHECK RELEVANCE---&quot;</span><span class="p">)</span>

    <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>
    <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">last_message</span><span class="o">.</span><span class="n">content</span>

    <span class="n">scored_result</span> <span class="o">=</span> <span class="n">retrieval_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> \
                                            <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">})</span>

    <span class="n">score</span> <span class="o">=</span> <span class="n">scored_result</span><span class="o">.</span><span class="n">score</span>

    <span class="k">if</span> <span class="n">score</span> <span class="o">==</span> <span class="s2">&quot;yes&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---DECISION: DOCS RELEVANT---&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;generate&quot;</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---DECISION: DOCS NOT RELEVANT---&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;rewrite&quot;</span>


<span class="c1">### Nodes</span>


<span class="k">def</span> <span class="nf">agent</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Invokes the agent model to generate a response based on the current state.</span>
<span class="sd">    Given the question, it will decide to retrieve using the retriever tool,</span>
<span class="sd">    or simply end.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (messages): The current state</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: The updated state with the agent response appended to messages</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---CALL AGENT---&quot;</span><span class="p">)</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">OllamaFunctions</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="c1"># We return a list, because this will get added to the existing list</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>


<span class="k">def</span> <span class="nf">rewrite</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform the query to produce a better question.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (messages): The current state</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: The updated state with re-phrased question</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---TRANSFORM QUERY---&quot;</span><span class="p">)</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">HumanMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot; </span><span class="se">\n</span>
<span class="s2">    Look at the input and try to reason about the underlying semantic intent /</span>
<span class="s2">    meaning. </span><span class="se">\n</span>
<span class="s2">    Here is the initial question:</span>
<span class="s2">    </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">    </span><span class="si">{</span><span class="n">question</span><span class="si">}</span>
<span class="s2">    </span><span class="se">\n</span><span class="s2"> ------- </span><span class="se">\n</span>
<span class="s2">    Formulate an improved question: &quot;&quot;&quot;</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Grader</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>


<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate answer</span>

<span class="sd">    Args:</span>
<span class="sd">        state (messages): The current state</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: The updated state with re-phrased question</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---GENERATE---&quot;</span><span class="p">)</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>
    <span class="n">question</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
    <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">docs</span> <span class="o">=</span> <span class="n">last_message</span><span class="o">.</span><span class="n">content</span>

    <span class="c1"># Prompt</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="s2">&quot;rlm/rag-prompt&quot;</span><span class="p">)</span>


    <span class="c1"># Post-processing</span>
    <span class="k">def</span> <span class="nf">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>

    <span class="c1"># Chain</span>
    <span class="n">rag_chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

    <span class="c1"># Run</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]}</span>


<span class="c1"># print(&quot;*&quot; * 20 + &quot;Prompt[rlm/rag-prompt]&quot; + &quot;*&quot; * 20)</span>
<span class="c1"># # Show what the prompt looks like</span>
<span class="c1"># prompt = hub.pull(&quot;rlm/rag-prompt&quot;).pretty_print()</span>


<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span>
<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span>

<span class="c1"># Define a new graph</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">AgentState</span><span class="p">)</span>

<span class="c1"># Define the nodes we will cycle between</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;agent&quot;</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>  <span class="c1"># agent</span>
<span class="n">retrieve</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">([</span><span class="n">retriever_tool</span><span class="p">])</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="n">retrieve</span><span class="p">)</span>  <span class="c1"># retrieval</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;rewrite&quot;</span><span class="p">,</span> <span class="n">rewrite</span><span class="p">)</span>  <span class="c1"># Re-writing the question</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span>
    <span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="n">generate</span>
<span class="p">)</span>  <span class="c1"># Generating a response after we know the documents are relevant</span>
<span class="c1"># Call agent node to decide to retrieve or not</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;agent&quot;</span><span class="p">)</span>

<span class="c1"># Decide whether to retrieve</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
    <span class="s2">&quot;agent&quot;</span><span class="p">,</span>
    <span class="c1"># Assess agent decision</span>
    <span class="n">tools_condition</span><span class="p">,</span>
    <span class="p">{</span>
        <span class="c1"># Translate the condition outputs to nodes in our graph</span>
        <span class="s2">&quot;tools&quot;</span><span class="p">:</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">,</span>
        <span class="n">END</span><span class="p">:</span> <span class="n">END</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># Edges taken after the `action` node is called.</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
    <span class="s2">&quot;retrieve&quot;</span><span class="p">,</span>
    <span class="c1"># Assess agent decision</span>
    <span class="n">grade_documents</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;rewrite&quot;</span><span class="p">,</span> <span class="s2">&quot;agent&quot;</span><span class="p">)</span>

<span class="c1"># Compile</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">OllamaLLM</span></code> object has no attribute <code class="docutils literal notranslate"><span class="pre">bind_tools</span></code>. You need to Install <code class="docutils literal notranslate"><span class="pre">langchain-experimental</span></code>:
OllamaFunctions is initialized with the desired model name:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">OllamaFunctions</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</pre></div>
</div>
<p>If you use OpenAI model, the code should be like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</pre></div>
</div>
</div>
</li>
<li><p>Graph visualization</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">get_graph</span><span class="p">(</span><span class="n">xray</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>Ouput</p>
<figure class="align-center" id="id14">
<span id="fig-agentic-rag-graph"></span><img alt="_images/agentic_rag_graph.png" src="_images/agentic_rag_graph.png" />
<figcaption>
<p><span class="caption-text">Agentic-RAG Graph</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
<li><p>Test</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pprint</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;What does Lilian Weng say about the types of agent memory?&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output from node &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39;:&quot;</span><span class="p">)</span>
        <span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;---&quot;</span><span class="p">)</span>
        <span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Ouput:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">---</span><span class="n">CALL</span> <span class="n">AGENT</span><span class="o">---</span>
<span class="s2">&quot;Output from node &#39;agent&#39;:&quot;</span>
<span class="s1">&#39;---&#39;</span>
<span class="p">{</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[</span> <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">additional_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">response_metadata</span><span class="o">=</span><span class="p">{},</span> <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;run-ba7e9f54-7b32-44be-a39b-b083a6db462d-0&#39;</span><span class="p">,</span> <span class="n">tool_calls</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;retrieve_blog_posts&#39;</span><span class="p">,</span> <span class="s1">&#39;args&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;query&#39;</span><span class="p">:</span> <span class="s1">&#39;types of agent memory&#39;</span><span class="p">},</span> <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="s1">&#39;call_4b1ac43a51c545cb942498b35321693a&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;tool_call&#39;</span><span class="p">}])]}</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">CHECK</span> <span class="n">RELEVANCE</span><span class="o">---</span>
<span class="o">---</span><span class="n">DECISION</span><span class="p">:</span> <span class="n">DOCS</span> <span class="n">RELEVANT</span><span class="o">---</span>
<span class="s2">&quot;Output from node &#39;retrieve&#39;:&quot;</span>
<span class="s1">&#39;---&#39;</span>
<span class="p">{</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[</span> <span class="n">ToolMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for &quot;dark&quot; environments and DQN for watermaze.(Image source: Laskin et al. 2023)</span><span class="se">\n</span><span class="s1">Component Two: Memory#</span><span class="se">\n</span><span class="s1">(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)</span><span class="se">\n</span><span class="s1">Types of Memory#</span><span class="se">\n</span><span class="s1">Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.</span><span class="se">\n\n\n</span><span class="s1">Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).</span><span class="se">\n\n</span><span class="s1">Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.</span><span class="se">\n</span><span class="s1">Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.</span><span class="se">\n\n\n</span><span class="s1">Tool use</span><span class="se">\n\n</span><span class="s1">The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.</span><span class="se">\n\n</span><span class="s1">Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;</span><span class="se">\n</span><span class="s1">Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.</span><span class="se">\n</span><span class="s1">Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.</span><span class="se">\n\n</span><span class="s1">Maximum Inner Product Search (MIPS)#</span><span class="se">\n</span><span class="s1">The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)</span><span class="se">\u200b</span><span class="s1"> algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.</span><span class="se">\n</span><span class="s1">A couple common choices of ANN algorithms for fast MIPS:</span><span class="se">\n\n</span><span class="s1">They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.</span><span class="se">\n</span><span class="s1">Generative Agents Simulation#</span><span class="se">\n</span><span class="s1">Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.</span><span class="se">\n</span><span class="s1">The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.</span><span class="se">\n\n</span><span class="s1">Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;retrieve_blog_posts&#39;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;2ca2d54b-214b-4463-a491-20c8d08e79cc&#39;</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="s1">&#39;call_4b1ac43a51c545cb942498b35321693a&#39;</span><span class="p">)]}</span>
<span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">---</span><span class="n">GENERATE</span><span class="o">---</span>
<span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.10</span><span class="o">/</span><span class="n">dist</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">langsmith</span><span class="o">/</span><span class="n">client</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">261</span><span class="p">:</span> <span class="n">LangSmithMissingAPIKeyWarning</span><span class="p">:</span> <span class="n">API</span> <span class="n">key</span> <span class="n">must</span> <span class="n">be</span> <span class="n">provided</span> <span class="n">when</span> <span class="n">using</span> <span class="n">hosted</span> <span class="n">LangSmith</span> <span class="n">API</span>
  <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<span class="s2">&quot;Output from node &#39;generate&#39;:&quot;</span>
<span class="s1">&#39;---&#39;</span>
<span class="p">{</span> <span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[</span> <span class="s1">&#39;{</span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="s1">&#39;   &quot;Lilian Weng describes three types of memory: Sensory &#39;</span>
                <span class="s1">&#39;Memory, Short-Term Memory, and Long-Term Memory. Sensory &#39;</span>
                <span class="s1">&#39;Memory is the earliest stage, lasting for up to a few &#39;</span>
                <span class="s1">&#39;seconds, and includes iconic (visual), echoic (auditory), and &#39;</span>
                <span class="s1">&#39;haptic memory. Short-Term Memory is used for in-context &#39;</span>
                <span class="s1">&#39;learning and is finite due to the limited context window &#39;</span>
                <span class="s1">&#39;length of Transformer. Long-Term Memory provides agents with &#39;</span>
                <span class="s1">&#39;the capability to retain and recall information over extended &#39;</span>
                <span class="s1">&#39;periods by leveraging an external vector store and fast &#39;</span>
                <span class="s2">&quot;retrieval.}&#39;{: .language-json }. In the given context, it &quot;</span>
                <span class="s1">&#39;does not explicitly mention any specific agent memory types &#39;</span>
                <span class="s1">&#39;related to reinforcement learning or simulation experiments. &#39;</span>
                <span class="s1">&#39;For those topics, you may want to refer to the sections on &quot;</span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="s1">&#39; </span><span class="se">\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span><span class="s1">&#39;</span><span class="p">]}</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="prompt.html" class="btn btn-neutral float-left" title="4. Prompt Engineering" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="finetuning.html" class="btn btn-neutral float-right" title="6. Fine Tuning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Wenqiang Feng, Di Zhen and Wenyun Wang.
      <span class="lastupdated">Last updated on Dec 21, 2024.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>