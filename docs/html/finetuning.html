

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>6. Fine Tuning &mdash; GenAI: Best Practices 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />

  
    <link rel="shortcut icon" href="_static/icon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Pre-training" href="pretraining.html" />
    <link rel="prev" title="5. Retrieval-Augmented Generation" href="rag.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            GenAI: Best Practices
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preface.html">1. Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="prelim.html">2. Preliminary</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedding.html">3. Word and Sentence Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompt.html">4. Prompt Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">5. Retrieval-Augmented Generation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. Fine Tuning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#cutting-edge-strategies-for-llm-fine-tuning">6.1. Cutting-Edge Strategies for LLM Fine-Tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lora-low-rank-adaptation">6.1.1. LoRA (Low-Rank Adaptation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#qlora-quantized-low-rank-adaptation">6.1.2. QLoRA (Quantized Low-Rank Adaptation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#peft-parameter-efficient-fine-tuning">6.1.3. PEFT (Parameter-Efficient Fine-Tuning)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sft-supervised-fine-tuning">6.1.4. SFT (Supervised Fine-Tuning)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rlhf-reinforcement-learning-from-human-feedback">6.1.5. RLHF (Reinforcement Learning from Human Feedback)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ppo">6.1.5.1. PPO</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dpo">6.1.5.2. DPO</a></li>
<li class="toctree-l4"><a class="reference internal" href="#main-difficulties-in-rlhf">6.1.5.3. Main Difficulties in RLHF</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#summary-table">6.1.6. Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#key-early-fine-tuning-methods">6.2. Key Early Fine-Tuning Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#full-fine-tuning">6.2.1. Full Fine-Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feature-based-approach">6.2.2. Feature-Based Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="#layer-specific-fine-tuning">6.2.3. Layer-Specific Fine-Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-adaptive-pre-training">6.2.4. Task-Adaptive Pre-training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#embedding-model-fine-tuning">6.3. Embedding Model Fine-Tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prepare-dataset">6.3.1. Prepare Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#import-and-evaluate-pretrained-baseline-model">6.3.2. Import and Evaluate Pretrained Baseline Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loss-function-with-matryoshka-representation">6.3.3. Loss Function with Matryoshka Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tune-embedding-model">6.3.4. Fine-tune Embedding Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluate-fine-tuned-model">6.3.5. Evaluate Fine-tuned Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#results-comparison">6.3.6. Results Comparison</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#llm-fine-tuning">6.4. LLM Fine-Tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#load-dataset-and-pretrained-model">6.4.1. Load Dataset and Pretrained Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-configuration">6.4.2. Fine-tuning Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tune-model">6.4.3. Fine-tune model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pretraining.html">7. Pre-training</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">8. LLM Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="guardrails.html">9. LLM Guardrails</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">10. Main Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GenAI: Best Practices</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><span class="section-number">6. </span>Fine Tuning</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="fine-tuning">
<span id="finetuning"></span><h1><span class="section-number">6. </span>Fine Tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading"></a></h1>
<div class="admonition-chinese-proverb admonition">
<p class="admonition-title">Chinese proverb</p>
<p>Good tools are prerequisite to the successful execution of a job. – old Chinese proverb</p>
</div>
<div class="admonition-colab-notebook-for-this-chapter admonition">
<p class="admonition-title">Colab Notebook for This Chapter</p>
<ul class="simple">
<li><p>Embedding Model Fine-tuning: <a class="reference external" href="https://colab.research.google.com/drive/14aYT8Ydm_e-z47yGpctAfk246K_PK1LC?usp=drive_link"><img alt="e_tune" src="_images/colab-badge.png" /></a></p></li>
<li><p>LLM (Llama 2 7B) Model Fine-tuning: <a class="reference external" href="https://colab.research.google.com/drive/1GPu2vNRdcObf0dmP7r_M42NYx0OXVD_F?usp=drive_link"><img alt="l_tune" src="_images/colab-badge.png" /></a></p></li>
</ul>
</div>
<p>Fine-tuning is a machine learning technique where a pre-trained model (like a large
language model or neural network) is further trained on a smaller, specific dataset
to adapt it to a particular task or domain. Instead of training a model from scratch,
fine-tuning leverages the knowledge already embedded in the pre-trained model,
saving time, computational resources, and data requirements.</p>
<figure class="align-center" id="id9">
<span id="fig-fine-tuning"></span><img alt="_images/fine_tuning.png" src="_images/fine_tuning.png" />
<figcaption>
<p><span class="caption-text">The three conventional feature-based and finetuning approaches (Souce <a class="reference external" href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">Finetuning Sebastian</a>).</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="cutting-edge-strategies-for-llm-fine-tuning">
<h2><span class="section-number">6.1. </span>Cutting-Edge Strategies for LLM Fine-Tuning<a class="headerlink" href="#cutting-edge-strategies-for-llm-fine-tuning" title="Link to this heading"></a></h2>
<p>Over the past year, fine-tuning methods have made remarkable strides. Modern methods
for fine-tuning LLMs focus on efficiency, scalability, and resource optimization.
The following strategies are at the forefront:</p>
<section id="lora-low-rank-adaptation">
<h3><span class="section-number">6.1.1. </span>LoRA (Low-Rank Adaptation)<a class="headerlink" href="#lora-low-rank-adaptation" title="Link to this heading"></a></h3>
<p><strong>LoRA</strong> reduces the number of trainable parameters by introducing <strong>low-rank decomposition</strong> into the fine-tuning process.</p>
<figure class="align-center" id="id10">
<span id="fig-lora"></span><img alt="_images/lora.png" src="_images/lora.png" />
<figcaption>
<p><span class="caption-text">Weight update matrix (Souce <a class="reference external" href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">LORA Sebastian</a>).</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>Instead of updating all model weights, LoRA injects <strong>low-rank adapters</strong> into the model’s layers.</p></li>
<li><p>The original pre-trained weights remain frozen; only the low-rank parameters are optimized.</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Reduces memory and computational requirements.</p></li>
<li><p>Enables fine-tuning on resource-constrained hardware.</p></li>
</ul>
</section>
<section id="qlora-quantized-low-rank-adaptation">
<h3><span class="section-number">6.1.2. </span>QLoRA (Quantized Low-Rank Adaptation)<a class="headerlink" href="#qlora-quantized-low-rank-adaptation" title="Link to this heading"></a></h3>
<p><strong>QLoRA</strong> combines <strong>low-rank adaptation</strong> with <strong>4-bit quantization</strong> of the pre-trained model.</p>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>The LLM is quantized to <strong>4-bit precision</strong> to reduce memory usage.</p></li>
<li><p>LoRA adapters are applied to the quantized model for fine-tuning.</p></li>
<li><p>Precision is maintained using methods like <strong>NF4 (Normalized Float 4)</strong> and double backpropagation.</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Further reduces memory usage compared to LoRA.</p></li>
<li><p>Enables fine-tuning of massive models on consumer-grade GPUs.</p></li>
</ul>
</section>
<section id="peft-parameter-efficient-fine-tuning">
<h3><span class="section-number">6.1.3. </span>PEFT (Parameter-Efficient Fine-Tuning)<a class="headerlink" href="#peft-parameter-efficient-fine-tuning" title="Link to this heading"></a></h3>
<p><strong>PEFT</strong> is a general framework for fine-tuning LLMs with minimal trainable parameters.</p>
<table class="borderless docutils align-default" style="width: 100%">
<tbody>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/peft_1.png"><img alt="_images/peft_1.png" src="_images/peft_1.png" style="width: 100%;" />
</a>
</td>
<td><a class="reference internal image-reference" href="_images/peft_2.png"><img alt="_images/peft_2.png" src="_images/peft_2.png" style="width: 100%;" />
</a>
</td>
</tr>
</tbody>
</table>
<p>Source: <a class="reference internal" href="reference.html#peft" id="id1"><span>[PEFT]</span></a></p>
<p><strong>Techniques Under PEFT</strong>:</p>
<ul class="simple">
<li><p><strong>LoRA</strong>: Low-rank adaptation of weights.</p></li>
<li><p><strong>Adapters</strong>: Small trainable layers inserted into the model.</p></li>
<li><p><strong>Prefix Tuning</strong>: Fine-tuning input prefixes instead of weights.</p></li>
<li><p><strong>Prompt Tuning</strong>: Optimizing soft prompts in the input space.</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Reduces the number of trainable parameters.</p></li>
<li><p>Faster training and lower hardware requirements.</p></li>
</ul>
</section>
<section id="sft-supervised-fine-tuning">
<h3><span class="section-number">6.1.4. </span>SFT (Supervised Fine-Tuning)<a class="headerlink" href="#sft-supervised-fine-tuning" title="Link to this heading"></a></h3>
<p><strong>SFT</strong> adapts an LLM using a labeled dataset in a fully supervised manner.</p>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>The model is initialized with pre-trained weights.</p></li>
<li><p>It is fine-tuned on a task-specific dataset with a supervised loss function (e.g., cross-entropy).</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Achieves high performance on specific tasks.</p></li>
<li><p>Essential for aligning models with labeled datasets.</p></li>
</ul>
</section>
<section id="rlhf-reinforcement-learning-from-human-feedback">
<h3><span class="section-number">6.1.5. </span>RLHF (Reinforcement Learning from Human Feedback)<a class="headerlink" href="#rlhf-reinforcement-learning-from-human-feedback" title="Link to this heading"></a></h3>
<p><strong>RLHF</strong> is a technique used to fine-tune language models, aligning their
behavior with human preferences or specific tasks. RLHF incorporates feedback
from humans to guide the model’s learning process, ensuring that its outputs
are not only coherent but also align with desired ethical, practical, or
stylistic goals.</p>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>The model is initialized with pre-trained weights.</p></li>
<li><p>The pretrained model is fine-tuned further using reinforcement learning,
guided by the reward model.</p></li>
<li><p>A reinforcement learning algorithm, such as Proximal Policy Optimization
(PPO), optimizes the model to maximize the reward assigned by the reward model.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li><dl class="simple">
<dt><strong>Direct Preference Optimization</strong></dt><dd><p>DPO is a technique for aligning large
language models (LLMs) with human preferences, offering an alternative
to the traditional Reinforcement Learning from Human Feedback (RLHF)
approach that uses Proximal Policy Optimization (PPO). Instead of
training a separate reward model and using reinforcement learning,
DPO simplifies the process by directly leveraging human preference
data to fine-tune the model through supervised learning.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Proximal Policy Optimization</strong></dt><dd><p>PPO is a reinforcement learning algorithm
commonly used in RLHF to fine-tune LLMs. PPO optimizes the model’s policy
by maximizing the reward signal provided by a reward model, which
represents human preferences.</p>
</dd>
</dl>
</li>
<li><p><strong>Comparison: DPO vs PPO</strong></p>
<blockquote>
<div><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Feature</strong></p></th>
<th class="head"><p><strong>DPO</strong></p></th>
<th class="head"><p><strong>PPO</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Training Paradigm</strong></p></td>
<td><p>Supervised fine-tuning with preferences</p></td>
<td><p>Reinforcement learning with a reward
model</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Workflow Complexity</strong></p></td>
<td><p>Simpler</p></td>
<td><p>More complex (requires reward model
and iterative RL)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Stability</strong></p></td>
<td><p>More stable (uses supervised learning)</p></td>
<td><p>Less stable (inherent to RL methods)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Efficiency</strong></p></td>
<td><p>Computationally efficient</p></td>
<td><p>Computationally intensive</p></td>
</tr>
<tr class="row-even"><td><p><strong>Scalability</strong></p></td>
<td><p>Scales well with large preference
datasets</p></td>
<td><p>Requires significant compute for RL
steps</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Use Case</strong></p></td>
<td><p>Directly aligns LLM with preferences</p></td>
<td><p>Optimizes policy for long-term reward
maximization</p></td>
</tr>
<tr class="row-even"><td><p><strong>Human Preference
Modeling</strong></p></td>
<td><p>Directly encoded in loss function</p></td>
<td><p>Encoded via a reward model</p></td>
</tr>
</tbody>
</table>
</div></blockquote>
</li>
</ul>
</div>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>RLHF ensures the model’s outputs are ethical, safe, and aligned with human
expectations, reducing harmful or biased content.</p></li>
<li><p>Responses become more relevant, helpful, and contextually appropriate,
enhancing user experience.</p></li>
<li><p>Fine-tuning with RLHF allows models to be customized for specific use cases,
such as customer service, creative writing, or technical support.</p></li>
</ul>
<p>The process of training a model using reinforcement learning from human
feedback (RLHF) involves three key steps, as outlined in the paper
titled “<a class="reference external" href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human
feedback</a>” by OpenAI <a class="reference internal" href="reference.html#longouyang" id="id2"><span>[LongOuyang]</span></a>.</p>
<figure class="align-center" id="id11">
<img alt="instructGPT_overview_RLHF" src="_images/instructGPT_overview_RLHF.png" />
<figcaption>
<p><span class="caption-text">InstructGPT Overview (Source: <a class="reference external" href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a>)</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="ppo">
<h4><span class="section-number">6.1.5.1. </span>PPO<a class="headerlink" href="#ppo" title="Link to this heading"></a></h4>
<p>Proximal Policy Optimization (PPO) (Paper: <a class="reference external" href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization
Algorithms</a>) is a key algorithm
used in RLHF to fine-tune language models based on human preferences. It
is utilized to optimize the policy of a language model by maximizing a
reward function derived from human feedback. This process helps align
the model’s outputs with human values and preferences. <a class="reference internal" href="reference.html#johnschulman" id="id3"><span>[JohnSchulman]</span></a></p>
<p><strong>State, Action, and Reward in the Context of LLMs</strong></p>
<p>In the context of LLMs, the components of reinforcement learning are
defined as follows:</p>
<ol class="arabic simple">
<li><p><strong>State</strong>: The state corresponds to the <strong>input prompt</strong> or context
provided to the language model. It represents the scenario or query
that requires a response.</p></li>
<li><p><strong>Action</strong>: The action is the <strong>output</strong> generated by the language
model, i.e., the response or continuation of text based on the given
state (prompt).</p></li>
<li><p><strong>Reward</strong>: The reward is a scalar value that quantifies how well the
generated response aligns with human preferences or task objectives.
It is typically derived from a <strong>reward model</strong> trained on human
feedback.</p></li>
<li><p><strong>Policy</strong>: A policy refers to the strategy or function that maps a
given state (input prompt and context) to an action (the next token
or sequence of tokens to generate). The policy governs how the LLM
generates responses and is optimized to maximize a reward signal,
such as alignment with human preferences or task-specific objectives.</p></li>
</ol>
<p><strong>Proximal Policy Optimization (PPO)</strong> is a reinforcement learning
algorithm designed to optimize the policy of an agent in a stable and
efficient manner. It is particularly effective in environments with
discrete or continuous action spaces. Here’s an overview of PPO along
with its objective function:</p>
<p><strong>PPO Objective Function</strong></p>
<p>PPO algorithm extends the CLIP objective by incorporating additional
terms for value function optimization and entropy regularization.</p>
<div class="math notranslate nohighlight">
\[J^{PPO}(\theta) = E[J^{CLIP}(\theta) - c_1(V_\theta(s)-V_{target})^2 + c_2 H(s,\pi_\theta(\cdot))]\]</div>
<p>where</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(J^{CLIP}(\theta)\)</span> is CLIP objective in policy gradient methods.
The use of the minimum function ensures that if the new policy’s
probability ratio deviates too much from 1 (indicating a significant
change), it will not receive excessive credit (or blame) for its
performance based on the advantage estimate.</p>
<div class="math notranslate nohighlight">
\[J^{CLIP}(\theta) = E[\min(r(\theta)\hat{A}_{\theta_{old}}(s,a)), \text{clip}(r(\theta),1-\epsilon, 1+\epsilon) \hat{A}_{\theta_{old}}(s,a)]\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(-(V_\theta(s) - V_{target})^2\)</span> is the negative mean squared
error (MSE), which we aim to maximize. It minimizes the difference
between the predicted value function <span class="math notranslate nohighlight">\(V_\theta(s)\)</span> and the
target value <span class="math notranslate nohighlight">\(V_{target}\)</span>. The coefficient <span class="math notranslate nohighlight">\(c_2\)</span> controls
the tradeoff between policy optimization and value function fitting.</p></li>
<li><p><span class="math notranslate nohighlight">\(H(s,\pi_\theta(\cdot))\)</span> represents the entropy of the policy.
Maximizing entropy encourages exploration by preventing premature
convergence to deterministic policies. The coefficient <span class="math notranslate nohighlight">\(c_2\)</span>
determines the weight of this entropy term.</p></li>
</ul>
<p>Below is a pseudocode of PPO-Clip Algorithm</p>
<figure class="align-center" id="id12">
<img alt="ppo_clip_algo" src="_images/ppo_clip_algo.png" />
<figcaption>
<p><span class="caption-text">PPO Clip Algorithm (Source: <a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">OpenAI Spinning Up - Proximal Policy Optimization</a>)</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Steps of RLHF Using PPO</strong></p>
<p>The RLHF process using PPO involves three main stages:</p>
<ol class="arabic">
<li><p><strong>Training a Reward Model</strong>: A reward model is trained to predict
human preferences based on labeled data. Human annotators rank
multiple responses for each prompt, and this ranking data is used to
train the reward model in a supervised manner. The reward model
learns to assign higher scores to responses that align better with
human preferences.</p></li>
<li><p><strong>Fine-Tuning the LLM with PPO</strong>: After training the reward model,
PPO is used to fine-tune the LLM. The steps are as follows:</p>
<ol class="arabic">
<li><p><strong>Initialize Policies</strong>: Start with a pre-trained LLM as both the
<strong>policy model</strong> (actor) and optionally as the critic for value
estimation.</p>
<ul>
<li><p>The <strong>actor</strong> is the language model that generates responses
(actions) based on input prompts (states).</p>
<p>For example: Input: “Explain quantum mechanics.” Output:
“Quantum mechanics is a branch of physics that studies particles
at atomic and subatomic scales.”</p>
</li>
<li><p>The <strong>critic</strong> is typically implemented as a <strong>value function</strong>,
which predicts how good a particular response (action) is in
terms of achieving long-term objectives. This model predicts a
scalar value for each token or sequence, representing its
expected reward or usefulness.</p>
<p>For example:</p>
<p>Input: “Explain quantum mechanics.” → “Quantum mechanics is…”
Output: A value score indicating how well this response aligns
with human preferences or task objectives.</p>
</li>
<li><p>Both the actor and critic can be initialized from the same
pre-trained LLM weights to leverage shared knowledge from
pretraining. However, their roles diverge during fine-tuning:
The actor focuses on generating responses. The critic focuses on
evaluating those responses.</p></li>
</ul>
</li>
<li><p><strong>Collect Rollouts</strong>: Interact with the environment by sampling
prompts from a dataset. Generate responses (actions) using the
current policy. Compute rewards for these responses using the
trained reward model.</p></li>
<li><p><strong>Compute Advantage Estimates</strong>: Use rewards from the reward model
and value estimates from the critic to compute advantages:</p>
<div class="math notranslate nohighlight">
\[\hat{A}(s, a) = R_t + \gamma V(s_{t+1}) - V(s_t),\]</div>
<p>where $ R_t $ is the reward from the reward model.</p>
</li>
<li><p><strong>Optimize Policy with PPO Objective</strong>: Optimize the policy using
PPO’s clipped surrogate objective:</p>
<div class="math notranslate nohighlight">
\[J^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r(\theta)\hat{A}(s, a), \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)\hat{A}(s, a)\right)\right],\]</div>
<p>where $ r(theta) = frac{pi_theta(a|s)}{pi_{theta_{text{old}}}(a|s)}
$ is the probability ratio between new and old policies.</p>
</li>
<li><p><strong>Update Value Function</strong>: Simultaneously update the value
function by minimizing mean squared error between predicted values
and rewards:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{value}} = \mathbb{E}\left[(V_\theta(s) - R_t)^2\right].\]</div>
</li>
<li><p><strong>Repeat</strong>: Iterate over multiple epochs until convergence,
ensuring stable updates by clipping policy changes.</p></li>
</ol>
</li>
<li><p><strong>Evaluation</strong>: Evaluate the fine-tuned LLM on unseen prompts to
ensure it generates outputs aligned with human preferences.
Optionally, collect additional human feedback to further refine both
the reward model and policy.</p></li>
</ol>
<p>The following diagrams summarizes the high-level RLHF process with PPO,
from preference data creation, to training a reward model, and using
reward model in an RL loop to fine tune LLM.</p>
<figure class="align-center" id="id13">
<img alt="PPO_RLHF_flowchart" src="_images/PPO_RLHF_flowchart.png" />
<figcaption>
<p><span class="caption-text">Flowchart of PPO in RLHF</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The following workflow chart illustrates the more detailed training
process of RLHF with PPO. <a class="reference internal" href="reference.html#ruizheng" id="id4"><span>[RuiZheng]</span></a></p>
<figure class="align-center" id="id14">
<img alt="RLHF_training_realworld" src="_images/RLHF_training_realworld.png" />
<figcaption>
<p><span class="caption-text">RLHF Training Workflow (Source: <a class="reference external" href="https://arxiv.org/abs/2307.04964">Secrets of RLHF in Large Language Models Part I PPO</a>)</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>RLHF Training Tricks</strong></p>
<p>There are practical challenges that arise during RLHF training. These
challenges stem from the inherent complexities of RL, especially when
applied to aligning LLMs with human preferences. Therefore, tricks are
essential for addressing the practical limitations of RLHF, ensuring the
training process remains efficient, stable, and aligned with human
preferences while minimizing the impact of inherent challenges in RL
systems.</p>
<figure class="align-center" id="id15">
<img alt="RLHF_training_tricks" src="_images/RLHF_training_tricks.png" />
<figcaption>
<p><span class="caption-text">RLHF Training Tricks (Source: <a class="reference external" href="https://arxiv.org/abs/2307.04964">Secrets of RLHF in Large Language Models Part I
PPO</a>)</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="dpo">
<h4><span class="section-number">6.1.5.2. </span>DPO<a class="headerlink" href="#dpo" title="Link to this heading"></a></h4>
<p>The main reason why RLHF with PPO is hard is that it takes a lot of
redundant effort. Policy Model is all we need, all other efforts are not
necessary. <strong>DPO (Direct Preference Optimization)</strong> is a novel
alternative to traditional RLHF for fine-tuning LLMs. It simplifies the
RLHF process by eliminating the need for complex reward models and RL
algorithms. Instead, DPO reframes the problem of aligning LLMs with
human preferences as a classification problem using human-labeled
preference data. <a class="reference internal" href="reference.html#rafaelrafailov" id="id5"><span>[RafaelRafailov]</span></a></p>
<p>The main idea is DPO and difference between DPO and PPO are shown in the
figure below</p>
<figure class="align-center" id="id16">
<img alt="DPO_idea" src="_images/DPO_idea.png" />
<figcaption>
<p><span class="caption-text">DPO Idea in the Paper (Source: <a class="reference external" href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization Your Language Model is Secretly a Reward Model</a>)</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>DPO Objective</strong></p>
<p><strong>RLHF objective</strong> is defined as follows. Keep in mind that no matter
whether DPO or PPO is used, the objective is always like this.</p>
<div class="math notranslate nohighlight">
\[\max_{\pi_\theta} E_{x \sim D, y \sim \pi_\theta(y|x)}\Big[r_{\phi}(x,y) - \beta D_{KL}\big[\pi_\theta(y|x) || \pi_{ref}(y|x)\big]\Big]\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta D_{KL}\big[\pi_\theta(y|x) || \pi_{ref}(y|x)\big]\)</span> is
a regularization term. When applying RL to NLP, regularization is often
needed. Otherwise RL would explore every possible situation and find out
hidden tricks which deviate from a language model.</p>
<p><strong>DPO’s objective function</strong> is derived by incoroprating the probability
of preference from reward function of optimal policy. DPO paper has
provided detailed steps of deriving the gradient of the DPO objective:<a class="reference internal" href="reference.html#rafaelrafailov" id="id6"><span>[RafaelRafailov]</span></a></p>
<div class="math notranslate nohighlight">
\[L_{DPO}(\pi_\theta; \pi_{ref}) = -E_{(x,y_w,y_l) \sim D} \Big[\log \sigma \Big(\beta \log {\pi_{\theta}(y_w|x)\over \pi_{ref}(y_w|x)} - \beta \log {\pi_{\theta}(y_l|x)\over \pi_{ref}(y_l|x)}\Big)\Big)\Big]\]</div>
<p><strong>Key ideas of DPO objective</strong>:</p>
<ul class="simple">
<li><p>DPO’s objective aims to increase the likelihood of generating
preferred responses over less preferred ones. By focusing directly on
preference data, DPO eliminates the need to first fit a reward model
that predicts scalar rewards based on human preferences. This
simplifies the training pipeline and reduces computational overhead.</p></li>
<li><p>Value functions exist to help reduce the variance of the reward model.
In DPO, the value function is not involved because DPO does not rely
on a traditional RL framework, such as Actor-Critic methods. Instead,
DPO directly optimizes the policy using human preference data as a
<strong>classification task</strong>, skipping the intermediate steps of training a
reward model or estimating value functions.</p></li>
<li><p>DPO was originally designed to work with <strong>pairwise</strong> preference data,
however, recent advancements and adaptations have extended its
applicability to ranking preference data as well (e.g RankDPO).</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">def</span> <span class="nf">dpo_loss</span><span class="p">(</span><span class="n">pi_logps</span><span class="p">,</span> <span class="n">ref_logps</span><span class="p">,</span> <span class="n">yw_idxs</span><span class="p">,</span> <span class="n">yl_idxs</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    pi_logps: policy logprobs, shape (B,)</span>
<span class="sd">    ref_logps: reference model logprobs, shape (B,)</span>
<span class="sd">    yw_idxs: preferred completion indices in [0, B-1], shape (T,)</span>
<span class="sd">    yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)</span>
<span class="sd">    beta: temperature controlling strength of KL penalty</span>

<span class="sd">    Each pair of (yw_idxs[i], yl_idxs[i]) represents the</span>
<span class="sd">    indices of a single preference pair.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pi_yw_logps</span><span class="p">,</span> <span class="n">pi_yl_logps</span> <span class="o">=</span> <span class="n">pi_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span> <span class="n">pi_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>
    <span class="n">ref_yw_logps</span><span class="p">,</span> <span class="n">ref_yl_logps</span> <span class="o">=</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yw_idxs</span><span class="p">],</span> <span class="n">ref_logps</span><span class="p">[</span><span class="n">yl_idxs</span><span class="p">]</span>

    <span class="n">pi_logratios</span> <span class="o">=</span> <span class="n">pi_yw_logps</span> <span class="o">-</span> <span class="n">pi_yl_logps</span>
    <span class="n">ref_logratios</span> <span class="o">=</span> <span class="n">ref_yw_logps</span> <span class="o">-</span> <span class="n">ref_yl_logps</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="o">-</span><span class="n">F</span><span class="o">.</span><span class="n">logsigmoid</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logratios</span> <span class="o">-</span> <span class="n">ref_logratios</span><span class="p">))</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi_logps</span> <span class="o">-</span> <span class="n">ref_logps</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">rewards</span>
</pre></div>
</div>
<p><strong>Steps of RLHF Using DPO</strong></p>
<p><strong>1. Initial Setup and Supervised Fine-Tuning (SFT)</strong>: Begin by
fine-tuning a pre-trained LLM using supervised learning on a dataset
that is representative of the tasks the model will perform. This step
ensures the model has a strong foundation in the relevant domain,
preparing it for preference-based optimization.</p>
<p><strong>2. Collect Preference Data</strong>: Gather human feedback in the form of
pairwise preferences or rankings. Annotators evaluate responses
generated by the model and indicate which ones they prefer. Construct a
dataset of prompts and corresponding preferred and less-preferred
responses.</p>
<p><strong>3. Iterative Rounds of DPO</strong></p>
<ul class="simple">
<li><p><strong>Sampling and Annotation</strong>: In each round, sample a set of responses
from the model for given prompts. Collect new preference annotations
based on these samples, allowing for dynamic updates to the preference
dataset. (Public preference data works as well. Off-policy and
on-policy data both work).</p></li>
<li><p><strong>Preference Optimization</strong>: Use DPO to adjust the model’s outputs
based on collected preference data:</p></li>
<li><p><strong>Model Update</strong>: Fine-tune the model using this loss function to
increase the likelihood of generating preferred responses.</p></li>
</ul>
<p><strong>4. Evaluation and Iteration</strong></p>
<ul class="simple">
<li><p><strong>Performance Assessment</strong>: After each round, evaluate the model’s
performance on new prompts to ensure it aligns with human preferences.
Use feedback from these evaluations to inform subsequent rounds of
sampling and optimization.</p></li>
<li><p><strong>Iterative Refinement</strong>: Continue this loop process over multiple
rounds, iteratively refining the model’s alignment with human
preferences through continuous sampling and preference optimization.</p></li>
</ul>
<p><strong>DPO Variants</strong></p>
<p>The key area of research involves developing variants of DPO and
conducting theoretical analyses to understand its limitations and
potential improvements. This includes exploring different loss functions
or optimization strategies that can be applied within the DPO framework.</p>
<ul>
<li><p>One significant area of research focuses on refining the loss function
used in DPO. This includes exploring ways to eliminate the need for a
reference model, which can simplify the optimization process.</p>
<p>Examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2403.07691">ORPO: Monolithic Preference Optimization without Reference
Model</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2405.14734">SimPO: Simple Preference Optimization with a Reference-Free
Reward</a></p></li>
</ul>
</li>
<li><p>Another key direction involves leveraging existing supervised
fine-tuning data as preference data for DPO. This strategy aims to
enhance the quality of preference data by utilizing high-quality
labeled datasets that may already exist from previous SFT processes.</p>
<p>Examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2402.08005v1">Refined Direct Preference Optimization with Synthetic Data for
Behavioral Alignment of LLMs</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="main-difficulties-in-rlhf">
<h4><span class="section-number">6.1.5.3. </span>Main Difficulties in RLHF<a class="headerlink" href="#main-difficulties-in-rlhf" title="Link to this heading"></a></h4>
<p><strong>Data Collection</strong></p>
<p>In practice, people noticed that the collection of human feedback in the
form of the preference dataset is a slow manual process that needs to be
repeated whenever alignment criteria change. And there is increasing
difficulty in annotating preference data as models become more advanced,
particularly because distinguishing between outputs becomes more nuanced
and subjective.</p>
<ul class="simple">
<li><p>The paper “<a class="reference external" href="https://arxiv.org/abs/2411.02481">CDR: Customizable Density Ratios of Strong-over-weak LLMs
for Preference Annotation</a>”
explains that as models become more advanced, it becomes harder to
identify which output is better due to subtle differences in quality.
This makes preference data annotation increasingly difficult and
subjective.</p></li>
<li><p>Another paper, “<a class="reference external" href="https://arxiv.org/abs/2407.14916">Improving Context-Aware Preference Modeling for
Language Models</a>,” discusses how
the underspecified nature of natural language and multidimensional
criteria make direct preference feedback difficult to interpret. This
highlights the challenge of providing consistent annotations when
outputs are highly sophisticated and nuanced.</p></li>
<li><p>“<a class="reference external" href="https://www.arxiv.org/abs/2408.12799">Less for More: Enhancing Preference Learning in Generative Language
Models</a>” also notes that
ambiguity among annotators leads to inconsistently annotated datasets,
which becomes a greater issue as model outputs grow more complex.</p></li>
</ul>
<p><strong>Reward Hacking</strong></p>
<p>Reward hacking is a common problem in reinforcement learning, where the
agent learns to exploit the system by maximizing its reward through
actions that deviate from the intended goal. In the context of RLHF,
reward hacking occurs when training settles in an unintended region of
the loss landscape. In this scenario, the model generates responses that
achieve high reward scores, but these responses may fail to be
meaningful or useful to the user.</p>
<p>In PPO, reward hacking occurs when the model exploits flaws or
ambiguities in the <strong>reward model</strong> to achieve high rewards without
genuinely aligning with human intentions. This is because PPO relies on
a learned reward model to guide policy updates, and any inaccuracies or
biases in this model can lead to unintended behaviors being rewarded.
PPO is particularly vulnerable to reward hacking if the reward model is
not robustly designed or if it fails to capture the true objectives of
human feedback. The iterative nature of PPO, which involves continuous
policy updates based on reward signals, can exacerbate this issue if not
carefully managed.</p>
<p>DPO avoids explicit reward modeling by directly optimizing policy based
on preference data. However, it can still encounter issues similar to
reward hacking if the preference data is <strong>biased</strong> or if the
optimization process leads to <strong>overfitting</strong> specific patterns in the
data that do not generalize well. While DPO does not suffer from reward
hacking in the traditional sense (since it lacks a separate reward
model), it can still find biased solutions that exploit
<strong>out-of-distribution responses</strong> or deviate from intended behavior due
to distribution shifts between training and deployment contexts.</p>
<ul class="simple">
<li><p>The article “<a class="reference external" href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">Reward Hacking in Reinforcement
Learning</a>”
by Lilian Weng discusses how reward hacking occurs when a RL agent
exploits flaws or ambiguities in the reward function to achieve high
rewards without genuinely learning the intended task. It highlights
that in RLHF for language models, reward hacking is a critical
challenge, as models might learn to exploit unit tests or mimic biases
to achieve high rewards, which can hinder real-world deployment.</p></li>
<li><p>The research “<a class="reference external" href="https://arxiv.org/abs/2210.10760">Scaling Laws for Reward Model
Overoptimization</a>” explores how
optimizing against reward models trained to predict human preferences
can lead to overoptimization, hindering the actual objective.</p>
<ol class="arabic simple">
<li><p><strong>Impact of Policy Model Size</strong>: Holding the RM size constant,
experiments showed that larger policy models exhibited similar
overoptimization trends as smaller models, despite achieving higher
initial gold scores. This implies that their higher performance on
gold rewards does not lead to excessive optimization pressure on
the RM.</p></li>
<li><p><strong>Relationship with RM Data Size</strong>: Data size had a notable effect
on RM performance and overoptimization. Models trained on fewer
than ~2,000 comparison labels showed near-chance performance, with
limited improvement in gold scores. Beyond this threshold, all RMs,
regardless of size, benefited from increased data, with larger RMs
showing greater improvements in gold rewards compared to smaller
ones.</p></li>
<li><p><strong>Scaling Laws for RM Parameters and Data Size</strong>: Overoptimization
patterns scaled smoothly with both RM parameter count and data
size. Larger RMs demonstrated better alignment with gold rewards
and less susceptibility to overoptimization when trained on
sufficient data, indicating improved robustness.</p></li>
<li><p><strong>Proxy vs. Gold Reward Trends</strong>: For small data sizes, proxy
reward scores deviated significantly from gold reward scores,
highlighting overoptimization risks. As data size increased, the
gap between proxy and gold rewards narrowed, reducing
overoptimization effects.</p></li>
</ol>
</li>
</ul>
<p>Note that the KL divergence term in the RLHF objective is intended to
prevent the policy from deviating too much from a reference model,
thereby maintaining stability during training. However, it does not
fully prevent reward hacking. Reward hacking occurs when an agent
exploits flaws or ambiguities in the reward model to achieve high
rewards without genuinely aligning with human intentions. The KL
divergence penalty does not correct these flaws in the reward model
itself, meaning that if the reward model is misaligned, the agent can
still find ways to exploit it. KL does not directly address whether the
actions align with the true objectives or desired outcomes.</p>
</section>
</section>
<section id="summary-table">
<h3><span class="section-number">6.1.6. </span>Summary Table<a class="headerlink" href="#summary-table" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><strong>Method</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Key Benefit</strong></p></td>
</tr>
<tr class="row-even"><td><p><strong>LoRA</strong></p></td>
<td><p>Low-rank adapters for parameter-efficient
tuning.</p></td>
<td><p>Reduces trainable parameters significantly.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>QLoRA</strong></p></td>
<td><p>LoRA with 4-bit quantization of the model.</p></td>
<td><p>Fine-tunes massive models on smaller
hardware.</p></td>
</tr>
<tr class="row-even"><td><p><strong>PEFT</strong></p></td>
<td><p>General framework for efficient fine-tuning.</p></td>
<td><p>Includes LoRA, Adapters, Prefix Tuning,
etc.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>SFT</strong></p></td>
<td><p>Supervised fine-tuning with labeled data.</p></td>
<td><p>High performance on task-specific datasets</p></td>
</tr>
</tbody>
</table>
<p>These strategies represent the forefront of <strong>LLM fine-tuning</strong>, offering efficient and scalable solutions for
real-world applications. To choose the most suitable strategy, consider the following factors:</p>
<ul class="simple">
<li><p><strong>Resource-Constrained Environments</strong>: Use <strong>LoRA</strong> or <strong>QLoRA</strong>.</p></li>
<li><p><strong>Large-Scale Models</strong>: <strong>QLoRA</strong> for low-memory fine-tuning.</p></li>
<li><p><strong>High Performance with Labeled Data</strong>: <strong>SFT</strong>.</p></li>
<li><p><strong>Minimal Setup</strong>: <strong>Zero-shot</strong> or <strong>Few-shot</strong> learning.</p></li>
<li><p><strong>General Efficiency</strong>: Use <strong>PEFT</strong> frameworks.</p></li>
</ul>
</section>
</section>
<section id="key-early-fine-tuning-methods">
<h2><span class="section-number">6.2. </span>Key Early Fine-Tuning Methods<a class="headerlink" href="#key-early-fine-tuning-methods" title="Link to this heading"></a></h2>
<p>Early fine-tuning methods laid the foundation for current approaches. These methods
primarily focused on updating the entire model or selected components.</p>
<section id="full-fine-tuning">
<h3><span class="section-number">6.2.1. </span>Full Fine-Tuning<a class="headerlink" href="#full-fine-tuning" title="Link to this heading"></a></h3>
<p>All the parameters of a pre-trained model are updated using task-specific data <a class="reference internal" href="#fig-fine-tuning"><span class="std std-ref">The three conventional feature-based and finetuning approaches (Souce Finetuning Sebastian).</span></a> (right).</p>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>The pre-trained model serves as the starting point.</p></li>
<li><p>Fine-tuning is conducted on a smaller, labeled dataset using a supervised loss function.</p></li>
<li><p>A low learning rate is used to prevent <strong>catastrophic forgetting</strong>.</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Effective at adapting models to specific tasks.</p></li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul class="simple">
<li><p>Computationally expensive.</p></li>
<li><p>Risk of overfitting on small datasets.</p></li>
</ul>
</section>
<section id="feature-based-approach">
<h3><span class="section-number">6.2.2. </span>Feature-Based Approach<a class="headerlink" href="#feature-based-approach" title="Link to this heading"></a></h3>
<p>The pre-trained model is used as a <strong>feature extractor</strong>, while only a task-specific head is trained <a class="reference internal" href="#fig-fine-tuning"><span class="std std-ref">The three conventional feature-based and finetuning approaches (Souce Finetuning Sebastian).</span></a> (left).</p>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>The model processes inputs and extracts features (embeddings).</p></li>
<li><p>A separate classifier (e.g., linear or MLP) is trained on top of these features.</p></li>
<li><p>The pre-trained model weights remain <strong>frozen</strong>.</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Computationally efficient since only the task-specific head is trained.</p></li>
</ul>
</section>
<section id="layer-specific-fine-tuning">
<h3><span class="section-number">6.2.3. </span>Layer-Specific Fine-Tuning<a class="headerlink" href="#layer-specific-fine-tuning" title="Link to this heading"></a></h3>
<p>Only certain layers of the pre-trained model are fine-tuned while the rest remain frozen <a class="reference internal" href="#fig-fine-tuning"><span class="std std-ref">The three conventional feature-based and finetuning approaches (Souce Finetuning Sebastian).</span></a> (middle).</p>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>Earlier layers (which capture general features) are frozen.</p></li>
<li><p>Later layers (closer to the output) are fine-tuned on task-specific data.</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Balances computational efficiency and task adaptation.</p></li>
</ul>
</section>
<section id="task-adaptive-pre-training">
<h3><span class="section-number">6.2.4. </span>Task-Adaptive Pre-training<a class="headerlink" href="#task-adaptive-pre-training" title="Link to this heading"></a></h3>
<p>Before fine-tuning on a specific task, the model undergoes additional <strong>pre-training</strong> on a domain-specific corpus.</p>
<p><strong>How It Works</strong>:</p>
<ul class="simple">
<li><p>A general pre-trained model is further pre-trained (unsupervised) on domain-specific data.</p></li>
<li><p>Fine-tuning is then performed on the downstream task.</p></li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul class="simple">
<li><p>Provides a better starting point for domain-specific tasks.</p></li>
</ul>
</section>
</section>
<section id="embedding-model-fine-tuning">
<h2><span class="section-number">6.3. </span>Embedding Model Fine-Tuning<a class="headerlink" href="#embedding-model-fine-tuning" title="Link to this heading"></a></h2>
<p>In the chapter <a class="reference internal" href="rag.html#rag"><span class="std std-ref">Retrieval-Augmented Generation</span></a>, we discussed how embedding models are crucial for the success of RAG applications.
However, their general-purpose training often limits their effectiveness for company- or domain-specific
use cases. Customizing embeddings with domain-specific data can significantly improve the retrieval
performance of your RAG application.</p>
<p>In this chapter, we will demonstrate how to fine-tune embedding models using the
<code class="docutils literal notranslate"><span class="pre">SentenceTransformersTrainer</span></code>, building on insights shared in the blog <a class="reference internal" href="reference.html#finetuneembedding" id="id7"><span>[fineTuneEmbedding]</span></a> and
Sentence Transformer <a class="reference external" href="https://sbert.net/docs/sentence_transformer/training_overview.html#dataset-format">Training Overview</a>. Our main contribution was introducing LoRA to enable functionality on
NVIDIA T4 GPUs, while the rest of the pipeline and code remained almost unchanged.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please ensure that the package versions are set as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span>  <span class="s2">&quot;torch==2.1.2&quot;</span> <span class="n">tensorboard</span>

<span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> \
    <span class="n">sentence</span><span class="o">-</span><span class="n">transformers</span><span class="o">&gt;=</span><span class="mi">3</span> \
    <span class="n">datasets</span><span class="o">==</span><span class="mf">2.19.1</span>  \
    <span class="n">transformers</span><span class="o">==</span><span class="mf">4.41.2</span> \
    <span class="n">peft</span><span class="o">==</span><span class="mf">0.10.0</span>
</pre></div>
</div>
<p>Otherwise, you may encounter the error.</p>
</div>
<section id="prepare-dataset">
<h3><span class="section-number">6.3.1. </span>Prepare Dataset<a class="headerlink" href="#prepare-dataset" title="Link to this heading"></a></h3>
<p>We are going to directly use the synthetic dataset <code class="docutils literal notranslate"><span class="pre">philschmid/finanical-rag-embedding-dataset</span></code>, which includes 7,000
positive text pairs of questions and corresponding context from the <a class="reference external" href="https://stocklight.com/stocks/us/nasdaq-nvda/nvidia/annual-reports/nasdaq-nvda-2023-10K-23668751.pdf">2023_10 NVIDIA SEC Filing</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># Load dataset from the hub</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;philschmid/finanical-rag-embedding-dataset&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

<span class="c1"># rename columns</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">rename_column</span><span class="p">(</span><span class="s2">&quot;question&quot;</span><span class="p">,</span> <span class="s2">&quot;anchor&quot;</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">rename_column</span><span class="p">(</span><span class="s2">&quot;context&quot;</span><span class="p">,</span> <span class="s2">&quot;positive&quot;</span><span class="p">)</span>

<span class="c1"># Add an id column to the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">add_column</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)))</span>

<span class="c1"># split dataset into a 10% test set</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># save datasets to disk</span>
<span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="s2">&quot;train_dataset.json&quot;</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="s2">&quot;test_dataset.json&quot;</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In practice, most dataset configurations will take one of four forms:</p>
<ul class="simple">
<li><p><strong>Positive Pair</strong>: A pair of related sentences. This can be used both for symmetric tasks
(semantic textual similarity) or asymmetric tasks (semantic search), with examples
including pairs of paraphrases, pairs of full texts and their summaries, pairs of
duplicate questions, pairs of <code class="docutils literal notranslate"><span class="pre">(query,</span> <span class="pre">response)</span></code>, or pairs of
<code class="docutils literal notranslate"><span class="pre">(source_language,</span> <span class="pre">target_language)</span></code>.
Natural Language Inference datasets can also be formatted this way by pairing entailing sentences.</p></li>
<li><p><strong>Triplets</strong>: <code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">positive,</span> <span class="pre">negative)</span></code> text triplets. These datasets don’t need labels.</p></li>
<li><p><strong>Pair with Similarity Score</strong>: A pair of sentences with a score indicating their similarity.
Common examples are “Semantic Textual Similarity” datasets.</p></li>
<li><p><strong>Texts with Classes</strong>: A text with its corresponding class. This data format is easily
converted by loss functions into three sentences (triplets) where the first is an “anchor”,
the second a “positive” of the same class as the anchor, and the third a “negative” of a different class.</p></li>
</ul>
<p>Note that it is often simple to transform a dataset from one format to another, such that it works with
your loss function of choice.</p>
</div>
</section>
<section id="import-and-evaluate-pretrained-baseline-model">
<h3><span class="section-number">6.3.2. </span>Import and Evaluate Pretrained Baseline Model<a class="headerlink" href="#import-and-evaluate-pretrained-baseline-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.evaluation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">InformationRetrievalEvaluator</span><span class="p">,</span>
    <span class="n">SequentialEvaluator</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.util</span> <span class="kn">import</span> <span class="n">cos_sim</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">concatenate_datasets</span>
<span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>

<span class="n">model_id</span> <span class="o">=</span>  <span class="s2">&quot;BAAI/bge-base-en-v1.5&quot;</span>
<span class="n">matryoshka_dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">768</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># Important: large to small</span>

<span class="c1"># Load a model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="p">)</span>

<span class="c1"># load test dataset</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="s2">&quot;test_dataset.json&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="s2">&quot;train_dataset.json&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">corpus_dataset</span> <span class="o">=</span> <span class="n">concatenate_datasets</span><span class="p">([</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">])</span>

<span class="c1"># Convert the datasets to dictionaries</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">corpus_dataset</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">],</span> <span class="n">corpus_dataset</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">])</span>
<span class="p">)</span>  <span class="c1"># Our corpus (cid =&gt; document)</span>
<span class="n">queries</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="nb">zip</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">],</span> <span class="n">test_dataset</span><span class="p">[</span><span class="s2">&quot;anchor&quot;</span><span class="p">])</span>
<span class="p">)</span>  <span class="c1"># Our queries (qid =&gt; question)</span>

<span class="c1"># Create a mapping of relevant document (1 in our case) for each query</span>
<span class="n">relevant_docs</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Query ID to relevant documents (qid =&gt; set([relevant_cids])</span>
<span class="k">for</span> <span class="n">q_id</span> <span class="ow">in</span> <span class="n">queries</span><span class="p">:</span>
    <span class="n">relevant_docs</span><span class="p">[</span><span class="n">q_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_id</span><span class="p">]</span>


<span class="n">matryoshka_evaluators</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Iterate over the different dimensions</span>
<span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">matryoshka_dimensions</span><span class="p">:</span>
    <span class="n">ir_evaluator</span> <span class="o">=</span> <span class="n">InformationRetrievalEvaluator</span><span class="p">(</span>
        <span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span>
        <span class="n">corpus</span><span class="o">=</span><span class="n">corpus</span><span class="p">,</span>
        <span class="n">relevant_docs</span><span class="o">=</span><span class="n">relevant_docs</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;dim_</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">truncate_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>  <span class="c1"># Truncate the embeddings to a certain dimension</span>
        <span class="n">score_functions</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cosine&quot;</span><span class="p">:</span> <span class="n">cos_sim</span><span class="p">},</span>
    <span class="p">)</span>
    <span class="n">matryoshka_evaluators</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ir_evaluator</span><span class="p">)</span>

<span class="c1"># Create a sequential evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">SequentialEvaluator</span><span class="p">(</span><span class="n">matryoshka_evaluators</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you encounter the error <code class="docutils literal notranslate"><span class="pre">Cannot</span> <span class="pre">import</span> <span class="pre">name</span> <span class="pre">'EncoderDecoderCache'</span> <span class="pre">from</span> <span class="pre">'transformers'</span></code>,
ensure that the package versions are set to <code class="docutils literal notranslate"><span class="pre">peft==0.10.0</span></code> and <code class="docutils literal notranslate"><span class="pre">transformers==4.37.2</span></code>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate the model</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Print the main score</span>
<span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">matryoshka_dimensions</span><span class="p">:</span>
    <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;dim_</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">_cosine_ndcg@10&quot;</span>
    <span class="nb">print</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dim_768_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.754897248109794</span>
<span class="n">dim_512_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.7549275773474213</span>
<span class="n">dim_256_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.7454714780163237</span>
<span class="n">dim_128_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.7116728650043451</span>
<span class="n">dim_64_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.6477174937632066</span>
</pre></div>
</div>
</section>
<section id="loss-function-with-matryoshka-representation">
<h3><span class="section-number">6.3.3. </span>Loss Function with Matryoshka Representation<a class="headerlink" href="#loss-function-with-matryoshka-representation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformerModelCardData</span><span class="p">,</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Hugging Face model ID: https://huggingface.co/BAAI/bge-base-en-v1.5</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;BAAI/bge-base-en-v1.5&quot;</span>

<span class="c1"># load model with SDPA for using Flash Attention 2</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;attn_implementation&quot;</span><span class="p">:</span> <span class="s2">&quot;sdpa&quot;</span><span class="p">},</span>
    <span class="n">model_card_data</span><span class="o">=</span><span class="n">SentenceTransformerModelCardData</span><span class="p">(</span>
        <span class="n">language</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span>
        <span class="n">license</span><span class="o">=</span><span class="s2">&quot;apache-2.0&quot;</span><span class="p">,</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;BGE base Financial Matryoshka&quot;</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Apply PEFT with PromptTuningConfig</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">FEATURE_EXTRACTION</span><span class="p">,</span>
    <span class="n">inference_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="n">peft_config</span><span class="p">,</span> <span class="s2">&quot;dense&quot;</span><span class="p">)</span>

<span class="c1"># train loss</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.losses</span> <span class="kn">import</span> <span class="n">MatryoshkaLoss</span><span class="p">,</span> <span class="n">MultipleNegativesRankingLoss</span>

<span class="n">matryoshka_dimensions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">768</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>  <span class="c1"># Important: large to small</span>
<span class="n">inner_train_loss</span> <span class="o">=</span> <span class="n">MultipleNegativesRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">MatryoshkaLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                            <span class="n">inner_train_loss</span><span class="p">,</span>
                            <span class="n">matryoshka_dims</span><span class="o">=</span><span class="n">matryoshka_dimensions</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Loss functions play a critical role in the performance of your fine-tuned model.
Sadly, there is no “one size fits all” loss function. Ideally,
this table should help narrow down your choice of loss function(s) by matching
them to your data formats.</p>
<p>You can often convert one training data format into another, allowing more loss
functions to be viable for your scenario. For example,</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Inputs</p></th>
<th class="head"><p>Labels</p></th>
<th class="head"><p>Appropriate Loss Functions</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">single</span> <span class="pre">sentences</span></code></p></td>
<td><p><cite>class</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BatchAllTripletLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchHardSoftMarginTripletLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchHardTripletLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchSemiHardTripletLoss</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">single</span> <span class="pre">sentences</span></code></p></td>
<td><p><cite>none</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ContrastiveTensionLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">DenoisingAutoEncoderLoss</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">anchor)</span></code> pairs</p></td>
<td><p><cite>none</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ContrastiveTensionLossInBatchNegatives</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">(damaged_sentence,</span> <span class="pre">original_sentence)</span></code> pairs</p></td>
<td><p><cite>none</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">DenoisingAutoEncoderLoss</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">(sentence_A,</span> <span class="pre">sentence_B)</span></code> pairs</p></td>
<td><p><cite>class</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SoftmaxLoss</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">positive)</span></code> pairs</p></td>
<td><p><cite>none</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">MultipleNegativesSymmetricRankingLoss</span></code>,
<code class="docutils literal notranslate"><span class="pre">CachedMultipleNegativesSymmetricRankingLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">MegaBatchMarginLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CachedGISTEmbedLoss</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">positive/negative)</span></code> pairs</p></td>
<td><p><cite>1 if positive, 0 if negative</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ContrastiveLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">OnlineContrastiveLoss</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">(sentence_A,</span> <span class="pre">sentence_B)</span></code> pairs</p></td>
<td><p><cite>float similarity score</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CoSENTLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">AnglELoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CosineSimilarityLoss</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">(anchor,</span> <span class="pre">positive,</span> <span class="pre">negative)</span></code> triplets</p></td>
<td><p><cite>none</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">TripletLoss</span></code>,
<code class="docutils literal notranslate"><span class="pre">CachedGISTEmbedLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><cite>(anchor, positive, negative_1, …, negative_n)`</cite></p></td>
<td><p><cite>none</cite></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">CachedGISTEmbedLoss</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="fine-tune-embedding-model">
<h3><span class="section-number">6.3.4. </span>Fine-tune Embedding Model<a class="headerlink" href="#fine-tune-embedding-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformerTrainingArguments</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.training_args</span> <span class="kn">import</span> <span class="n">BatchSamplers</span>

<span class="c1"># load train dataset again</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="s2">&quot;train_dataset.json&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

<span class="c1"># define training arguments</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span> <span class="c1"># output directory and hugging face model ID</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>                         <span class="c1"># number of epochs</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>             <span class="c1"># train batch size</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>             <span class="c1"># for a global batch size of 512</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>              <span class="c1"># evaluation batch size</span>
    <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>                           <span class="c1"># warmup ratio</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>                         <span class="c1"># learning rate, 2e-5 is a good value</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>                 <span class="c1"># use constant learning rate scheduler</span>
    <span class="n">optim</span><span class="o">=</span><span class="s2">&quot;adamw_torch_fused&quot;</span><span class="p">,</span>                  <span class="c1"># use fused adamw optimizer</span>
    <span class="n">tf32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>                                  <span class="c1"># use tf32 precision</span>
    <span class="n">bf16</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>                                  <span class="c1"># use bf16 precision</span>
    <span class="n">batch_sampler</span><span class="o">=</span><span class="n">BatchSamplers</span><span class="o">.</span><span class="n">NO_DUPLICATES</span><span class="p">,</span>  <span class="c1"># MultipleNegativesRankingLoss benefits from no duplicate samples in a batch</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>                      <span class="c1"># evaluate after each epoch</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>                      <span class="c1"># save after each epoch</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>                           <span class="c1"># log every 10 steps</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>                         <span class="c1"># save only the last 3 models</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>                <span class="c1"># load the best model when training ends</span>
    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="s2">&quot;eval_dim_128_cosine_ndcg@10&quot;</span><span class="p">,</span>  <span class="c1"># Optimizing for the best ndcg@10 score for the 128 dimension</span>
    <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>                     <span class="c1"># maximize the ndcg@10 score</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformerTrainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="c1"># bg-base-en-v1</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>  <span class="c1"># training arguments</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">select_columns</span><span class="p">(</span>
        <span class="p">[</span><span class="s2">&quot;anchor&quot;</span><span class="p">,</span> <span class="s2">&quot;positive&quot;</span><span class="p">]</span>
    <span class="p">),</span>  <span class="c1"># training dataset</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">train_loss</span><span class="p">,</span>
    <span class="n">evaluator</span><span class="o">=</span><span class="n">evaluator</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># start training</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># save the best model</span>
<span class="c1">#trainer.save_model()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;bge-base-finetuning&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="evaluate-fine-tuned-model">
<h3><span class="section-number">6.3.5. </span>Evaluate Fine-tuned Model<a class="headerlink" href="#evaluate-fine-tuned-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">fine_tuned_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span>
    <span class="s1">&#39;bge-base-finetuning&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="p">)</span>
<span class="c1"># Evaluate the model</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">fine_tuned_model</span><span class="p">)</span>

<span class="c1"># # COMMENT IN for full results</span>
<span class="c1"># print(results)</span>

<span class="c1"># Print the main score</span>
<span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">matryoshka_dimensions</span><span class="p">:</span>
    <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;dim_</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">_cosine_ndcg@10&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dim_768_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.7650276801072632</span>
<span class="n">dim_512_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.7603951540556889</span>
<span class="n">dim_256_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.754743133407988</span>
<span class="n">dim_128_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.7205317098443929</span>
<span class="n">dim_64_cosine_ndcg</span><span class="o">@</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.6609117856061502</span>
</pre></div>
</div>
</section>
<section id="results-comparison">
<h3><span class="section-number">6.3.6. </span>Results Comparison<a class="headerlink" href="#results-comparison" title="Link to this heading"></a></h3>
<p>Although we did not observe the significant performance boost reported in the original
blog, the fine-tuned model outperformed the baseline model across all dimensions using
only 6.3k samples and partial parameter fine-tuning. MOre details can be found as follows:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Dimension</p></th>
<th class="head"><p>Baseline</p></th>
<th class="head"><p>Fine-tuned</p></th>
<th class="head"><p>Improvement</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>768</p></td>
<td><p>0.75490</p></td>
<td><p>0.76503</p></td>
<td><p>1.34%</p></td>
</tr>
<tr class="row-odd"><td><p>512</p></td>
<td><p>0.75492</p></td>
<td><p>0.76040</p></td>
<td><p>0.73%</p></td>
</tr>
<tr class="row-even"><td><p>256</p></td>
<td><p>0.74547</p></td>
<td><p>0.75474</p></td>
<td><p>1.24%</p></td>
</tr>
<tr class="row-odd"><td><p>128</p></td>
<td><p>0.71167</p></td>
<td><p>0.72053</p></td>
<td><p>1.24%</p></td>
</tr>
<tr class="row-even"><td><p>64</p></td>
<td><p>0.64772</p></td>
<td><p>0.66091</p></td>
<td><p>2.04%</p></td>
</tr>
</tbody>
</table>
<figure class="align-center" id="id17">
<span id="fig-wandb"></span><img alt="_images/fine_tuning_wandb.png" src="_images/fine_tuning_wandb.png" />
<figcaption>
<p><span class="caption-text">Epoch, Training Loss/steps in Wandb</span><a class="headerlink" href="#id17" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="llm-fine-tuning">
<h2><span class="section-number">6.4. </span>LLM Fine-Tuning<a class="headerlink" href="#llm-fine-tuning" title="Link to this heading"></a></h2>
<p>In this chapter, we will demonstrate how to fine-tune a Llama 2 model with 7 billion parameters using
a T4 GPU with 16 GB of VRAM. Due to VRAM limitations, traditional fine-tuning is not feasible,
making parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA essential. For this
demonstration, we use QLoRA, which leverages 4-bit precision to significantly reduce VRAM consumption.</p>
<p>The folloing code is from notebook <a class="reference internal" href="reference.html#finetunellm" id="id8"><span>[fineTuneLLM]</span></a>, and the copyright belongs to the original author.</p>
<section id="load-dataset-and-pretrained-model">
<h3><span class="section-number">6.4.1. </span>Load Dataset and Pretrained Model<a class="headerlink" href="#load-dataset-and-pretrained-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1 : Load dataset (you can process it here)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset_name</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

<span class="c1"># Step 2 :Load tokenizer and model with QLoRA configuration</span>
<span class="n">compute_dtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">bnb_4bit_compute_dtype</span><span class="p">)</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="n">use_4bit</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="n">bnb_4bit_quant_type</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="n">use_nested_quant</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Step 3 :Check GPU compatibility with bfloat16</span>
<span class="k">if</span> <span class="n">compute_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="ow">and</span> <span class="n">use_4bit</span><span class="p">:</span>
    <span class="n">major</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">major</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Your GPU supports bfloat16: accelerate training with bf16=True&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>

<span class="c1"># Step 4 :Load base model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pretraining_tp</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Step 5 :Load LLaMA tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">({</span><span class="s1">&#39;pad_token&#39;</span><span class="p">:</span> <span class="s1">&#39;[PAD]&#39;</span><span class="p">})</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>
</pre></div>
</div>
</section>
<section id="fine-tuning-configuration">
<h3><span class="section-number">6.4.2. </span>Fine-tuning Configuration<a class="headerlink" href="#fine-tuning-configuration" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 6 :Load LoRA configuration</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="n">lora_alpha</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="n">lora_dropout</span><span class="p">,</span>
    <span class="n">r</span><span class="o">=</span><span class="n">lora_r</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;CAUSAL_LM&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Step 7 :Set training parameters</span>
<span class="n">training_arguments</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">num_train_epochs</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">per_device_train_batch_size</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span>
    <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="n">save_steps</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="n">logging_steps</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="n">fp16</span><span class="p">,</span>
    <span class="n">bf16</span><span class="o">=</span><span class="n">bf16</span><span class="p">,</span>
    <span class="n">max_grad_norm</span><span class="o">=</span><span class="n">max_grad_norm</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span>
    <span class="n">warmup_ratio</span><span class="o">=</span><span class="n">warmup_ratio</span><span class="p">,</span>
    <span class="n">group_by_length</span><span class="o">=</span><span class="n">group_by_length</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="n">lr_scheduler_type</span><span class="p">,</span>
    <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;tensorboard&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fine-tune-model">
<h3><span class="section-number">6.4.3. </span>Fine-tune model<a class="headerlink" href="#fine-tune-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 8 :Set supervised fine-tuning parameters</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">peft_config</span><span class="o">=</span><span class="n">peft_config</span><span class="p">,</span>
    <span class="n">dataset_text_field</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_arguments</span><span class="p">,</span>
    <span class="n">packing</span><span class="o">=</span><span class="n">packing</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Step 9 :Train model</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Step 10 :Save trained model</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">new_model</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center" id="id18">
<span id="fig-fine-tuning-llm"></span><img alt="_images/fine_tuning_llm.png" src="_images/fine_tuning_llm.png" />
<figcaption>
<p><span class="caption-text">Llama 2 Model Fine-Tuning TensorBoard</span><a class="headerlink" href="#id18" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="rag.html" class="btn btn-neutral float-left" title="5. Retrieval-Augmented Generation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pretraining.html" class="btn btn-neutral float-right" title="7. Pre-training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Wenqiang Feng, Di Zhen and Wenyun Wang.
      <span class="lastupdated">Last updated on Dec 31, 2024.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>