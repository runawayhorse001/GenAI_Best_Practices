%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,11pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{2}



\title{GenAI: Best Practices}
\date{December 17, 2024}
\release{1.0}
\author{Wenqiang Feng and Di Zhen}
\newcommand{\sphinxlogo}{\sphinxincludegraphics{logo.png}\par}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}\phantomsection\label{\detokenize{index:index}}\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{logo}.png}
\end{figure}
\end{quote}

\sphinxAtStartPar
Welcome to our \sphinxstylestrong{GenAI: Best Practices}!!! The PDF version
can be downloaded from \sphinxhref{../latex/GenAI.pdf}{HERE}.



\sphinxstepscope


\chapter{Preface}
\label{\detokenize{preface:id1}}\label{\detokenize{preface::doc}}
\begin{sphinxadmonition}{note}{Chinese proverb}

\sphinxAtStartPar
Good tools are prerequisite to the successful execution of a job. \textendash{} old Chinese proverb
\end{sphinxadmonition}


\section{About}
\label{\detokenize{preface:about}}

\subsection{About this book}
\label{\detokenize{preface:about-this-book}}
\sphinxAtStartPar
This is the book for our Generative AI: Best practics \sphinxcite{reference:genai}.
The PDF version can be downloaded from \sphinxhref{../latex/GenAI.pdf}{HERE}.
\sphinxstylestrong{You may download and distribute it. Please beaware,
however, that the note contains typos as well as inaccurate or incorrect description.}

\sphinxAtStartPar
In this book, I aim to demonstrate best practices for Generative AI
through detailed demo code and practical examples. If you notice that
your work has not been properly cited, please do not hesitate to
reach out and let me know.


\subsection{About the authors}
\label{\detokenize{preface:about-the-authors}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Wenqiang Feng}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sr. Mgr Data Enginner and PhD in Mathematics

\item {} 
\sphinxAtStartPar
University of Tennessee at Knoxville

\item {} 
\sphinxAtStartPar
Webpage: \sphinxurl{http://web.utk.edu/~wfeng1/}

\item {} 
\sphinxAtStartPar
Email: \sphinxhref{mailto:von198@gmail.com}{von198@gmail.com}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Biography}

\sphinxAtStartPar
Wenqiang Feng is the Senior Manager of Data Engineering and former Director of
AI Engineering/Data Science at American Express (AMEX). Before his tenure at
AMEX, Dr. Feng served as a Senior Data Scientist in the Machine Learning Lab
at H\&R Block and as a Data Scientist at Applied Analytics Group, DST (now SS\&C).
Throughout his career, Dr. Feng has focused on equipping clients with cutting\sphinxhyphen{}edge
skills and technologies, including Big Data analytics, advanced modeling
techniques, and data enhancement strategies.

\sphinxAtStartPar
Dr. Feng brings extensive expertise in data mining, analytic systems, machine
learning algorithms, business intelligence, and the application of Big Data
tools to solve complex, cross\sphinxhyphen{}functional industry challenges. Prior to his
role at DST, Dr. Feng was an IMA Data Science Fellow at the Institute for
Mathematics and its Applications (IMA) at the University of Minnesota.
In this capacity, he collaborated with startups to develop predictive
analytics solutions that informed strategic marketing decisions.

\sphinxAtStartPar
Dr. Feng holds a Ph.D. in Computational Mathematics and a Master‚Äôs degree
in Statistics from the University of Tennessee, Knoxville. He also earned a
Master‚Äôs degree in Computational Mathematics from Missouri University
of Science and Technology (MST) and a Master‚Äôs degree in Applied
Mathematics from the University of Science and Technology of China (USTC).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Declaration}

\sphinxAtStartPar
The work of Wenqiang Feng was supported by the IMA, while working at IMA. However, any opinion, finding,
and conclusions or recommendations expressed in this material are those of the author and do not necessarily
reflect the views of the IMA, UTK and DST.

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
ChatGPT has been extensively used in the creation of this book. If you notice that your work has not been
cited or has been cited incorrectly, please notify us.
\end{sphinxadmonition}

\end{itemize}


\section{Feedback and suggestions}
\label{\detokenize{preface:feedback-and-suggestions}}
\sphinxAtStartPar
Your comments and suggestions are highly appreciated. I am more than happy to receive
corrections, suggestions or feedback through email (Wenqiang Feng: \sphinxhref{mailto:von198@gmail.com}{von198@gmail.com} and
Di Zhen: \sphinxhref{mailto:dizhen318@gmail.com}{dizhen318@gmail.com}
) for improvements.

\sphinxstepscope


\chapter{Preliminary}
\label{\detokenize{prelim:preliminary}}\label{\detokenize{prelim:prelim}}\label{\detokenize{prelim::doc}}
\sphinxAtStartPar
In this chapter, we will introduce some math and NLP preliminaries which is highly
used in Generative AI.


\section{Math Preliminary}
\label{\detokenize{prelim:math-preliminary}}

\subsection{Vector}
\label{\detokenize{prelim:vector}}
\sphinxAtStartPar
A vector is a mathematical representation of data characterized by both magnitude and
direction. In this context, each data point is represented as a feature vector, with
each component corresponding to a specific feature or attribute of the data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{gensim}\PYG{n+nn}{.}\PYG{n+nn}{downloader} \PYG{k}{as} \PYG{n+nn}{api}
\PYG{c+c1}{\PYGZsh{} Download pre\PYGZhy{}trained GloVe model}
\PYG{n}{glove\PYGZus{}vectors} \PYG{o}{=} \PYG{n}{api}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{glove\PYGZhy{}twitter\PYGZhy{}25}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get word vectors (embeddings)}
\PYG{n}{word1} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{king}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{word2} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{queen}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} embedding}
\PYG{n}{king} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word1}\PYG{p}{]}
\PYG{n}{queen} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word2}\PYG{p}{]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{king}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{queen}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{king}\PYG{p}{:}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.74501}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11992}   \PYG{l+m+mf}{0.37329}   \PYG{l+m+mf}{0.36847}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4472}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2288}    \PYG{l+m+mf}{0.70118}
\PYG{l+m+mf}{0.82872}   \PYG{l+m+mf}{0.39486}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.58347}   \PYG{l+m+mf}{0.41488}   \PYG{l+m+mf}{0.37074}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.6906}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20101}
\PYG{l+m+mf}{0.11472}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34661}   \PYG{l+m+mf}{0.36208}   \PYG{l+m+mf}{0.095679} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01765}   \PYG{l+m+mf}{0.68498}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.049013}
\PYG{l+m+mf}{0.54049}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21005}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65397}   \PYG{l+m+mf}{0.64556} \PYG{p}{]}
\PYG{n}{queen}\PYG{p}{:}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.1266}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52064}   \PYG{l+m+mf}{0.45565}   \PYG{l+m+mf}{0.21079}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05081}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65158}   \PYG{l+m+mf}{1.1395}
\PYG{l+m+mf}{0.69897}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20612}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.71803}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02811}   \PYG{l+m+mf}{0.10977}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.3089}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.49299}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51375}   \PYG{l+m+mf}{0.10363}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11764}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.084972}  \PYG{l+m+mf}{0.02558}   \PYG{l+m+mf}{0.6859}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.29196}
\PYG{l+m+mf}{0.4594}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.39955}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40371}   \PYG{l+m+mf}{0.31828} \PYG{p}{]}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{vector}.png}
\caption{Vector}\label{\detokenize{prelim:id1}}\label{\detokenize{prelim:fig-logo}}\end{figure}


\subsection{Norm}
\label{\detokenize{prelim:norm}}
\sphinxAtStartPar
A norm is a function that maps a vector to a single positive value, representing its
magnitude. Norms are essential for calculating distances between vectors, which play
a crucial role in measuring prediction errors, performing feature selection, and
applying regularization techniques in models.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{1Bauo}.png}
\caption{Geometrical Interpretation of Norm (\sphinxhref{https://math.stackexchange.com/questions/805954/what-does-the-dot-product-of-two-vectors-represent}{source\_1})}\label{\detokenize{prelim:id2}}\label{\detokenize{prelim:fig-1bauo}}\end{figure}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Formula:
\begin{quote}

\sphinxAtStartPar
The \(\displaystyle \ell^p\) norm for \(\vec{v} = (v_1, v_2, \cdots, v_n)\) is
\begin{equation*}
\begin{split}||\vec{v}||_p = \sqrt[p]{|v_1|^p + |v_2|^p + \cdots +|v_n|^p }\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^1\) norm: Sum of absolute values of vector components, often used for feature selection due to its tendency to produce sparse solutions.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} l1 norm}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n+nb}{ord}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{}    max(sum(abs(x), axis=0))}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 13.188952}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^2\) norm: Square root of the sum of squared vector components, the most common norm used in many machine learning algorithms.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} l2 norm}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n+nb}{ord}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 4.3206835}
\end{sphinxVerbatim}
\end{quote}

\item {} 
\sphinxAtStartPar
\(\displaystyle \ell^\infty\) norm (Maximum norm): The largest absolute value of a vector component.

\end{itemize}


\subsection{Distances}
\label{\detokenize{prelim:distances}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Manhattan Distance (\(\displaystyle \ell^1\) Distance)
\begin{quote}

\sphinxAtStartPar
Also known as taxicab or city block distance, Manhattan distance measures the absolute differences
between the components of two vectors. It represents the distance a point would travel along grid
lines in a Cartesian plane, similar to navigating through city streets.

\sphinxAtStartPar
For two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\), the
Manhattan Distance distance \(d(\vec{u},\vec{v})\) is
\begin{equation*}
\begin{split}d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_1 = |u_1-v_1| + |u_2-v_2|+ \cdots +|u_n-v_n|\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
Euclidean Distance (\(\displaystyle \ell^2\) Distance)
\begin{quote}

\sphinxAtStartPar
Euclidean distance is the most common way to measure the distance between two points (vectors) in space.
It is essentially the straight\sphinxhyphen{}line distance between them, calculated using the Pythagorean theorem.

\sphinxAtStartPar
For two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\), the
Euclidean Distance distance \(d(\vec{u},\vec{v})\) is
\begin{equation*}
\begin{split}d(\vec{u},\vec{v}) = ||\vec{u}-\vec{v}||_2 = \sqrt{(u_1-v_1)^2 + (u_2-v_2)^2+ \cdots +(u_n-v_n)^2}\end{split}
\end{equation*}\end{quote}

\item {} 
\sphinxAtStartPar
Minkowski Distance (\(\displaystyle \ell^p\) Distance)
\begin{quote}

\sphinxAtStartPar
Minkowski distance is a generalization of both Euclidean and Manhattan distances. It incorporates a parameter,
\(p\), which allows for adjusting the sensitivity of the distance metric.
\end{quote}

\item {} 
\sphinxAtStartPar
Cos Similarity
\begin{quote}

\sphinxAtStartPar
Cosine similarity measures the angle between two vectors rather than their straight\sphinxhyphen{}line distance.
It evaluates the similarity of two vectors by focusing on their orientation rather than their magnitude.
This makes it particularly useful for high\sphinxhyphen{}dimensional data, such as text, where the direction of the
vectors is often more significant than their magnitude.

\sphinxAtStartPar
The Cos similarity for two vector \(\vec{u} = (u_1, u_2, \cdots, u_n)\) and \(\vec{v} = (v_1, v_2, \cdots, v_n)\) is
\begin{equation*}
\begin{split}cos(\theta) = \frac{\vec{u}\cdot\vec{v}}{||\vec{u}|| ||\vec{v}||}\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
1 means the vectors point in exactly the same direction (perfect similarity).

\item {} 
\sphinxAtStartPar
0 means they are orthogonal (no similarity).

\item {} 
\sphinxAtStartPar
\sphinxhyphen{}1 means they point in opposite directions (complete dissimilarity).

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compute cosine similarity between the two word vectors}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{king}\PYG{p}{,}\PYG{n}{queen}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{king}\PYG{p}{)}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{queen}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} 0.92024213}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compute cosine similarity between the two word vectors}
\PYG{n}{similarity} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{n}{word1}\PYG{p}{,} \PYG{n}{word2}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{king}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{queen}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine similarity between }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{similarity}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.74501}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11992}   \PYG{l+m+mf}{0.37329}   \PYG{l+m+mf}{0.36847}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4472}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2288}    \PYG{l+m+mf}{0.70118}
\PYG{l+m+mf}{0.82872}   \PYG{l+m+mf}{0.39486}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.58347}   \PYG{l+m+mf}{0.41488}   \PYG{l+m+mf}{0.37074}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.6906}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20101}
\PYG{l+m+mf}{0.11472}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34661}   \PYG{l+m+mf}{0.36208}   \PYG{l+m+mf}{0.095679} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01765}   \PYG{l+m+mf}{0.68498}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.049013}
\PYG{l+m+mf}{0.54049}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21005}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65397}   \PYG{l+m+mf}{0.64556} \PYG{p}{]}
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.1266}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52064}   \PYG{l+m+mf}{0.45565}   \PYG{l+m+mf}{0.21079}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05081}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.65158}   \PYG{l+m+mf}{1.1395}
\PYG{l+m+mf}{0.69897}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20612}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.71803}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02811}   \PYG{l+m+mf}{0.10977}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{3.3089}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.49299}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51375}   \PYG{l+m+mf}{0.10363}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11764}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.084972}  \PYG{l+m+mf}{0.02558}   \PYG{l+m+mf}{0.6859}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.29196}
\PYG{l+m+mf}{0.4594}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.39955}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40371}   \PYG{l+m+mf}{0.31828} \PYG{p}{]}
\PYG{n}{Cosine} \PYG{n}{similarity} \PYG{n}{between} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{and} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.920242190361023}
\end{sphinxVerbatim}
\end{quote}

\end{itemize}


\section{NLP Preliminary}
\label{\detokenize{prelim:nlp-preliminary}}

\subsection{Vocabulary}
\label{\detokenize{prelim:vocabulary}}
\sphinxAtStartPar
In Natural Language Processing (NLP), \sphinxstylestrong{vocabulary} refers to the complete set of unique words or tokens
that a model recognizes or works with during training and inference. Vocabulary plays a critical role in
text processing and understanding, as it defines the scope of linguistic units a model can handle.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Types of Vocabulary in NLP
\begin{quote}

\sphinxAtStartPar
1. \sphinxstylestrong{Word\sphinxhyphen{}level Vocabulary}:
\sphinxhyphen{} Each word in the text is treated as a unique token.
\sphinxhyphen{} For example, the sentence ‚ÄúI love NLP‚Äù would generate the vocabulary: \sphinxcode{\sphinxupquote{\{I, love, NLP\}}}.

\sphinxAtStartPar
2. \sphinxstylestrong{Subword\sphinxhyphen{}level Vocabulary}:
\sphinxhyphen{} Text is broken down into smaller units like prefixes, suffixes, or character sequences.
\sphinxhyphen{} For example, the word ‚Äúloving‚Äù might be split into \sphinxcode{\sphinxupquote{\{lov, ing\}}} using techniques like Byte Pair Encoding (BPE) or SentencePiece.
\sphinxhyphen{} Subword vocabularies handle rare or unseen words more effectively.

\sphinxAtStartPar
3. \sphinxstylestrong{Character\sphinxhyphen{}level Vocabulary}:
\sphinxhyphen{} Each character is treated as a token.
\sphinxhyphen{} For example, the word ‚Äúlove‚Äù would generate the vocabulary: \sphinxcode{\sphinxupquote{\{l, o, v, e\}}}.
\end{quote}

\item {} 
\sphinxAtStartPar
Importance of Vocabulary
\begin{quote}

\sphinxAtStartPar
1. \sphinxstylestrong{Text Representation}:
\sphinxhyphen{} Vocabulary is the basis for converting text into numerical representations like one\sphinxhyphen{}hot vectors, embeddings, or input IDs for machine learning models.

\sphinxAtStartPar
2. \sphinxstylestrong{Model Efficiency}:
\sphinxhyphen{} A larger vocabulary increases the model‚Äôs memory and computational requirements.
\sphinxhyphen{} A smaller vocabulary may lack the capacity to represent all words effectively, leading to a loss of meaning.

\sphinxAtStartPar
3. \sphinxstylestrong{Handling Out\sphinxhyphen{}of\sphinxhyphen{}Vocabulary (OOV) Words}:
\sphinxhyphen{} Words not present in the vocabulary are either replaced with a special token like \sphinxcode{\sphinxupquote{\textless{}UNK\textgreater{}}} or processed using subword/character\sphinxhyphen{}based techniques.
\end{quote}

\item {} 
\sphinxAtStartPar
Building a Vocabulary
\begin{quote}

\sphinxAtStartPar
Common practices include:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Tokenizing the text into words, subwords, or characters.

\item {} 
\sphinxAtStartPar
Counting the frequency of tokens.

\item {} 
\sphinxAtStartPar
Keeping only the most frequent tokens up to a predefined size (e.g., top 50,000 tokens).

\item {} 
\sphinxAtStartPar
Adding special tokens like \sphinxcode{\sphinxupquote{\textless{}PAD\textgreater{}}}, \sphinxcode{\sphinxupquote{\textless{}UNK\textgreater{}}}, \sphinxcode{\sphinxupquote{\textless{}BOS\textgreater{}}} (beginning of sentence), and \sphinxcode{\sphinxupquote{\textless{}EOS\textgreater{}}} (end of sentence).

\end{enumerate}
\end{quote}

\item {} 
\sphinxAtStartPar
Challenges

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Balancing Vocabulary Size}:
A larger vocabulary increases the richness of representation but requires more computational resources.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Domain\sphinxhyphen{}specific Vocabularies}:
In specialized fields like medicine or law, standard vocabularies may not be sufficient, requiring domain\sphinxhyphen{}specific tokenization strategies.

\end{itemize}


\subsection{Tagging}
\label{\detokenize{prelim:tagging}}
\sphinxAtStartPar
Tagging in NLP refers to the process of assigning labels or annotations
to words, phrases, or other linguistic units in a text. These labels provide additional information about
the syntactic, semantic, or structural role of the elements in the text.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Types of Tagging
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Part\sphinxhyphen{}of\sphinxhyphen{}Speech (POS) Tagging}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Assigns grammatical tags (e.g., noun, verb, adjective) to each word in a sentence.

\item {} 
\sphinxAtStartPar
Example: For the sentence ‚ÄúThe dog barks,‚Äù the tags might be:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{The/DET}} (Determiner)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{dog/NOUN}} (Noun)
\sphinxhyphen{} \sphinxcode{\sphinxupquote{barks/VERB}} (Verb).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Named Entity Recognition (NER) Tagging}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Identifies and classifies named entities in a text, such as names of people, organizations, locations, dates, or monetary values.

\item {} 
\sphinxAtStartPar
Example: In the sentence ‚ÄúJohn works at Google in California,‚Äù the tags might be:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{John/PERSON}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{Google/ORGANIZATION}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{California/LOCATION}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chunking (Syntactic Tagging)}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Groups words into syntactic chunks like noun phrases (NP) or verb phrases (VP).

\item {} 
\sphinxAtStartPar
Example: For the sentence ‚ÄúThe quick brown fox jumps,‚Äù a chunking result might be:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{{[}NP The quick brown fox{]} {[}VP jumps{]}}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sentiment Tagging}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Assigns sentiment labels (e.g., positive, negative, neutral) to words, phrases, or entire documents.

\item {} 
\sphinxAtStartPar
Example: The word ‚Äúhappy‚Äù might be tagged as \sphinxcode{\sphinxupquote{positive}}, while ‚Äúsad‚Äù might be tagged as \sphinxcode{\sphinxupquote{negative}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dependency Parsing Tags}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Identifies the grammatical relationships between words in a sentence, such as subject, object, or modifier.

\item {} \begin{description}
\sphinxlineitem{Example: In ‚ÄúShe enjoys cooking,‚Äù the tags might show:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{She/nsubj}} (nominal subject)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{enjoys/ROOT}} (root of the sentence)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{cooking/dobj}} (direct object).

\end{itemize}

\end{description}

\end{itemize}

\end{enumerate}

\item {} 
\sphinxAtStartPar
Importance of Tagging
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Understanding Language Structure}: Tags help NLP models understand the grammatical and syntactic structure of text.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Improving Downstream Tasks}: Tagging is foundational for tasks like machine
translation, sentiment analysis, question answering, and summarization.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Feature Engineering}: Tags serve as features for training machine learning models in
text classification or sequence labeling tasks.

\end{itemize}

\item {} 
\sphinxAtStartPar
Tagging Techniques
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rule\sphinxhyphen{}based Tagging}: Relies on predefined linguistic rules to assign tags.
Example: Using dictionaries or regular expressions to match specific patterns.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistical Tagging}: Uses probabilistic models like Hidden Markov Models (HMMs)
to predict tags based on word sequences.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Neural Network\sphinxhyphen{}based Tagging}: Employs deep learning models like LSTMs, GRUs, or Transformers
to tag text with high accuracy.

\end{enumerate}

\item {} 
\sphinxAtStartPar
Challenges
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ambiguity}:Words with multiple meanings can lead to incorrect tagging.
Example: The word ‚Äúbank‚Äù could mean a financial institution or a riverbank.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Domain\sphinxhyphen{}Specific Language}: General tagging models may fail to perform well on specialized text
like medical or legal documents.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data Sparsity}: Rare words or phrases may lack sufficient training data for accurate tagging.

\end{itemize}

\end{itemize}


\subsection{Lemmatization}
\label{\detokenize{prelim:lemmatization}}
\sphinxAtStartPar
Lemmatization in NLP is the process of reducing a word to its base or dictionary form, known as
the \sphinxstylestrong{lemma}. Unlike stemming, which simply removes word suffixes, lemmatization considers
the context and grammatical role of the word to produce a linguistically accurate root form.
\begin{itemize}
\item {} 
\sphinxAtStartPar
How Lemmatization Works
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Contextual Analysis}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lemmatization relies on a vocabulary (lexicon) and morphological analysis to identify a word‚Äôs base form.

\item {} 
\sphinxAtStartPar
For example:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{running}} \(\rightarrow\) \sphinxcode{\sphinxupquote{run}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{better}} \(\rightarrow\) \sphinxcode{\sphinxupquote{good}}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Part\sphinxhyphen{}of\sphinxhyphen{}Speech (POS) Tagging}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The process uses POS tags to determine the correct lemma for a word.

\item {} 
\sphinxAtStartPar
Example:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{barking}} (verb) \(\rightarrow\) \sphinxcode{\sphinxupquote{bark}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{barking}} (adjective, as in ‚Äúbarking dog‚Äù) \(\rightarrow\) \sphinxcode{\sphinxupquote{barking}}.

\end{itemize}

\end{enumerate}

\item {} 
\sphinxAtStartPar
Importance of Lemmatization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Improves Text Normalization}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lemmatization helps normalize text by grouping different forms of a word into a single representation.

\item {} 
\sphinxAtStartPar
Example:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{run}}, \sphinxcode{\sphinxupquote{running}}, and \sphinxcode{\sphinxupquote{ran}} \(\rightarrow\) \sphinxcode{\sphinxupquote{run}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Enhances NLP Applications}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Lemmatized text improves the performance of tasks like information retrieval, text classification, and sentiment analysis.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reduces Vocabulary Size}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
By mapping inflected forms to their base form, lemmatization reduces redundancy in text, resulting in a smaller vocabulary.

\end{itemize}

\end{enumerate}

\item {} 
\sphinxAtStartPar
Lemmatization vs. Stemming
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Lemmatization}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Produces linguistically accurate root forms.

\item {} 
\sphinxAtStartPar
Considers the word‚Äôs context and POS.

\item {} 
\sphinxAtStartPar
Example:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{studies}} \(\rightarrow\) \sphinxcode{\sphinxupquote{study}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Stemming}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Applies heuristic rules to strip word suffixes without considering context.

\item {} 
\sphinxAtStartPar
May produce non\sphinxhyphen{}dictionary forms.

\item {} 
\sphinxAtStartPar
Example:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{studies}} \(\rightarrow\) \sphinxcode{\sphinxupquote{studi}}.

\end{itemize}

\end{itemize}

\item {} 
\sphinxAtStartPar
Techniques for Lemmatization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rule\sphinxhyphen{}Based Lemmatization}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Relies on predefined linguistic rules and dictionaries.

\item {} 
\sphinxAtStartPar
Example: WordNet\sphinxhyphen{}based lemmatizers.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistical Lemmatization}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Uses probabilistic models to predict lemmas based on the context.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep Learning\sphinxhyphen{}Based Lemmatization}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Employs neural networks and sequence\sphinxhyphen{}to\sphinxhyphen{}sequence models for highly accurate lemmatization in complex contexts.

\end{itemize}

\end{enumerate}

\item {} 
\sphinxAtStartPar
Challenges
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ambiguity}:
Words with multiple meanings may result in incorrect lemmatization without proper context.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Example:
\sphinxhyphen{} \sphinxcode{\sphinxupquote{left}} (verb) \(\rightarrow\) \sphinxcode{\sphinxupquote{leave}}
\sphinxhyphen{} \sphinxcode{\sphinxupquote{left}} (noun/adjective) \(\rightarrow\) \sphinxcode{\sphinxupquote{left}}.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Language\sphinxhyphen{}Specific Complexity}:
Lemmatization rules vary widely across languages, requiring language\sphinxhyphen{}specific tools and resources.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource Dependency}:
Lemmatizers require extensive lexicons and morphological rules, which can be resource\sphinxhyphen{}intensive to develop.

\end{itemize}

\end{itemize}


\subsection{Tokenization}
\label{\detokenize{prelim:tokenization}}
\sphinxAtStartPar
Tokenization in NLP refers to the process of splitting a text into smaller units, called \sphinxstylestrong{tokens}, which
can be words, subwords, sentences, or characters. These tokens serve as the basic building blocks for further
analysis in NLP tasks.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Types of Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Word Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Splits the text into individual words or terms.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Sentence: ‚ÄúI love NLP.‚Äù

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"I", "love", "NLP"{]}}}.

\end{itemize}

\end{description}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sentence Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Divides a text into sentences.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Text: ‚ÄúI love NLP. It‚Äôs amazing.‚Äù

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"I love NLP.", "It‚Äôs amazing."{]}}}.

\end{itemize}

\end{description}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Subword Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Breaks words into smaller units, often using methods like Byte Pair Encoding (BPE) or SentencePiece.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Word: \sphinxcode{\sphinxupquote{unhappiness}}.

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"un", "happiness"{]}}} (or subword units like \sphinxcode{\sphinxupquote{{[}"un", "happi", "ness"{]}}}).

\end{itemize}

\end{description}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{3}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Character Tokenization}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Treats each character in a word as a separate token.

\item {} \begin{description}
\sphinxlineitem{Example:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Word: \sphinxcode{\sphinxupquote{hello}}.

\item {} 
\sphinxAtStartPar
Tokens: \sphinxcode{\sphinxupquote{{[}"h", "e", "l", "l", "o"{]}}}.

\end{itemize}

\end{description}

\end{itemize}

\item {} 
\sphinxAtStartPar
Importance of Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Text Preprocessing}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tokenization is the first step in many NLP tasks like text classification, translation, and
summarization, as it converts text into manageable pieces.

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Text Representation}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tokens are converted into numerical representations (e.g., word embeddings) for model input
in tasks like sentiment analysis, named entity recognition (NER), or language modeling.

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Improving Accuracy}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Proper tokenization ensures that a model processes text at the correct granularity (e.g.,
words or subwords), improving accuracy for tasks like machine translation or text generation.

\end{itemize}

\item {} 
\sphinxAtStartPar
Challenges of Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ambiguity}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Certain words or phrases can be tokenized differently based on context.

\item {} 
\sphinxAtStartPar
Example: ‚ÄúNew York‚Äù can be treated as one token (location) or two separate tokens (\sphinxcode{\sphinxupquote{{[}"New", "York"{]}}}).

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Handling Punctuation}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Deciding how to treat punctuation marks can be challenging. For example, should commas, periods,
or quotes be treated as separate tokens or grouped with adjacent words?

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multi\sphinxhyphen{}word Expressions (MWEs)}:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Some expressions consist of multiple words that should be treated as a single token, such as ‚ÄúNew York‚Äù or ‚Äúmachine learning.‚Äù

\end{itemize}

\item {} 
\sphinxAtStartPar
Techniques for Tokenization
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rule\sphinxhyphen{}Based Tokenization}: Uses predefined rules to split text based on spaces, punctuation, and other delimiters.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistical and Machine Learning\sphinxhyphen{}Based Tokenization}: Uses trained models to predict token boundaries based on patterns learned from large corpora.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Deep Learning\sphinxhyphen{}Based Tokenization}: Modern tokenization models, such as those used in transformers (e.g., BERT, GPT), may rely on subword tokenization and neural networks to handle complex tokenization tasks.

\end{enumerate}

\end{itemize}


\subsection{BERT Tokenization}
\label{\detokenize{prelim:bert-tokenization}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Vocabulary: The BERT Tokenizer‚Äôs vocabulary contains 30,522 unique tokens.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{BertTokenizer}\PYG{p}{,} \PYG{n}{BertModel}
\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{BertTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bert\PYGZhy{}base\PYGZhy{}uncased}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} model = BertModel.from\PYGZus{}pretrained(\PYGZdq{}bert\PYGZhy{}base\PYGZhy{}uncased\PYGZdq{})}

\PYG{c+c1}{\PYGZsh{} vocabulary size}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{vocab\PYGZus{}size}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} vocabulary}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{vocab}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} vocabulary size}
\PYG{l+m+mi}{30522}

\PYG{c+c1}{\PYGZsh{} vocabulary}
\PYG{n}{OrderedDict}\PYG{p}{(}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[PAD]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[unused0]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
              \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}
              \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{writing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{3015}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bay}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{3016}\PYG{p}{)}\PYG{p}{,}
              \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}
              \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsh{}\PYGZsh{}?}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{30520}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsh{}\PYGZsh{}\PYGZti{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{30521}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Tokens and IDs
\begin{itemize}
\item {} 
\sphinxAtStartPar
Tokens to IDs

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{text} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gen AI is awesome}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{encoded\PYGZus{}input} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} tokens to ids}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{encoded\PYGZus{}input}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} output}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,}  \PYG{l+m+mi}{8991}\PYG{p}{,}  \PYG{l+m+mi}{9932}\PYG{p}{,}  \PYG{l+m+mi}{2003}\PYG{p}{,} \PYG{l+m+mi}{12476}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{token\PYGZus{}type\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{attention\PYGZus{}mask}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
You might notice that there are only four words, yet we have six token IDs.
This is due to the inclusion of two additional special tokens \sphinxcode{\sphinxupquote{{[}CLS{]}}} and \sphinxcode{\sphinxupquote{{[}SEP{]}}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{x} \PYG{p}{:} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{+} \PYG{n}{text}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} output}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{101}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{8991}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{9932}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2003}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{12476}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Special Tokens

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Special tokens}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{x} \PYG{p}{:} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[MASK]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[EOS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} tokens to ids}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{101}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[MASK]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{103}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[EOS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1031}\PYG{p}{,} \PYG{l+m+mi}{1041}\PYG{p}{,} \PYG{l+m+mi}{2891}\PYG{p}{,} \PYG{l+m+mi}{1033}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
IDs to tokens

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} ids to tokens}
\PYG{n}{token\PYGZus{}id} \PYG{o}{=} \PYG{n}{encoded\PYGZus{}input}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{convert\PYGZus{}ids\PYGZus{}to\PYGZus{}tokens}\PYG{p}{(}\PYG{n+nb}{id}\PYG{p}{,} \PYG{n}{skip\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}\PYG{n+nb}{id} \PYGZbs{}
      \PYG{k}{for} \PYG{n+nb}{id} \PYG{o+ow}{in} \PYG{n}{token\PYGZus{}id}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} output}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{101}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{8991}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{9932}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{2003}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{12476}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{102}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}vocabulary tokens

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{text} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gen AI is awesome üëç}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{encoded\PYGZus{}input} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{x} \PYG{p}{:} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{+} \PYG{n}{text}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{convert\PYGZus{}ids\PYGZus{}to\PYGZus{}tokens}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{skip\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{} output}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{101}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{8991}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{9932}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2003}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{12476}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{üëç}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{p}{[}\PYG{n}{UNK}\PYG{p}{]}
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Subword Tokenization

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Subword Tokenization}
\PYG{n}{text} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GenAI is awesome üëç}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{x} \PYG{p}{:} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{+} \PYG{n}{text}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{convert\PYGZus{}ids\PYGZus{}to\PYGZus{}tokens}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{skip\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} output}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{101}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GenAI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{8991}\PYG{p}{,} \PYG{l+m+mi}{4886}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2003}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{12476}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{üëç}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{p}{[}\PYG{n}{UNK}\PYG{p}{]}
\end{sphinxVerbatim}

\end{itemize}

\end{itemize}


\section{Platform and Packages}
\label{\detokenize{prelim:platform-and-packages}}

\subsection{Google Colab}
\label{\detokenize{prelim:google-colab}}
\sphinxAtStartPar
\sphinxstylestrong{Google Colab} (short for Colaboratory) is a free, cloud\sphinxhyphen{}based platform that provides users with the ability to write
and execute Python code in an interactive notebook environment. It is based on Jupyter notebooks and is powered by
Google Cloud services, allowing for seamless integration with Google Drive and other Google services. We will primarily
use Google Colab with free T4 GPU runtime throughout this book.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Key Features

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Free Access to GPUs and TPUs}
Colab offers free access to Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), making it an ideal environment for machine learning, deep learning, and other computationally intensive tasks.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Integration with Google Drive}
You can store and access notebooks directly from your Google Drive, making it easy to collaborate with others and keep your projects organized.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{No Setup Required}
Since Colab is entirely cloud\sphinxhyphen{}based, you don‚Äôt need to worry about setting up an environment or managing dependencies. Everything is ready to go out of the box.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Support for Python Libraries}
Colab comes pre\sphinxhyphen{}installed with many popular Python libraries, including TensorFlow, PyTorch, Keras, and OpenCV, among others. You can also install any additional libraries using \sphinxtitleref{pip}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Collaborative Features}
Multiple users can work on the same notebook simultaneously, making it ideal for collaboration. Changes are synchronized in real\sphinxhyphen{}time.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rich Media Support}
Colab supports the inclusion of rich media, such as images, videos, and LaTeX equations, directly within the notebook. This makes it a great tool for data analysis, visualization, and educational purposes.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Easy Sharing}
Notebooks can be easily shared with others via a shareable link, just like Google Docs. Permissions can be set for viewing or editing the document.

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
GPU Activation
\sphinxcode{\sphinxupquote{Runtime \sphinxhyphen{}\sphinxhyphen{}\textgreater{} change runtime type \sphinxhyphen{}\sphinxhyphen{}\textgreater{} T4/A100 GPU}}

\end{itemize}


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\sphinxthistablewithborderlessstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook
\noindent\sphinxincludegraphics[width=1.000\linewidth]{{runtime}.png}
&
\noindent\sphinxincludegraphics[width=1.000\linewidth]{{T4}.png}
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{HuggingFace}
\label{\detokenize{prelim:huggingface}}
\sphinxAtStartPar
\sphinxstylestrong{Hugging Face} is a company and open\sphinxhyphen{}source community focused on providing tools and resources for NLP
and machine learning. It is best known for its popular \sphinxstylestrong{Transformers} library, which allows easy access
to pre\sphinxhyphen{}trained models for a wide variety of NLP tasks. MOreover,  Hugging Face‚Äôs libraries provide simple
Python APIs that make it easy to load models, preprocess data, and run inference. This simplicity allows
both beginners and advanced users to leverage cutting\sphinxhyphen{}edge NLP models. We will mainly use the embedding models
and Large Language Models (LLMs) from \sphinxstylestrong{Hugging Face Model Hub} central repository.


\subsection{Ollama}
\label{\detokenize{prelim:ollama}}
\sphinxAtStartPar
Ollama is a package designed to run LLMs locally on your personal device or
server, rather than relying on external cloud services. It provides a simple
interface to download and use AI models tailored for various tasks, ensuring
privacy and control over data while still leveraging the power of LLMs.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Key features of Ollama:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Local Execution: Models run entirely on your hardware, making it ideal for users who prioritize data privacy.

\item {} 
\sphinxAtStartPar
Pre\sphinxhyphen{}trained Models: Offers a curated set of LLMs optimized for local usage.

\item {} 
\sphinxAtStartPar
Cross\sphinxhyphen{}Platform: Compatible with macOS, Linux, and other operating systems, depending on hardware specifications.

\item {} 
\sphinxAtStartPar
Ease of Use: Designed to make setting up and using local AI models simple for non\sphinxhyphen{}technical users.

\item {} 
\sphinxAtStartPar
Efficiency: Focused on lightweight models optimized for local performance without needing extensive computational resources.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
To simplify the management of access tokens for various LLMs, we will use Ollama in Google Colab.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Ollama installation in Google Colab
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
colab\sphinxhyphen{}xterm

\end{enumerate}
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
!pip\PYG{+w}{ }install\PYG{+w}{ }colab\PYGZhy{}xterm
\PYGZpc{}load\PYGZus{}ext\PYG{+w}{ }colabxterm
\end{sphinxVerbatim}
\end{quote}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
download ollama

\end{enumerate}
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/content\PYGZsh{}\PYG{+w}{ }curl\PYG{+w}{ }https://ollama.ai/install.sh\PYG{+w}{ }\PYG{p}{|}\PYG{+w}{ }sh
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{ollama_download}.png}
\end{figure}
\end{quote}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{2}
\item {} 
\sphinxAtStartPar
launch Ollama serve

\end{enumerate}
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/content\PYGZsh{}\PYG{+w}{ }ollama\PYG{+w}{ }serve
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{ollama_serve}.png}
\end{figure}
\end{quote}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{3}
\item {} 
\sphinxAtStartPar
download models

\end{enumerate}
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/content\PYGZsh{}\PYG{+w}{ }ollama\PYG{+w}{ }pull\PYG{+w}{ }mistral\PYG{+w}{ }\PYG{c+c1}{\PYGZsh{}llama3.2 \PYGZsh{}bge\PYGZhy{}m3}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{pull_models}.png}
\end{figure}
\end{quote}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{4}
\item {} 
\sphinxAtStartPar
check

\end{enumerate}
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
!ollama\PYG{+w}{ }list

\PYG{c+c1}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}
NAME\PYG{+w}{               }ID\PYG{+w}{              }SIZE\PYG{+w}{      }MODIFIED
llama3.2:latest\PYG{+w}{    }a80c4f17acd5\PYG{+w}{    }\PYG{l+m}{2}.0\PYG{+w}{ }GB\PYG{+w}{    }\PYG{l+m}{14}\PYG{+w}{ }seconds\PYG{+w}{ }ago
mistral:latest\PYG{+w}{     }f974a74358d6\PYG{+w}{    }\PYG{l+m}{4}.1\PYG{+w}{ }GB\PYG{+w}{    }About\PYG{+w}{ }a\PYG{+w}{ }minute\PYG{+w}{ }ago
\end{sphinxVerbatim}
\end{quote}

\end{itemize}


\subsection{langchain}
\label{\detokenize{prelim:langchain}}
\sphinxAtStartPar
LangChain is a powerful framework for building AI applications that combine the
capabilities of large language models with external tools, memory, and custom
workflows. It enables developers to create intelligent, context\sphinxhyphen{}aware,
and dynamic applications with ease.

\sphinxAtStartPar
It has widely applied in:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Conversational AI}
Create chatbots or virtual assistants that maintain context, integrate with APIs, and provide intelligent responses.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Knowledge Management}
Combine LLMs with external knowledge bases or databases to answer complex questions or summarize documents.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Automation}
Automate workflows by chaining LLMs with tools for decision\sphinxhyphen{}making, data extraction, or content generation.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Creative Applications}
Use LangChain for generating stories, crafting marketing copy, or producing artistic content.

\end{enumerate}

\sphinxAtStartPar
We will primarily use LangChain in this book. For instance, to work with downloaded Ollama LLMs, the \sphinxcode{\sphinxupquote{langchain\_ollama}}
package is required.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} chain of thought prompting}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}ollama}\PYG{n+nn}{.}\PYG{n+nn}{llms} \PYG{k+kn}{import} \PYG{n}{OllamaLLM}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}core}\PYG{n+nn}{.}\PYG{n+nn}{prompts} \PYG{k+kn}{import} \PYG{n}{ChatPromptTemplate}
\PYG{k+kn}{from} \PYG{n+nn}{langchain}\PYG{n+nn}{.}\PYG{n+nn}{output\PYGZus{}parsers} \PYG{k+kn}{import} \PYG{n}{CommaSeparatedListOutputParser}


\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Question: }\PYG{l+s+si}{\PYGZob{}question\PYGZcb{}}

\PYG{l+s+s2}{Answer: Let}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s think step by step.}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mistral}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model} \PYG{o}{|} \PYG{n}{output\PYGZus{}parser}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{question}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{What is Mixture of Experts(MoE) in AI?}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZob{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{answer}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{:}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{MoE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{or Mixture of Experts}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{is a neural network architecture that allows for }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{    efficient computation and model parallelism. It consists of multiple }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{experts}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{each of }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{    which is a smaller neural network that specializes in handling different parts of the input }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{    data. The final output is obtained by combining the outputs of these experts based on their }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{    expertise relevance to the input. This architecture is particularly useful in tasks where }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{    the data exhibits complex and diverse patterns.}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Word and Sentence Embedding}
\label{\detokenize{embedding:word-and-sentence-embedding}}\label{\detokenize{embedding:embedding}}\label{\detokenize{embedding::doc}}
\sphinxAtStartPar
Word embedding is a method in natural language processing (NLP) to represent words as dense
vectors of real numbers, capturing semantic relationships between them. Instead of treating
words as discrete symbols (like one\sphinxhyphen{}hot encoding), word embeddings map words into a
continuous vector space where similar words are located closer together.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{embedding_diagram}.png}
\caption{Embedding Diagram}\label{\detokenize{embedding:id2}}\label{\detokenize{embedding:fig-embedding}}\end{figure}


\section{Traditional word embeddings}
\label{\detokenize{embedding:traditional-word-embeddings}}
\sphinxAtStartPar
\sphinxstylestrong{Bag of Words (BoW)} is a simple and widely used text representation technique in natural
language processing (NLP). It represents a text (e.g., a document or a sentence) as a collection
of words, ignoring grammar, order, and context but keeping their frequency.

\sphinxAtStartPar
Key Features of Bag of Words:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Vocabulary Creation}:
\sphinxhyphen{} A list of all unique words in the dataset (the ‚Äúvocabulary‚Äù) is created.
\sphinxhyphen{} Each word becomes a feature.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Representation}:
\sphinxhyphen{} Each document is represented as a vector or a frequency count of words from the vocabulary.
\sphinxhyphen{} If a word from the vocabulary is present in the document, its count is included in the vector.
\sphinxhyphen{} Words not present in the document are assigned a count of zero.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simplicity}:
\sphinxhyphen{} The method is computationally efficient and straightforward.
\sphinxhyphen{} However, it ignores the sequence and semantic meaning of the words.

\end{enumerate}

\sphinxAtStartPar
Applications:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Text Classification

\item {} 
\sphinxAtStartPar
Sentiment Analysis

\item {} 
\sphinxAtStartPar
Document Similarity

\end{itemize}

\sphinxAtStartPar
Limitations:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Context Ignorance}:
\sphinxhyphen{} BoW does not capture word order or semantics.
\sphinxhyphen{} For example, ‚Äúnot good‚Äù and ‚Äúgood‚Äù might appear similar in BoW.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dimensionality}:
\sphinxhyphen{} As the vocabulary size increases, the vector representation grows, leading to high\sphinxhyphen{}dimensional data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sparse Representations}:
\sphinxhyphen{} Many entries in the vectors might be zeros, leading to sparsity.

\end{enumerate}


\subsection{One Hot Encoder}
\label{\detokenize{embedding:one-hot-encoder}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{Counter}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{OneHotEncoder}

\PYG{c+c1}{\PYGZsh{} sample corpus}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{word}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{python}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pyspark}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{genai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pyspark}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{python}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pyspark}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} corpus frequency}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Vocabulary frequency:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{Counter}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{word}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} corpus order}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Vocabulary order:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{sorted}\PYG{p}{(}\PYG{n+nb}{set}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{word}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} One\PYGZhy{}hot encode the data}
\PYG{n}{onehot\PYGZus{}encoder} \PYG{o}{=} \PYG{n}{OneHotEncoder}\PYG{p}{(}\PYG{n}{sparse\PYGZus{}output}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{onehot\PYGZus{}encoded} \PYG{o}{=} \PYG{n}{onehot\PYGZus{}encoder}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{word}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the encoded order base on the order of the copus}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Encoded representation:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{onehot\PYGZus{}encoded}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Vocabulary} \PYG{n}{frequency}\PYG{p}{:}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{python}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pyspark}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{genai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}

\PYG{n}{Vocabulary} \PYG{n}{order}\PYG{p}{:}
\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{genai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pyspark}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{python}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{Encoded} \PYG{n}{representation}\PYG{p}{:}
\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.} \PYG{l+m+mf}{0.} \PYG{l+m+mf}{1.}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{0.} \PYG{l+m+mf}{1.} \PYG{l+m+mf}{0.}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{1.} \PYG{l+m+mf}{0.} \PYG{l+m+mf}{0.}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{0.} \PYG{l+m+mf}{1.} \PYG{l+m+mf}{0.}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{0.} \PYG{l+m+mf}{0.} \PYG{l+m+mf}{1.}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{0.} \PYG{l+m+mf}{1.} \PYG{l+m+mf}{0.}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}


\subsection{CountVectorizer}
\label{\detokenize{embedding:countvectorizer}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}extraction}\PYG{n+nn}{.}\PYG{n+nn}{text} \PYG{k+kn}{import} \PYG{n}{CountVectorizer}

\PYG{c+c1}{\PYGZsh{} sample corpus}
\PYG{n}{corpus} \PYG{o}{=} \PYG{p}{[}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is hot}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Initialize the CountVectorizer}
\PYG{n}{vectorizer} \PYG{o}{=} \PYG{n}{CountVectorizer}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Fit and transform}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{vectorizer}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}


\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Vocabulary:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{vectorizer}\PYG{o}{.}\PYG{n}{get\PYGZus{}feature\PYGZus{}names\PYGZus{}out}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Embedded representation:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{toarray}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Vocabulary}\PYG{p}{:}
\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fun}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hot}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{Embedded} \PYG{n}{representation}\PYG{p}{:}
\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
To overcome these limitations, advanced techniques like \sphinxstylestrong{TF\sphinxhyphen{}IDF}, \sphinxstylestrong{word embeddings}
(e.g., Word2Vec, GloVe), and contextual embeddings (e.g., BERT) are often used.


\subsection{TF\sphinxhyphen{}IDF}
\label{\detokenize{embedding:tf-idf}}
\sphinxAtStartPar
\sphinxstylestrong{TF\sphinxhyphen{}IDF (Term Frequency\sphinxhyphen{}Inverse Document Frequency)} is a statistical measure used
in text analysis to evaluate the importance of a word in a document relative to a
collection (or corpus) of documents. It builds upon the \sphinxstylestrong{Bag of Words (BoW)} model
by not only considering the frequency of a word in a document but also taking
into account how common or rare the word is across the corpus. The pyspark implementation
can be found at \sphinxcite{reference:pyspark}.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Components of TF\sphinxhyphen{}ID

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{t}: the term in corpus.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{d}: the document.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{D}: the corpus.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{|D|}: the length of the corpus or total number of documents.
\begin{quote}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Document Frequency (DF)}:

\item {} 
\sphinxAtStartPar
\(DF(t,D)\): the number of documents that contains term \(t\).

\item {} \begin{description}
\sphinxlineitem{\sphinxstylestrong{Term Frequency (TF)}:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Measures how frequently a term appears in a document. The higher the frequency, the more important the term is assumed to be to that document.

\item {} 
\sphinxAtStartPar
Formula:

\end{itemize}
\begin{equation*}
\begin{split}TF(t, d) = \frac{\text{Number of occurrences of term } t \text{ in document } d}{\text{Total number of terms in document } d}\end{split}
\end{equation*}
\end{description}

\item {} \begin{description}
\sphinxlineitem{\sphinxstylestrong{Inverse Document Frequency (IDF)}:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Measures how important a term is by reducing the weight of common terms (like ‚Äúthe‚Äù or ‚Äúand‚Äù) that appear in many documents.

\item {} 
\sphinxAtStartPar
Formula:

\end{itemize}
\begin{equation*}
\begin{split}IDF(t, D) = \log\left(\frac{|D|+1}{DF(t,D) + 1}\right) + 1\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Adding 1 to the denominator avoids division by zero when a term is present in all documents.

\item {} 
\sphinxAtStartPar
Note that the IDF formula above differs from the standard textbook notation that defines the IDF

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The IDF formula above differs from the standard textbook notation that defines the IDF as
\begin{equation*}
\begin{split}IDF(t) = \log [ |D| / (DF(t,D) + 1) ]).\end{split}
\end{equation*}\end{sphinxadmonition}

\end{description}

\item {} \begin{description}
\sphinxlineitem{\sphinxstylestrong{TF\sphinxhyphen{}IDF Score}:}\begin{itemize}
\item {} 
\sphinxAtStartPar
The final score is the product of TF and IDF.

\item {} 
\sphinxAtStartPar
Formula:

\end{itemize}
\begin{equation*}
\begin{split}TF\text{-}IDF(t, d, D) = TF(t, d) \cdot IDF(t, D)\end{split}
\end{equation*}
\end{description}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{Counter}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}extraction}\PYG{n+nn}{.}\PYG{n+nn}{text} \PYG{k+kn}{import} \PYG{n}{TfidfVectorizer}

\PYG{c+c1}{\PYGZsh{} sample corpus}
\PYG{n}{corpus} \PYG{o}{=} \PYG{p}{[}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is hot}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Initialize the TfidfVectorizer}
\PYG{n}{vectorizer} \PYG{o}{=} \PYG{n}{TfidfVectorizer}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} norm default norm=\PYGZsq{}l2\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} Fit and transform}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{vectorizer}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Vocabulary:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{vectorizer}\PYG{o}{.}\PYG{n}{get\PYGZus{}feature\PYGZus{}names\PYGZus{}out}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} [item for row in matrix for item in row]}
\PYG{n}{corpus\PYGZus{}flatted} \PYG{o}{=} \PYG{p}{[}\PYG{n}{item} \PYG{k}{for} \PYG{n}{sub\PYGZus{}list} \PYG{o+ow}{in} \PYG{p}{[}\PYG{n}{s}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{corpus}\PYG{p}{]}
                     \PYG{k}{for} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n}{sub\PYGZus{}list}\PYG{p}{]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Vocabulary frequency:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{Counter}\PYG{p}{(}\PYG{n}{corpus\PYGZus{}flatted}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Embedded representation:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{toarray}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Vocabulary}\PYG{p}{:}
\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fun}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hot}\PYG{l+s+s1}{\PYGZsq{}} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{Vocabulary} \PYG{n}{frequency}\PYG{p}{:}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}

\PYG{n}{Embedded} \PYG{n}{representation}\PYG{p}{:}
\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.41285857} \PYG{l+m+mf}{0.69903033} \PYG{l+m+mf}{0.}         \PYG{l+m+mf}{0.41285857} \PYG{l+m+mf}{0.}         \PYG{l+m+mf}{0.41285857}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{0.41285857} \PYG{l+m+mf}{0.}         \PYG{l+m+mf}{0.69903033} \PYG{l+m+mf}{0.41285857} \PYG{l+m+mf}{0.}         \PYG{l+m+mf}{0.41285857}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{0.41285857} \PYG{l+m+mf}{0.}         \PYG{l+m+mf}{0.}         \PYG{l+m+mf}{0.41285857} \PYG{l+m+mf}{0.69903033} \PYG{l+m+mf}{0.41285857}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
The above results can be validated by the following steps (IDF in document 1):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Step 1: Vocabulary  `[\PYGZsq{}ai\PYGZsq{} \PYGZsq{}awesome\PYGZsq{} \PYGZsq{}fun\PYGZsq{} \PYGZsq{}gen\PYGZsq{} \PYGZsq{}hot\PYGZsq{} \PYGZsq{}is\PYGZsq{}]`}

\PYG{n}{tf\PYGZus{}idf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{term}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{vectorizer}\PYG{o}{.}\PYG{n}{get\PYGZus{}feature\PYGZus{}names\PYGZus{}out}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYGZbs{}
         \PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{term}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Step 2: |D|}
\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{|D|}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{vectorizer}\PYG{o}{.}\PYG{n}{get\PYGZus{}feature\PYGZus{}names\PYGZus{}out}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Step 3: Compute TF for doc 1:  Gen AI is awesome}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} TF for \PYGZdq{}ai\PYGZdq{} in Document 1 = 1 (appears once doc 1)}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} TF for \PYGZdq{}awesome\PYGZdq{} in Document 1 = 1 (appears once in doc 1)}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} TF for \PYGZdq{}fun\PYGZdq{} in Document 1 = 0 (does not appear in doc 1)}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} TF for \PYGZdq{}gen\PYGZdq{} in Document 1 = 1 (appear oncein doc 1 )}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} TF for \PYGZdq{}hot\PYGZdq{} in Document 1 = 0 (does not appear doc 1 )}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} TF for \PYGZdq{}is\PYGZdq{} in Document 1 = 1 (appear once in doc 1 )}

\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Step 4:  Compute DF for doc 1}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} DF For \PYGZdq{}ai\PYGZdq{}: Appears in all 3 documents.}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} DF For \PYGZdq{}awesome\PYGZdq{}: Appears in 1 document.}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} DF For \PYGZdq{}fun\PYGZdq{}: Appears in 1 document.}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} DF For \PYGZdq{}Gen\PYGZdq{}: Appears in all 3 documents.}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} DF For \PYGZdq{}hot\PYGZdq{}: Appears in 1 document.}
\PYG{c+c1}{\PYGZsh{} \PYGZhy{} DF For \PYGZdq{}is\PYGZdq{}: Appears in all 3 documents.}

\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Step 5: Compute IDF}
\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IDF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{p}{(}\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{|D|}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}

\PYG{c+c1}{\PYGZsh{} Step 6: Compute TF\PYGZhy{}IDF}
\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TF\PYGZhy{}IDF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{IDF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Step 7: l2 normlization}
\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TF\PYGZhy{}IDF(l2)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TF\PYGZhy{}IDF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{tf\PYGZus{}idf}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TF\PYGZhy{}IDF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tf\PYGZus{}idf}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
         \PYG{o}{|}\PYG{n}{D}\PYG{o}{|}  \PYG{n}{TF}  \PYG{n}{DF}       \PYG{n}{IDF}    \PYG{n}{TF}\PYG{o}{\PYGZhy{}}\PYG{n}{IDF}  \PYG{n}{TF}\PYG{o}{\PYGZhy{}}\PYG{n}{IDF}\PYG{p}{(}\PYG{n}{l2}\PYG{p}{)}
\PYG{n}{term}
\PYG{n}{ai}         \PYG{l+m+mi}{3}   \PYG{l+m+mi}{1}   \PYG{l+m+mi}{3}  \PYG{l+m+mf}{1.000000}  \PYG{l+m+mf}{1.000000}    \PYG{l+m+mf}{0.412859}
\PYG{n}{awesome}    \PYG{l+m+mi}{3}   \PYG{l+m+mi}{1}   \PYG{l+m+mi}{1}  \PYG{l+m+mf}{1.693147}  \PYG{l+m+mf}{1.693147}    \PYG{l+m+mf}{0.699030}
\PYG{n}{fun}        \PYG{l+m+mi}{3}   \PYG{l+m+mi}{0}   \PYG{l+m+mi}{1}  \PYG{l+m+mf}{1.693147}  \PYG{l+m+mf}{0.000000}    \PYG{l+m+mf}{0.000000}
\PYG{n}{gen}        \PYG{l+m+mi}{3}   \PYG{l+m+mi}{1}   \PYG{l+m+mi}{3}  \PYG{l+m+mf}{1.000000}  \PYG{l+m+mf}{1.000000}    \PYG{l+m+mf}{0.412859}
\PYG{n}{hot}        \PYG{l+m+mi}{3}   \PYG{l+m+mi}{0}   \PYG{l+m+mi}{1}  \PYG{l+m+mf}{1.693147}  \PYG{l+m+mf}{0.000000}    \PYG{l+m+mf}{0.000000}
\PYG{o+ow}{is}         \PYG{l+m+mi}{3}   \PYG{l+m+mi}{1}   \PYG{l+m+mi}{3}  \PYG{l+m+mf}{1.000000}  \PYG{l+m+mf}{1.000000}    \PYG{l+m+mf}{0.412859}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Fun Fact}

\sphinxAtStartPar
TfidfVectorizer is equivalent to CountVectorizer followed by TfidfTransformer.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}extraction}\PYG{n+nn}{.}\PYG{n+nn}{text} \PYG{k+kn}{import} \PYG{n}{CountVectorizer}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}extraction}\PYG{n+nn}{.}\PYG{n+nn}{text} \PYG{k+kn}{import} \PYG{n}{TfidfTransformer}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{pipeline} \PYG{k+kn}{import} \PYG{n}{Pipeline}

\PYG{c+c1}{\PYGZsh{} sample corpus}
\PYG{n}{corpus} \PYG{o}{=} \PYG{p}{[}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is hot}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} pipeline}
\PYG{n}{pipe} \PYG{o}{=} \PYG{n}{Pipeline}\PYG{p}{(}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{CountVectorizer}\PYG{p}{(}\PYG{n}{lowercase}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
               \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tfid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{TfidfTransformer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{pipe}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} TF}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{pipe}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}\PYG{o}{.}\PYG{n}{toarray}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} IDF}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{pipe}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tfid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{idf\PYGZus{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Pipeline}\PYG{p}{(}\PYG{n}{steps}\PYG{o}{=}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{CountVectorizer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tfid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{TfidfTransformer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{l+m+mi}{0} \PYG{l+m+mi}{0} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}
\PYG{p}{[}\PYG{l+m+mf}{1.}         \PYG{l+m+mf}{1.69314718} \PYG{l+m+mf}{1.69314718} \PYG{l+m+mf}{1.}         \PYG{l+m+mf}{1.69314718} \PYG{l+m+mf}{1.}        \PYG{p}{]}
\end{sphinxVerbatim}
\end{sphinxadmonition}
\end{quote}

\item {} 
\sphinxAtStartPar
Applications of TF\sphinxhyphen{}IDF
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Information Retrieval}: Ranking documents based on relevance to a query.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Text Classification}: Feature extraction for machine learning models.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Document Similarity}: Comparing documents by their weighted term vectors.

\end{enumerate}

\item {} 
\sphinxAtStartPar
Advantages
\begin{itemize}
\item {} 
\sphinxAtStartPar
Highlights important terms while reducing the weight of common terms.

\item {} 
\sphinxAtStartPar
Simple to implement and effective for many tasks.

\end{itemize}

\item {} 
\sphinxAtStartPar
Limitations
\begin{itemize}
\item {} 
\sphinxAtStartPar
Does not capture semantic relationships or word order.

\item {} 
\sphinxAtStartPar
Less effective for very large corpora or when working with very short documents.

\item {} 
\sphinxAtStartPar
Sparse representation due to high\sphinxhyphen{}dimensional feature vectors.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
For more advanced representations, embeddings like \sphinxstylestrong{Word2Vec} or \sphinxstylestrong{BERT} are often used.


\section{Static word embeddings}
\label{\detokenize{embedding:static-word-embeddings}}
\sphinxAtStartPar
Static word embeddings are word representations that assign a fixed vector to each word,
regardless of its context in a sentence or paragraph. These embeddings are pre\sphinxhyphen{}trained on
large corpora and remain unchanged during usage, making them ‚Äústatic.‚Äù These embeddings are
usually pre\sphinxhyphen{}trained on large text corpora using algorithms like Word2Vec, GloVe, or FastText.


\subsection{Word2Vec}
\label{\detokenize{embedding:word2vec}}\begin{itemize}
\item {} 
\sphinxAtStartPar
The Context Window

\item {} 
\sphinxAtStartPar
CBOW and Skip\sphinxhyphen{}Gram Model

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{gensim}
\PYG{k+kn}{from} \PYG{n+nn}{gensim}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{Word2Vec}
\PYG{k+kn}{from} \PYG{n+nn}{nltk}\PYG{n+nn}{.}\PYG{n+nn}{tokenize} \PYG{k+kn}{import} \PYG{n}{sent\PYGZus{}tokenize}\PYG{p}{,} \PYG{n}{word\PYGZus{}tokenize}

\PYG{c+c1}{\PYGZsh{} sample corpus}
\PYG{n}{corpus} \PYG{o}{=} \PYG{p}{[}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is hot}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}


\PYG{k}{def} \PYG{n+nf}{tokenize\PYGZus{}gensim}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}\PYG{p}{:}

   \PYG{n}{tokens} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
   \PYG{c+c1}{\PYGZsh{} iterate through each sentence in the corpus}
   \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{corpus}\PYG{p}{:}

      \PYG{c+c1}{\PYGZsh{} tokenize the sentence into words}
      \PYG{n}{temp} \PYG{o}{=} \PYG{n}{gensim}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{tokenize}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{lowercase}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{deacc}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYGZbs{}
                                    \PYG{n}{errors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{to\PYGZus{}lower}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYGZbs{}
                                    \PYG{n}{lower}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

      \PYG{n}{tokens}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{temp}\PYG{p}{)}\PYG{p}{)}

   \PYG{k}{return} \PYG{n}{tokens}


\PYG{n}{tokens} \PYG{o}{=} \PYG{n}{tokenize\PYGZus{}gensim}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create Word2Vec model}
\PYG{c+c1}{\PYGZsh{} sg (\PYGZob{}0, 1\PYGZcb{}, optional) \textendash{} Training algorithm: 1 for skip\PYGZhy{}gram; otherwise CBOW.}
\PYG{c+c1}{\PYGZsh{} CBOW}
\PYG{n}{model1} \PYG{o}{=} \PYG{n}{gensim}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Word2Vec}\PYG{p}{(}\PYG{n}{tokens}\PYG{p}{,} \PYG{n}{sg}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{min\PYGZus{}count}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                              \PYG{n}{vector\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{window}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Vocabulary}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model1}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{key\PYGZus{}to\PYGZus{}index}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model1}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{get\PYGZus{}normed\PYGZus{}vectors}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine similarity between }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{gen}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ai}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ \PYGZhy{} Word2Vec(CBOW) : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{model1}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Create Word2Vec model}
\PYG{c+c1}{\PYGZsh{} sg (\PYGZob{}0, 1\PYGZcb{}, optional) \textendash{} Training algorithm: 1 for skip\PYGZhy{}gram; otherwise CBOW.}
\PYG{c+c1}{\PYGZsh{} skip\PYGZhy{}gram}
\PYG{n}{model2} \PYG{o}{=} \PYG{n}{gensim}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Word2Vec}\PYG{p}{(}\PYG{n}{tokens}\PYG{p}{,} \PYG{n}{sg}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{min\PYGZus{}count}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                              \PYG{n}{vector\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{window}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Vocabulary}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model2}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{key\PYGZus{}to\PYGZus{}index}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model2}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{get\PYGZus{}normed\PYGZus{}vectors}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine similarity between }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{gen}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ai}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ \PYGZhy{} Word2Vec(skip\PYGZhy{}gram) : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{model2}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}
\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02660277}  \PYG{l+m+mf}{0.0117296}   \PYG{l+m+mf}{0.25318226}  \PYG{l+m+mf}{0.44695902} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4615286}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.35307196}
   \PYG{l+m+mf}{0.3204311}   \PYG{l+m+mf}{0.4451589}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.24882038} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.18670462}\PYG{p}{]}
\PYG{p}{[} \PYG{l+m+mf}{0.41619968} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.08647515} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2558276}   \PYG{l+m+mf}{0.3695945}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.274073}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.10240843}
   \PYG{l+m+mf}{0.1622154}   \PYG{l+m+mf}{0.05593351} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.46721786} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5328355} \PYG{p}{]}
\PYG{p}{[} \PYG{l+m+mf}{0.43418837}  \PYG{l+m+mf}{0.30108306}  \PYG{l+m+mf}{0.40128633}  \PYG{l+m+mf}{0.0453006}   \PYG{l+m+mf}{0.37712952} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20221795}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05619935}  \PYG{l+m+mf}{0.34255028} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.44665098} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2337343} \PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.41098067} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05088534}  \PYG{l+m+mf}{0.5218584}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40045303} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.12768732} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.10601949}
   \PYG{l+m+mf}{0.44194022} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.32449666}  \PYG{l+m+mf}{0.00247097} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2600907} \PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.44081825}  \PYG{l+m+mf}{0.22984274} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40207896} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20159177} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.00161115} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0135952}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.3516631}   \PYG{l+m+mf}{0.44133204}  \PYG{l+m+mf}{0.2286844}   \PYG{l+m+mf}{0.423816}  \PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.42753762}  \PYG{l+m+mf}{0.23561442} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21681462}  \PYG{l+m+mf}{0.04321203}  \PYG{l+m+mf}{0.44539306} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.23385239}
   \PYG{l+m+mf}{0.23675178} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.35568893} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.18596812}  \PYG{l+m+mf}{0.49255413}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{Cosine} \PYG{n}{similarity} \PYG{n}{between} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{and} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZhy{}} \PYG{n}{Word2Vec}\PYG{p}{(}\PYG{n}{CBOW}\PYG{p}{)} \PYG{p}{:}  \PYG{l+m+mf}{0.32937223}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}
\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.02660277}  \PYG{l+m+mf}{0.0117296}   \PYG{l+m+mf}{0.25318226}  \PYG{l+m+mf}{0.44695902} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.4615286}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.35307196}
   \PYG{l+m+mf}{0.3204311}   \PYG{l+m+mf}{0.4451589}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.24882038} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.18670462}\PYG{p}{]}
\PYG{p}{[} \PYG{l+m+mf}{0.41619968} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.08647515} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2558276}   \PYG{l+m+mf}{0.3695945}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.274073}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.10240843}
   \PYG{l+m+mf}{0.1622154}   \PYG{l+m+mf}{0.05593351} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.46721786} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5328355} \PYG{p}{]}
\PYG{p}{[} \PYG{l+m+mf}{0.43418837}  \PYG{l+m+mf}{0.30108306}  \PYG{l+m+mf}{0.40128633}  \PYG{l+m+mf}{0.0453006}   \PYG{l+m+mf}{0.37712952} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20221795}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05619935}  \PYG{l+m+mf}{0.34255028} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.44665098} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2337343} \PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.41098067} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05088534}  \PYG{l+m+mf}{0.5218584}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40045303} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.12768732} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.10601949}
   \PYG{l+m+mf}{0.44194022} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.32449666}  \PYG{l+m+mf}{0.00247097} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2600907} \PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.44081825}  \PYG{l+m+mf}{0.22984274} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.40207896} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20159177} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.00161115} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0135952}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.3516631}   \PYG{l+m+mf}{0.44133204}  \PYG{l+m+mf}{0.2286844}   \PYG{l+m+mf}{0.423816}  \PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.42753762}  \PYG{l+m+mf}{0.23561442} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21681462}  \PYG{l+m+mf}{0.04321203}  \PYG{l+m+mf}{0.44539306} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.23385239}
   \PYG{l+m+mf}{0.23675178} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.35568893} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.18596812}  \PYG{l+m+mf}{0.49255413}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{Cosine} \PYG{n}{similarity} \PYG{n}{between} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{and} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZhy{}} \PYG{n}{Word2Vec}\PYG{p}{(}\PYG{n}{skip}\PYG{o}{\PYGZhy{}}\PYG{n}{gram}\PYG{p}{)} \PYG{p}{:}  \PYG{l+m+mf}{0.32937223}
\end{sphinxVerbatim}


\subsection{GloVE}
\label{\detokenize{embedding:glove}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{gensim}\PYG{n+nn}{.}\PYG{n+nn}{downloader} \PYG{k}{as} \PYG{n+nn}{api}
\PYG{c+c1}{\PYGZsh{} Download pre\PYGZhy{}trained GloVe model}
\PYG{n}{glove\PYGZus{}vectors} \PYG{o}{=} \PYG{n}{api}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{glove\PYGZhy{}wiki\PYGZhy{}gigaword\PYGZhy{}50}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Get word vectors (embeddings)}
\PYG{n}{word1} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{king}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{word2} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{queen}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{vector1} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word1}\PYG{p}{]}
\PYG{n}{vector2} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{p}{[}\PYG{n}{word2}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} Compute cosine similarity between the two word vectors}
\PYG{n}{similarity} \PYG{o}{=} \PYG{n}{glove\PYGZus{}vectors}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{n}{word1}\PYG{p}{,} \PYG{n}{word2}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{vector1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word vectors for }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{vector2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine similarity between }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{word2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{similarity}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{[}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{p}{]} \PYG{l+m+mf}{100.0}\PYG{o}{\PYGZpc{}} \PYG{l+m+mf}{66.0}\PYG{o}{/}\PYG{l+m+mf}{66.0}\PYG{n}{MB} \PYG{n}{downloaded}
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[} \PYG{l+m+mf}{0.50451}   \PYG{l+m+mf}{0.68607}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.59517}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.022801}  \PYG{l+m+mf}{0.60046}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.13498}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.08813}
\PYG{l+m+mf}{0.47377}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.61798}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.31012}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.076666}  \PYG{l+m+mf}{1.493}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.034189} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.98173}
\PYG{l+m+mf}{0.68229}   \PYG{l+m+mf}{0.81722}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51874}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.31503}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.55809}   \PYG{l+m+mf}{0.66421}   \PYG{l+m+mf}{0.1961}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.13495}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11476}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.30344}   \PYG{l+m+mf}{0.41177}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{2.223}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0756}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0783}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.34354}   \PYG{l+m+mf}{0.33505}   \PYG{l+m+mf}{1.9927}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.04234}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.64319}   \PYG{l+m+mf}{0.71125}   \PYG{l+m+mf}{0.49159}
\PYG{l+m+mf}{0.16754}   \PYG{l+m+mf}{0.34344}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25663}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.8523}    \PYG{l+m+mf}{0.1661}    \PYG{l+m+mf}{0.40102}   \PYG{l+m+mf}{1.1685}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0137}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21585}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.15155}   \PYG{l+m+mf}{0.78321}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.91241}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.6106}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.64426}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.51042} \PYG{p}{]}
\PYG{n}{Word} \PYG{n}{vectors} \PYG{k}{for} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[} \PYG{l+m+mf}{0.37854}    \PYG{l+m+mf}{1.8233}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2648}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1043}     \PYG{l+m+mf}{0.35829}    \PYG{l+m+mf}{0.60029}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.17538}    \PYG{l+m+mf}{0.83767}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.056798}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.75795}    \PYG{l+m+mf}{0.22681}    \PYG{l+m+mf}{0.98587}
\PYG{l+m+mf}{0.60587}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.31419}    \PYG{l+m+mf}{0.28877}    \PYG{l+m+mf}{0.56013}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.77456}    \PYG{l+m+mf}{0.071421}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5741}     \PYG{l+m+mf}{0.21342}    \PYG{l+m+mf}{0.57674}    \PYG{l+m+mf}{0.3868}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.12574}    \PYG{l+m+mf}{0.28012}
\PYG{l+m+mf}{0.28135}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.8053}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0421}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.19255}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.55375}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.054526}
\PYG{l+m+mf}{1.5574}     \PYG{l+m+mf}{0.39296}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2475}     \PYG{l+m+mf}{0.34251}    \PYG{l+m+mf}{0.45365}    \PYG{l+m+mf}{0.16237}
\PYG{l+m+mf}{0.52464}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.070272}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.83744}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0326}     \PYG{l+m+mf}{0.45946}    \PYG{l+m+mf}{0.25302}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.17837}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.73398}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.20025}    \PYG{l+m+mf}{0.2347}    \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.56095}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{2.2839}
\PYG{l+m+mf}{0.0092753} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.60284}  \PYG{p}{]}
\PYG{n}{Cosine} \PYG{n}{similarity} \PYG{n}{between} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{king}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{and} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.7839043140411377}
\end{sphinxVerbatim}


\subsection{Fast Text}
\label{\detokenize{embedding:fast-text}}
\sphinxAtStartPar
Fast Text incorporates subword information (useful for handling rare or unseen words)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{gensim}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{FastText}

\PYG{k+kn}{import} \PYG{n+nn}{gensim}
\PYG{k+kn}{from} \PYG{n+nn}{gensim}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{Word2Vec}

\PYG{c+c1}{\PYGZsh{} sample corpus}
\PYG{n}{corpus} \PYG{o}{=} \PYG{p}{[}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is hot}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}


\PYG{k}{def} \PYG{n+nf}{tokenize\PYGZus{}gensim}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}\PYG{p}{:}

   \PYG{n}{tokens} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
   \PYG{c+c1}{\PYGZsh{} iterate through each sentence in the corpus}
   \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{corpus}\PYG{p}{:}

      \PYG{c+c1}{\PYGZsh{} tokenize the sentence into words}
      \PYG{n}{temp} \PYG{o}{=} \PYG{n}{gensim}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{tokenize}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{lowercase}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{deacc}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYGZbs{}
                                    \PYG{n}{errors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{strict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{to\PYGZus{}lower}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYGZbs{}
                                    \PYG{n}{lower}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

      \PYG{n}{tokens}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{temp}\PYG{p}{)}\PYG{p}{)}

   \PYG{k}{return} \PYG{n}{tokens}

\PYG{n}{tokens} \PYG{o}{=} \PYG{n}{tokenize\PYGZus{}gensim}\PYG{p}{(}\PYG{n}{corpus}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} create FastText model}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{FastText}\PYG{p}{(}\PYG{n}{tokens}\PYG{p}{,} \PYG{n}{vector\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{window}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{min\PYGZus{}count}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{workers}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Train the model}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{tokens}\PYG{p}{,} \PYG{n}{total\PYGZus{}examples}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{tokens}\PYG{p}{)}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Vocabulary}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{key\PYGZus{}to\PYGZus{}index}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{get\PYGZus{}normed\PYGZus{}vectors}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine similarity between }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{gen}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{and }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ai}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ \PYGZhy{} Word2Vec : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{n}{model}\PYG{o}{.}\PYG{n}{wv}\PYG{o}{.}\PYG{n}{similarity}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{WARNING}\PYG{p}{:}\PYG{n}{gensim}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{word2vec}\PYG{p}{:}\PYG{n}{Effective} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alpha}\PYG{l+s+s1}{\PYGZsq{}} \PYG{n}{higher} \PYG{n}{than} \PYG{n}{previous} \PYG{n}{training} \PYG{n}{cycles}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}
\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01875759}  \PYG{l+m+mf}{0.086543}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25080433}  \PYG{l+m+mf}{0.2824868}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.23755953} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11316587}
   \PYG{l+m+mf}{0.473383}    \PYG{l+m+mf}{0.39204055} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.30422893} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5566626} \PYG{p}{]}
\PYG{p}{[} \PYG{l+m+mf}{0.5088161}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.3323528}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.128698}   \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.11877266} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.38699347}  \PYG{l+m+mf}{0.20977001}
   \PYG{l+m+mf}{0.05947014} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05622245} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.36257952} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5177341} \PYG{p}{]}
\PYG{p}{[} \PYG{l+m+mf}{0.18038039}  \PYG{l+m+mf}{0.51484865}  \PYG{l+m+mf}{0.40694886}  \PYG{l+m+mf}{0.05965518} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05985437} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.10832689}
   \PYG{l+m+mf}{0.37992737}  \PYG{l+m+mf}{0.5992712}   \PYG{l+m+mf}{0.01503773}  \PYG{l+m+mf}{0.1192203} \PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5694013}   \PYG{l+m+mf}{0.23560704}  \PYG{l+m+mf}{0.0265804}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.41392225} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.00285366} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.3076269}
   \PYG{l+m+mf}{0.2076883}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.425648}    \PYG{l+m+mf}{0.29903153}  \PYG{l+m+mf}{0.19965051}\PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.23892775}  \PYG{l+m+mf}{0.10744874} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.03730153} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.23521401}  \PYG{l+m+mf}{0.32083488}  \PYG{l+m+mf}{0.21598674}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.29570717} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.03044808}  \PYG{l+m+mf}{0.75250715}  \PYG{l+m+mf}{0.26538488}\PYG{p}{]}
\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.31881964} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.06544963} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.44274488}  \PYG{l+m+mf}{0.15485793}  \PYG{l+m+mf}{0.39120612} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05415314}
   \PYG{l+m+mf}{0.15772066} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.05987714} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6986104}   \PYG{l+m+mf}{0.03967094}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{Cosine} \PYG{n}{similarity} \PYG{n}{between} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gen}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{and} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ai}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZhy{}} \PYG{n}{Word2Vec} \PYG{p}{:}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.21662527}
\end{sphinxVerbatim}


\section{Contextual word embeddings}
\label{\detokenize{embedding:contextual-word-embeddings}}
\sphinxAtStartPar
Contextual word embeddings are word representations where the embedding of a word
changes depending on its context in a sentence or document. These embeddings capture
the meaning of a word as influenced by its surrounding words, addressing the limitations
of static embeddings by incorporating contextual nuances.


\subsection{BERT}
\label{\detokenize{embedding:bert}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{BertTokenizer}\PYG{p}{,} \PYG{n}{BertModel}
\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{BertTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bert\PYGZhy{}base\PYGZhy{}uncased}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{BertModel}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}uncased}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}


\PYG{n}{text} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Gen AI is awesome}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{encoded\PYGZus{}input} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{embeddings} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{encoded\PYGZus{}input}\PYG{p}{)}\PYG{o}{.}\PYG{n}{last\PYGZus{}hidden\PYGZus{}state}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{encoded\PYGZus{}input}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{x} \PYG{p}{:} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{+} \PYG{n}{text}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[EOS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{embeddings}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{embeddings}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,}  \PYG{l+m+mi}{8991}\PYG{p}{,}  \PYG{l+m+mi}{9932}\PYG{p}{,}  \PYG{l+m+mi}{2003}\PYG{p}{,} \PYG{l+m+mi}{12476}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{token\PYGZus{}type\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{attention\PYGZus{}mask}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{101}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{8991}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{9932}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{is}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2003}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{12476}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[SEP]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[EOS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{1031}\PYG{p}{,} \PYG{l+m+mi}{1041}\PYG{p}{,} \PYG{l+m+mi}{2891}\PYG{p}{,} \PYG{l+m+mi}{1033}\PYG{p}{]}\PYG{p}{\PYGZcb{}}


\PYG{n}{torch}\PYG{o}{.}\PYG{n}{Size}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{768}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1129}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1477}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0056}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1335}\PYG{p}{,}  \PYG{l+m+mf}{0.2605}\PYG{p}{,}  \PYG{l+m+mf}{0.2113}\PYG{p}{]}\PYG{p}{,}
         \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6841}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.1196}\PYG{p}{,}  \PYG{l+m+mf}{0.3349}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5958}\PYG{p}{,}  \PYG{l+m+mf}{0.1657}\PYG{p}{,}  \PYG{l+m+mf}{0.6988}\PYG{p}{]}\PYG{p}{,}
         \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5385}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2649}\PYG{p}{,}  \PYG{l+m+mf}{0.2639}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1544}\PYG{p}{,}  \PYG{l+m+mf}{0.2532}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1363}\PYG{p}{]}\PYG{p}{,}
         \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1794}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6086}\PYG{p}{,}  \PYG{l+m+mf}{0.1292}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1620}\PYG{p}{,}  \PYG{l+m+mf}{0.1721}\PYG{p}{,}  \PYG{l+m+mf}{0.4356}\PYG{p}{]}\PYG{p}{,}
         \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0187}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.7320}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.3420}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.4028}\PYG{p}{,}  \PYG{l+m+mf}{0.1425}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.2014}\PYG{p}{]}\PYG{p}{,}
         \PYG{p}{[} \PYG{l+m+mf}{0.5493}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1029}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1571}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.3503}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.7601}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1398}\PYG{p}{]}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{grad\PYGZus{}fn}\PYG{o}{=}\PYG{o}{\PYGZlt{}}\PYG{n}{NativeLayerNormBackward0}\PYG{o}{\PYGZgt{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{gte\sphinxhyphen{}large\sphinxhyphen{}en\sphinxhyphen{}v1.5}
\label{\detokenize{embedding:gte-large-en-v1-5}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{gte\sphinxhyphen{}large\sphinxhyphen{}en\sphinxhyphen{}v1.5}} is a state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art text embedding model developed
by Alibaba‚Äôs Institute for Intelligent Computing. It‚Äôs designed for natural
language processing tasks and excels in generating dense vector representations
(embeddings) of text for applications such as text retrieval, classification,
clustering, and reranking.

\sphinxAtStartPar
It can handle up to 8192 tokens, making it suitable for long\sphinxhyphen{}context tasks. More
details can be found at: \sphinxurl{https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5} .

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Requires transformers\PYGZgt{}=4.36.0}

\PYG{k+kn}{import} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{nn}\PYG{n+nn}{.}\PYG{n+nn}{functional} \PYG{k}{as} \PYG{n+nn}{F}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoModel}\PYG{p}{,} \PYG{n}{AutoTokenizer}

\PYG{n}{input\PYGZus{}texts} \PYG{o}{=} \PYG{p}{[}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is hot}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}

\PYG{n}{model\PYGZus{}path} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alibaba\PYGZhy{}NLP/gte\PYGZhy{}large\PYGZhy{}en\PYGZhy{}v1.5}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{model\PYGZus{}path}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{AutoModel}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{model\PYGZus{}path}\PYG{p}{,} \PYG{n}{trust\PYGZus{}remote\PYGZus{}code}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Tokenize the input texts}
\PYG{n}{batch\PYGZus{}dict} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{input\PYGZus{}texts}\PYG{p}{,} \PYG{n}{max\PYGZus{}length}\PYG{o}{=}\PYG{l+m+mi}{8192}\PYG{p}{,} \PYG{n}{padding}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYGZbs{}
                     \PYG{n}{truncation}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{batch\PYGZus{}dict}\PYG{p}{)}


\PYG{n}{outputs} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{batch\PYGZus{}dict}\PYG{p}{)}
\PYG{n}{embeddings} \PYG{o}{=} \PYG{n}{outputs}\PYG{o}{.}\PYG{n}{last\PYGZus{}hidden\PYGZus{}state}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} (Optionally) normalize embeddings}
\PYG{n}{embeddings} \PYG{o}{=} \PYG{n}{F}\PYG{o}{.}\PYG{n}{normalize}\PYG{p}{(}\PYG{n}{embeddings}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{scores} \PYG{o}{=} \PYG{p}{(}\PYG{n}{embeddings}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{@} \PYG{n}{embeddings}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{100}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{embeddings}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{scores}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,}  \PYG{l+m+mi}{8991}\PYG{p}{,}  \PYG{l+m+mi}{9932}\PYG{p}{,}  \PYG{l+m+mi}{2003}\PYG{p}{,} \PYG{l+m+mi}{12476}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,}  \PYG{l+m+mi}{8991}\PYG{p}{,}  \PYG{l+m+mi}{9932}\PYG{p}{,}  \PYG{l+m+mi}{2003}\PYG{p}{,}  \PYG{l+m+mi}{4569}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,}  \PYG{l+m+mi}{8991}\PYG{p}{,}  \PYG{l+m+mi}{9932}\PYG{p}{,}  \PYG{l+m+mi}{2003}\PYG{p}{,}  \PYG{l+m+mi}{2980}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{token\PYGZus{}type\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{attention\PYGZus{}mask}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[} \PYG{l+m+mf}{0.0079}\PYG{p}{,}  \PYG{l+m+mf}{0.0008}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0001}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.0418}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0138}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0236}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[} \PYG{l+m+mf}{0.0079}\PYG{p}{,}  \PYG{l+m+mf}{0.0218}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0171}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.0412}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0230}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0237}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[} \PYG{l+m+mf}{0.0073}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0106}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0194}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.0711}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0204}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0036}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
      \PYG{n}{grad\PYGZus{}fn}\PYG{o}{=}\PYG{o}{\PYGZlt{}}\PYG{n}{DivBackward0}\PYG{o}{\PYGZgt{}}\PYG{p}{)}
\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{92.85284423828125}\PYG{p}{,} \PYG{l+m+mf}{92.81655883789062}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}


\subsection{bge\sphinxhyphen{}base\sphinxhyphen{}en\sphinxhyphen{}v1.5}
\label{\detokenize{embedding:bge-base-en-v1-5}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{bge\sphinxhyphen{}base\sphinxhyphen{}en\sphinxhyphen{}v1.5}} model is a general\sphinxhyphen{}purpose text embedding model developed
by the Beijing Academy of Artificial Intelligence (BAAI). It transforms input text
into 768\sphinxhyphen{}dimensional vector embeddings, making it useful for tasks like semantic
search, text similarity, and clustering. This model is fine\sphinxhyphen{}tuned using contrastive
learning, which helps improve its ability to distinguish between similar and
dissimilar sentences effectively. More details can be found
at: \sphinxurl{https://huggingface.co/BAAI/bge-base-en-v1.5} .

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoTokenizer}\PYG{p}{,} \PYG{n}{AutoModel}
\PYG{k+kn}{import} \PYG{n+nn}{torch}

\PYG{c+c1}{\PYGZsh{} Sentences we want sentence embeddings for}
\PYG{n}{sentences} \PYG{o}{=} \PYG{p}{[}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is awesome}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is fun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gen AI is hot}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} Load model from HuggingFace Hub}
\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BAAI/bge\PYGZhy{}large\PYGZhy{}zh\PYGZhy{}v1.5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{AutoModel}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BAAI/bge\PYGZhy{}large\PYGZhy{}zh\PYGZhy{}v1.5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Tokenize sentences}
\PYG{n}{encoded\PYGZus{}input} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{sentences}\PYG{p}{,} \PYG{n}{padding}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{truncation}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{encoded\PYGZus{}input}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compute token embeddings}
\PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
   \PYG{n}{model\PYGZus{}output} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{encoded\PYGZus{}input}\PYG{p}{)}
   \PYG{c+c1}{\PYGZsh{} Perform pooling. In this case, cls pooling.}
   \PYG{n}{sentence\PYGZus{}embeddings} \PYG{o}{=} \PYG{n}{model\PYGZus{}output}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} normalize embeddings}
\PYG{n}{sentence\PYGZus{}embeddings} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{functional}\PYG{o}{.}\PYG{n}{normalize}\PYG{p}{(}\PYG{n}{sentence\PYGZus{}embeddings}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sentence embeddings:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sentence\PYGZus{}embeddings}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,} \PYG{l+m+mi}{10234}\PYG{p}{,}  \PYG{l+m+mi}{8171}\PYG{p}{,}  \PYG{l+m+mi}{8578}\PYG{p}{,}  \PYG{l+m+mi}{8310}\PYG{p}{,}   \PYG{l+m+mi}{143}\PYG{p}{,} \PYG{l+m+mi}{11722}\PYG{p}{,}  \PYG{l+m+mi}{9974}\PYG{p}{,}  \PYG{l+m+mi}{8505}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,} \PYG{l+m+mi}{10234}\PYG{p}{,}  \PYG{l+m+mi}{8171}\PYG{p}{,}  \PYG{l+m+mi}{8578}\PYG{p}{,}  \PYG{l+m+mi}{8310}\PYG{p}{,}  \PYG{l+m+mi}{9575}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{,}     \PYG{l+m+mi}{0}\PYG{p}{,}     \PYG{l+m+mi}{0}\PYG{p}{,}     \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}  \PYG{l+m+mi}{101}\PYG{p}{,} \PYG{l+m+mi}{10234}\PYG{p}{,}  \PYG{l+m+mi}{8171}\PYG{p}{,}  \PYG{l+m+mi}{8578}\PYG{p}{,}  \PYG{l+m+mi}{8310}\PYG{p}{,}  \PYG{l+m+mi}{9286}\PYG{p}{,}   \PYG{l+m+mi}{102}\PYG{p}{,}     \PYG{l+m+mi}{0}\PYG{p}{,}     \PYG{l+m+mi}{0}\PYG{p}{,}     \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{token\PYGZus{}type\PYGZus{}ids}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{attention\PYGZus{}mask}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
     \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n}{Sentence} \PYG{n}{embeddings}\PYG{p}{:} \PYG{n}{tensor}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[} \PYG{l+m+mf}{0.0700}\PYG{p}{,}  \PYG{l+m+mf}{0.0119}\PYG{p}{,}  \PYG{l+m+mf}{0.0049}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.0428}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0475}\PYG{p}{,}  \PYG{l+m+mf}{0.0242}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[} \PYG{l+m+mf}{0.0800}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0065}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0519}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.0057}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0770}\PYG{p}{,}  \PYG{l+m+mf}{0.0119}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[} \PYG{l+m+mf}{0.0740}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0185}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0369}\PYG{p}{,}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}  \PYG{l+m+mf}{0.0083}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.0026}\PYG{p}{,}  \PYG{l+m+mf}{0.0016}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{Prompt Engineering}
\label{\detokenize{prompt:prompt-engineering}}\label{\detokenize{prompt:prompt}}\label{\detokenize{prompt::doc}}

\section{Prompt}
\label{\detokenize{prompt:id1}}
\sphinxAtStartPar
A prompt is the input or query given to an LLM to elicit a specific response.
It acts as the user‚Äôs way of ‚Äúprogramming‚Äù the model without code, simply by
phrasing questions or tasks appropriately.


\section{Prompt Engineering}
\label{\detokenize{prompt:id2}}

\subsection{What‚Äôs Prompt Engineering}
\label{\detokenize{prompt:what-s-prompt-engineering}}
\sphinxAtStartPar
Prompt engineering is the practice of designing and refining input prompts to
guide LLMs to produce desired outputs effectively and consistently.
It involves crafting queries, commands, or instructions that align with
the model‚Äôs capabilities and the task‚Äôs requirements.


\subsection{Key Elements of a Prompt}
\label{\detokenize{prompt:key-elements-of-a-prompt}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Clarity: A clear and unambiguous prompt ensures the model understands the task.

\item {} 
\sphinxAtStartPar
Specificity: Including details like tone, format, length, or audience helps tailor the response.

\item {} 
\sphinxAtStartPar
Context:Providing background information ensures the model generates relevant outputs.

\end{itemize}


\section{Advanced Prompt Engineering}
\label{\detokenize{prompt:advanced-prompt-engineering}}

\subsection{Role Assignment}
\label{\detokenize{prompt:role-assignment}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Assign a specific role or persona to the AI to shape its style and expertise.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúYou are a professional data scientist. Explain how to build a machine learning model to a beginner.‚Äù}
\end{quote}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} You are a professional data scientist. Explain how to build a machine}
\PYG{c+c1}{\PYGZsh{} learning model to a beginner.}
\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Role: you are a }\PYG{l+s+si}{\PYGZob{}role\PYGZcb{}}
\PYG{l+s+s2}{task: }\PYG{l+s+si}{\PYGZob{}task\PYGZcb{}}
\PYG{l+s+s2}{Answer:}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{MODEL}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model} \PYG{o}{|} \PYG{n}{output\PYGZus{}parser}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{role}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{data scientist}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{task}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Explain how to build a machine }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                            learning model to a beginner}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
 \PYG{p}{\PYGZob{}}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{role}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{assistant}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{content}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{To build a machine learning model, let}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s follow these steps as a beginner: }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{1. **Define the Problem**: Understand what problem you are trying to solve. This could be anything from predicting house prices, recognizing images, or even recommending products. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{2. **Collect and Prepare Data**: Gather relevant data for your problem. This might involve web scraping, APIs, or using existing datasets. Once you have the data, clean it by handling missing values, outliers, and errors. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{3. **Explore and Visualize Data**: Understand the structure of your data, its distribution, and relationships between variables. This can help in identifying patterns and making informed decisions about the next steps. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{4. **Feature Engineering**: Create new features that might be useful for the model to make accurate predictions. This could involve creating interactions between existing features or using techniques like one\PYGZhy{}hot encoding. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{5. **Split Data**: Split your data into training, validation, and testing sets. The training set is used to train the model, the validation set is used to tune hyperparameters, and the testing set is used to evaluate the final performance of the model. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{6. **Choose a Model**: Select a machine learning algorithm that suits your problem. Some common algorithms include linear regression for regression problems, logistic regression for binary classification problems, decision trees, random forests, support vector machines (SVM), and neural networks for more complex tasks. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{7. **Train the Model**: Use your training data to train the chosen model. This involves feeding the data into the model and adjusting its parameters based on the error it makes. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{8. **Tune Hyperparameters**: Adjust the hyperparameters of the model to improve its performance. This could involve changing learning rates, number of layers in a neural network, or the complexity of a decision tree. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{9. **Evaluate the Model**: Use your testing data to evaluate the performance of the model. Common metrics include accuracy for classification problems, mean squared error for regression problems, and precision, recall, and F1 score for imbalanced datasets. }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{10. **Deploy the Model**: Once you are satisfied with the performance of your model, deploy it to a production environment where it can make predictions on new data.}\PYG{l+s+s2}{\PYGZdq{}}
 \PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Contextual Setup}
\label{\detokenize{prompt:contextual-setup}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Provide sufficient background or context for the AI to understand the task.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúI am planing to write a book about GenAI best practice, help me draft the contents for the book.‚Äù}
\end{quote}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Contextual Setup}

\PYG{c+c1}{\PYGZsh{} I am planing to write a book about GenAI best practice, help me draft the}
\PYG{c+c1}{\PYGZsh{} contents for the book.}
\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{Role: you are a }\PYG{l+s+si}{\PYGZob{}role\PYGZcb{}}
\PYG{l+s+s2}{task: }\PYG{l+s+si}{\PYGZob{}task\PYGZcb{}}
\PYG{l+s+s2}{Answer:}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{MODEL}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model} \PYG{o}{|} \PYG{n}{output\PYGZus{}parser}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{role}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{book writer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{task}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{I am planing to write a book about }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                        GenAI best practice, help me draft the }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                        contents for the book.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{1. Introduction}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Introduction to General Artificial Intelligence (GenAI) and its significance in today}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s world.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2. Chapter 1 \PYGZhy{} Understanding AI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exploring the basics of Artificial Intelligence, its history, and evolution.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{3. Chapter 2 \PYGZhy{} Types of AI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Detailed discussion on various types of AI such as Narrow AI, General AI, and Superintelligent AI.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{4. Chapter 3 \PYGZhy{} GenAI Architecture}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exploring the architecture of General AI systems, including neural networks, deep learning, and reinforcement learning.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{5. Chapter 4 \PYGZhy{} Ethics in AI Development}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Discussing the ethical considerations involved in developing GenAI, such as privacy, bias, and accountability.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{6. Chapter 5 \PYGZhy{} Data Collection and Management}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Understanding the importance of data in AI development, best practices for data collection, and responsible data management.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7. Chapter 6 \PYGZhy{} Model Training and Optimization}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exploring techniques for training AI models effectively, including hyperparameter tuning, regularization, and optimization strategies.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{8. Chapter 7 \PYGZhy{} Testing and Validation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Discussing the importance of testing and validation in ensuring the reliability and accuracy of GenAI systems.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{9. Chapter 8 \PYGZhy{} Deployment and Maintenance}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exploring best practices for deploying AI models into production environments, as well as ongoing maintenance and updates.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{10. Case Studies}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Real\PYGZhy{}world examples of successful GenAI implementations across various industries, highlighting key takeaways and lessons learned.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{11. Future Trends in GenAI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exploring emerging trends in the field of General AI, such as quantum computing, explainable AI, and human\PYGZhy{}AI collaboration.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{12. Conclusion}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Summarizing the key points discussed in the book and looking forward to the future of General AI.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Explicit Instructions}
\label{\detokenize{prompt:explicit-instructions}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Clearly specify the format, tone, style, or structure you want in the response.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúExplain the concept of word embeddings in 100 words, using simple language suitable for a high school student.‚Äù}
\end{quote}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Explicit Instructions}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}ollama}\PYG{n+nn}{.}\PYG{n+nn}{llms} \PYG{k+kn}{import} \PYG{n}{OllamaLLM}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}core}\PYG{n+nn}{.}\PYG{n+nn}{prompts} \PYG{k+kn}{import} \PYG{n}{ChatPromptTemplate}
\PYG{k+kn}{from} \PYG{n+nn}{langchain}\PYG{n+nn}{.}\PYG{n+nn}{output\PYGZus{}parsers} \PYG{k+kn}{import} \PYG{n}{CommaSeparatedListOutputParser}


\PYG{c+c1}{\PYGZsh{} Explain the concept of word embeddings in 100 words, using simple}
\PYG{c+c1}{\PYGZsh{} language suitable for a high school student}

\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{you are a }\PYG{l+s+si}{\PYGZob{}role\PYGZcb{}}
\PYG{l+s+s2}{task: }\PYG{l+s+si}{\PYGZob{}task\PYGZcb{}}
\PYG{l+s+s2}{instruction: }\PYG{l+s+si}{\PYGZob{}instruction\PYGZcb{}}
\PYG{l+s+s2}{Answer: Let}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s think step by step.}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{MODEL}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{role}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AI engineer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Explain the concept of word embeddings in }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                100 words}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{instruction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{using simple }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                language suitable for a high school student}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{assistant}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{message}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Word Embeddings are like giving words a special address in a big library. Each word gets its own unique location, and words that are used in similar ways get placed close together. This helps the computer understand the meaning of words better when it}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s reading text. For example, }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{king}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ might be near }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{queen}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{, because they are both types of royalty. And }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{apple}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ might be near }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{fruit}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{, because they are related concepts.}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Chain of Thought (CoT) Prompting}
\label{\detokenize{prompt:chain-of-thought-cot-prompting}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Encourage step\sphinxhyphen{}by\sphinxhyphen{}step reasoning for complex problems.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}
\begin{quote}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúSolve this math problem step by step: A train travels 60 miles in 1.5 hours. What is its average speed?‚Äù}
\end{quote}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} CoT}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}ollama}\PYG{n+nn}{.}\PYG{n+nn}{llms} \PYG{k+kn}{import} \PYG{n}{OllamaLLM}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}core}\PYG{n+nn}{.}\PYG{n+nn}{prompts} \PYG{k+kn}{import} \PYG{n}{ChatPromptTemplate}
\PYG{k+kn}{from} \PYG{n+nn}{langchain}\PYG{n+nn}{.}\PYG{n+nn}{output\PYGZus{}parsers} \PYG{k+kn}{import} \PYG{n}{CommaSeparatedListOutputParser}


\PYG{c+c1}{\PYGZsh{} Solve this math problem step by step: A train travels 60 miles in 1.5 hours.}
\PYG{c+c1}{\PYGZsh{} What is its average speed?}

\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{you are a }\PYG{l+s+si}{\PYGZob{}role\PYGZcb{}}
\PYG{l+s+s2}{task: }\PYG{l+s+si}{\PYGZob{}task\PYGZcb{}}
\PYG{l+s+s2}{question: }\PYG{l+s+si}{\PYGZob{}question\PYGZcb{}}
\PYG{l+s+s2}{Answer: Let}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s think step by step.}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{MODEL}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{role}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{math student}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Solve this math problem step by step: }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                A train travels 60 miles in 1.5 hours.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{question}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{What is its average speed per minute?}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
 \PYG{p}{\PYGZob{}}
\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Solution}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Step 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{First, let}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s find the average speed of the train per hour.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Step 2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The train travels 60 miles in 1.5 hours. So, its speed per hour is 60 miles / 1.5 hours = 40 miles/hour.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Step 3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Now, let}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s find the average speed of the train per minute. Since there are 60 minutes in an hour, the speed per minute would be the speed per hour multiplied by the number of minutes in an hour divided by 60.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Step 4}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{So, the average speed of the train per minute is (40 miles/hour * (1 hour / 60)) = (40/60) miles/minute = 2/3 miles/minute.}\PYG{l+s+s2}{\PYGZdq{}}
             \PYG{p}{\PYGZcb{}}
 \PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Few\sphinxhyphen{}Shot Prompting}
\label{\detokenize{prompt:few-shot-prompting}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Provide examples to guide the AI on how to respond.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}
\begin{quote}
\begin{description}
\sphinxlineitem{{\color{red}\bfseries{}*}‚ÄùHere are examples of loan application decision:}
\sphinxAtStartPar
‚Äòexample‚Äô: \{‚Äòinput‚Äô: \{‚Äòfico‚Äô:800, ‚Äòincome‚Äô:100000,‚Äôloan\_amount‚Äô: 10000\}
‚Äòdecision‚Äô: ‚Äúaccept‚Äù
Now Help me to make a decision to accpet or reject the loan application and
give the reason.
‚Äòinput‚Äô: ‚Äú\{‚Äòfico‚Äô:820, ‚Äòincome‚Äô:100000, ‚Äòloan\_amount‚Äô: 1,000\}‚Äù‚Äù*

\end{description}
\end{quote}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Few\PYGZhy{}Shot Prompting}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}ollama}\PYG{n+nn}{.}\PYG{n+nn}{llms} \PYG{k+kn}{import} \PYG{n}{OllamaLLM}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}core}\PYG{n+nn}{.}\PYG{n+nn}{prompts} \PYG{k+kn}{import} \PYG{n}{ChatPromptTemplate}
\PYG{k+kn}{from} \PYG{n+nn}{langchain}\PYG{n+nn}{.}\PYG{n+nn}{output\PYGZus{}parsers} \PYG{k+kn}{import} \PYG{n}{CommaSeparatedListOutputParser}


\PYG{c+c1}{\PYGZsh{} Here are examples of loan application decision:}
\PYG{c+c1}{\PYGZsh{} \PYGZsq{}example\PYGZsq{}: \PYGZob{}\PYGZsq{}input\PYGZsq{}: \PYGZob{}\PYGZsq{}fico\PYGZsq{}:800, \PYGZsq{}income\PYGZsq{}:100000,\PYGZsq{}loan\PYGZus{}amount\PYGZsq{}: 10000\PYGZcb{}}
\PYG{c+c1}{\PYGZsh{} \PYGZsq{}decision\PYGZsq{}: \PYGZdq{}accept\PYGZdq{}}
\PYG{c+c1}{\PYGZsh{} Now Help me to make a decision to accpet or reject the loan application and}
\PYG{c+c1}{\PYGZsh{} give the reason.}
\PYG{c+c1}{\PYGZsh{} \PYGZsq{}input\PYGZsq{}: \PYGZdq{}\PYGZob{}\PYGZsq{}fico\PYGZsq{}:820, \PYGZsq{}income\PYGZsq{}:100000, \PYGZsq{}loan\PYGZus{}amount\PYGZsq{}: 1,000\PYGZcb{}\PYGZdq{}}

\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{you are a }\PYG{l+s+si}{\PYGZob{}role\PYGZcb{}}
\PYG{l+s+s2}{task: }\PYG{l+s+si}{\PYGZob{}task\PYGZcb{}}
\PYG{l+s+s2}{examples: }\PYG{l+s+si}{\PYGZob{}example\PYGZcb{}}
\PYG{l+s+s2}{input: }\PYG{l+s+si}{\PYGZob{}input\PYGZcb{}}
\PYG{l+s+s2}{decision:}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{MODEL}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{role}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{banker}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Help me to make a decision to accpet or }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                reject the loan application }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{example}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fico}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{800}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{income}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{100000}\PYG{p}{,}\PYGZbs{}
                                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loan\PYGZus{}amount}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10000}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYGZbs{}
                                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{decision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{accept}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fico}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{820}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{income}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{100000}\PYG{p}{,} \PYGZbs{}
                                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loan\PYGZus{}amount}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1000}\PYG{p}{\PYGZcb{}}
                        \PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{decision}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{accept}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Iterative Prompting}
\label{\detokenize{prompt:iterative-prompting}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Build on the AI‚Äôs response by asking follow\sphinxhyphen{}up questions or refining the output.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Initial Prompt:} ‚Äú Help me to make a decision to accpet or reject the loan application.‚Äù

\item {} 
\sphinxAtStartPar
\sphinxstyleemphasis{Follow\sphinxhyphen{}Up:} ‚Äúgive me the reason‚Äù

\end{itemize}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Few\PYGZhy{}Shot Prompting}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}ollama}\PYG{n+nn}{.}\PYG{n+nn}{llms} \PYG{k+kn}{import} \PYG{n}{OllamaLLM}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}core}\PYG{n+nn}{.}\PYG{n+nn}{prompts} \PYG{k+kn}{import} \PYG{n}{ChatPromptTemplate}
\PYG{k+kn}{from} \PYG{n+nn}{langchain}\PYG{n+nn}{.}\PYG{n+nn}{output\PYGZus{}parsers} \PYG{k+kn}{import} \PYG{n}{CommaSeparatedListOutputParser}


\PYG{c+c1}{\PYGZsh{} Here are examples of loan application decision:}
\PYG{c+c1}{\PYGZsh{} \PYGZsq{}example\PYGZsq{}: \PYGZob{}\PYGZsq{}input\PYGZsq{}: \PYGZob{}\PYGZsq{}fico\PYGZsq{}:800, \PYGZsq{}income\PYGZsq{}:100000,\PYGZsq{}loan\PYGZus{}amount\PYGZsq{}: 10000\PYGZcb{}}
\PYG{c+c1}{\PYGZsh{} \PYGZsq{}decision\PYGZsq{}: \PYGZdq{}accept\PYGZdq{}}
\PYG{c+c1}{\PYGZsh{} Now Help me to make a decision to accpet or reject the loan application and}
\PYG{c+c1}{\PYGZsh{} give the reason.}
\PYG{c+c1}{\PYGZsh{} \PYGZsq{}input\PYGZsq{}: \PYGZdq{}\PYGZob{}\PYGZsq{}fico\PYGZsq{}:820, \PYGZsq{}income\PYGZsq{}:100000, \PYGZsq{}loan\PYGZus{}amount\PYGZsq{}: 1,000\PYGZcb{}\PYGZdq{}}

\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{you are a }\PYG{l+s+si}{\PYGZob{}role\PYGZcb{}}
\PYG{l+s+s2}{task: }\PYG{l+s+si}{\PYGZob{}task\PYGZcb{}}
\PYG{l+s+s2}{examples: }\PYG{l+s+si}{\PYGZob{}example\PYGZcb{}}
\PYG{l+s+s2}{input: }\PYG{l+s+si}{\PYGZob{}input\PYGZcb{}}
\PYG{l+s+s2}{decision:}
\PYG{l+s+s2}{reason:}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{MODEL}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{role}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{banker}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Help me to make a decision to accpet or }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                reject the loan application and }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                give the reason.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{example}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fico}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{800}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{income}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{100000}\PYG{p}{,}\PYGZbs{}
                                            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loan\PYGZus{}amount}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10000}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYGZbs{}
                                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{decision}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{accept}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fico}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{820}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{income}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{100000}\PYG{p}{,} \PYGZbs{}
                                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loan\PYGZus{}amount}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1000}\PYG{p}{\PYGZcb{}}
                        \PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{decision}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{accept}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reason}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The applicant has a high credit score (FICO 820), a stable income of \PYGZdl{}100,000, and is requesting a relatively small loan amount (\PYGZdl{}1000). These factors indicate a low risk for the bank.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Instructional Chaining}
\label{\detokenize{prompt:instructional-chaining}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Break down a task into a sequence of smaller prompts.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
step 1: check the fico score

\item {} 
\sphinxAtStartPar
step 2: check the income,

\item {} 
\sphinxAtStartPar
step 3: check the loan amount,

\item {} 
\sphinxAtStartPar
step 4: make a decision,

\item {} 
\sphinxAtStartPar
step 5: give the reason.

\end{itemize}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Instructional Chaining}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}ollama}\PYG{n+nn}{.}\PYG{n+nn}{llms} \PYG{k+kn}{import} \PYG{n}{OllamaLLM}
\PYG{k+kn}{from} \PYG{n+nn}{langchain\PYGZus{}core}\PYG{n+nn}{.}\PYG{n+nn}{prompts} \PYG{k+kn}{import} \PYG{n}{ChatPromptTemplate}
\PYG{k+kn}{from} \PYG{n+nn}{langchain}\PYG{n+nn}{.}\PYG{n+nn}{output\PYGZus{}parsers} \PYG{k+kn}{import} \PYG{n}{CommaSeparatedListOutputParser}


\PYG{c+c1}{\PYGZsh{} Now Help me to make a decision to accpet or reject the loan application and}
\PYG{c+c1}{\PYGZsh{} give the reason.}
\PYG{c+c1}{\PYGZsh{} \PYGZsq{}\PYGZsq{}input\PYGZsq{}: \PYGZob{}\PYGZsq{}fico\PYGZsq{}:320, \PYGZsq{}income\PYGZsq{}:10000, \PYGZsq{}loan\PYGZus{}amount\PYGZsq{}: 100000\PYGZcb{}}

\PYG{n}{template} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{you are a }\PYG{l+s+si}{\PYGZob{}role\PYGZcb{}}
\PYG{l+s+s2}{task: }\PYG{l+s+si}{\PYGZob{}task\PYGZcb{}}
\PYG{l+s+s2}{instruction: }\PYG{l+s+si}{\PYGZob{}instruction\PYGZcb{}}
\PYG{l+s+s2}{input: }\PYG{l+s+si}{\PYGZob{}input\PYGZcb{}}
\PYG{l+s+s2}{decision:}
\PYG{l+s+s2}{reason:}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{n}{prompt} \PYG{o}{=} \PYG{n}{ChatPromptTemplate}\PYG{o}{.}\PYG{n}{from\PYGZus{}template}\PYG{p}{(}\PYG{n}{template}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OllamaLLM}\PYG{p}{(}\PYG{n}{temperature}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{MODEL}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{output\PYGZus{}parser} \PYG{o}{=} \PYG{n}{CommaSeparatedListOutputParser}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{chain} \PYG{o}{=} \PYG{n}{prompt} \PYG{o}{|} \PYG{n}{model}

\PYG{n}{response} \PYG{o}{=} \PYG{n}{chain}\PYG{o}{.}\PYG{n}{invoke}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{role}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{banker}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Help me to make a decision to accpet or }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                reject the loan application and }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                give the reason.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{instruction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check the fico score}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check the income}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step 3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{check the loan amount}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step 4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{make a decision}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYGZbs{}
                                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step 5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{give the reason}\PYG{l+s+s2}{\PYGZdq{}}
                                        \PYG{p}{\PYGZcb{}}\PYG{p}{,}
                        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fico}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{320}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{income}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{10000}\PYG{p}{,} \PYGZbs{}
                                    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{loan\PYGZus{}amount}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{100000}\PYG{p}{\PYGZcb{}}
                        \PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{response}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{decision}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reject}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
   \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reason}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Based on the provided information, the applicant}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s FICO score is 320 which falls below our minimum acceptable credit score. Additionally, the proposed loan amount of \PYGZdl{}100,000 exceeds the income level of \PYGZdl{}10,000 per year, making it difficult for the borrower to repay the loan.}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Use Constraints}
\label{\detokenize{prompt:use-constraints}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Impose constraints to keep responses concise and on\sphinxhyphen{}topic.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúList 5 key trends in AI in bullet points, each under 15 words.‚Äù}

\end{itemize}


\subsection{Creative Prompting}
\label{\detokenize{prompt:creative-prompting}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Encourage unique or unconventional ideas by framing the task creatively.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúPretend you are a time traveler from the year 2124. How would you describe AI advancements to someone today?‚Äù}

\end{itemize}


\subsection{Feedback Incorporation}
\label{\detokenize{prompt:feedback-incorporation}}\begin{itemize}
\item {} 
\sphinxAtStartPar
If the response isn‚Äôt perfect, guide the AI to refine or retry.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúThis is too general. Could you provide more specific examples for the education industry?‚Äù}

\end{itemize}


\subsection{Scenario\sphinxhyphen{}Based Prompts}
\label{\detokenize{prompt:scenario-based-prompts}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Frame the query within a scenario for a contextual response.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúImagine you‚Äôre a teacher explaining ChatGPT to students. How would you introduce its uses and limitations?‚Äù}

\end{itemize}


\subsection{Multimodal Prompting}
\label{\detokenize{prompt:multimodal-prompting}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Use prompts designed for mixed text/image inputs (or outputs if using models like DALL¬∑E).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Example:}

\sphinxAtStartPar
\sphinxstyleemphasis{‚ÄúGenerate an image prompt for a futuristic cityscape, vibrant, with flying cars and greenery.‚Äù}

\end{itemize}

\sphinxstepscope


\chapter{Retrieval\sphinxhyphen{}Augmented Generation}
\label{\detokenize{rag:retrieval-augmented-generation}}\label{\detokenize{rag:rag}}\label{\detokenize{rag::doc}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{RAG_diagram}.png}
\caption{Retrieval\sphinxhyphen{}Augmented Generation Diagram}\label{\detokenize{rag:id2}}\label{\detokenize{rag:fig-rag}}\end{figure}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The naive chunking strategy was used in the diagram above. More advanced strategies,
such as Late Chunking \sphinxcite{reference:latechunking} (or Chunked Pooling), are discussed later in this chapter.
\end{sphinxadmonition}


\section{Overview}
\label{\detokenize{rag:overview}}
\sphinxAtStartPar
Retrieval\sphinxhyphen{}Augmented Generation (RAG) is a framework that enhances large language models (LLMs)
by combining their generative capabilities with external knowledge retrieval. The goal of RAG
is to improve accuracy, relevance, and factuality by providing the LLM with specific, up\sphinxhyphen{}to\sphinxhyphen{}date,
or domain\sphinxhyphen{}specific context from a knowledge base or database during the generation process.

\sphinxAtStartPar
As you can see in {\hyperref[\detokenize{rag:fig-rag}]{\sphinxcrossref{\DUrole{std}{\DUrole{std-ref}{Retrieval\sphinxhyphen{}Augmented Generation Diagram}}}}}, the RAG has there main  components
\begin{itemize}
\item {} 
\sphinxAtStartPar
Indexer: The indexer processes raw text or other forms of unstructured data and creates an
efficient structure (called an index) that allows for fast and accurate retrieval
by the retriever when a query is made.

\item {} 
\sphinxAtStartPar
Retriever: Responsible for finding relevant information from an external knowledge source,
such as a document database, a vector database, or the web.

\item {} 
\sphinxAtStartPar
Generator: An LLM (like GPT\sphinxhyphen{}4, T5, or similar) that uses the retrieved context to generate a response.
The model is ‚Äúaugmented‚Äù with the retrieved information, which reduces hallucination and enhances factual accuracy.

\end{itemize}


\section{Indexing}
\label{\detokenize{rag:indexing}}
\sphinxAtStartPar
The indexing processes raw text or other forms of unstructured data and creates an efficient structure
(called an index) that allows for fast and accurate retrieval by the retriever when a query is made.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{indexer}.png}
\end{figure}


\subsection{Naive Chunking}
\label{\detokenize{rag:naive-chunking}}
\sphinxAtStartPar
Chunking in Retrieval\sphinxhyphen{}Augmented Generation (RAG) involves splitting documents or knowledge bases
into smaller, manageable pieces (chunks) that can be efficiently retrieved and used by a language model (LLM).

\sphinxAtStartPar
Below are the common chunking strategies used in RAG workflows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fixed\sphinxhyphen{}Length Chunking}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Chunks are created with a predefined, fixed length (e.g., 200 words or 512 tokens).

\item {} 
\sphinxAtStartPar
Simple and easy to implement but might split content mid\sphinxhyphen{}sentence or lose semantic coherence.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{fixed\PYGZus{}length\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{chunk\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{words} \PYG{o}{=} \PYG{n}{text}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{p}{[}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{words}\PYG{p}{[}\PYG{n}{i}\PYG{p}{:}\PYG{n}{i} \PYG{o}{+} \PYG{n}{chunk\PYGZus{}size}\PYG{p}{]}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{words}\PYG{p}{)}\PYG{p}{,} \PYG{n}{chunk\PYGZus{}size}\PYG{p}{)}
    \PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Example Usage}
\PYG{n}{document} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{This is a sample document with multiple sentences to demonstrate fixed\PYGZhy{}length chunking.}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{chunks} \PYG{o}{=} \PYG{n}{fixed\PYGZus{}length\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{document}\PYG{p}{,} \PYG{n}{chunk\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{chunk} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{chunks}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Chunk }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{idx}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{chunk}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Output}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Chunk 1: This is a sample document with multiple sentences to demonstrate

\item {} 
\sphinxAtStartPar
Chunk 2: fixed\sphinxhyphen{}length chunking.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sliding Window Chunking}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Creates overlapping chunks to preserve context across splits.

\item {} 
\sphinxAtStartPar
Ensures important information in overlapping regions is retained.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{sliding\PYGZus{}window\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{chunk\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{overlap\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{words} \PYG{o}{=} \PYG{n}{text}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{chunks} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{words}\PYG{p}{)}\PYG{p}{,} \PYG{n}{chunk\PYGZus{}size} \PYG{o}{\PYGZhy{}} \PYG{n}{overlap\PYGZus{}size}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{chunk} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{words}\PYG{p}{[}\PYG{n}{i}\PYG{p}{:}\PYG{n}{i} \PYG{o}{+} \PYG{n}{chunk\PYGZus{}size}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{chunks}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{chunk}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{chunks}

\PYG{c+c1}{\PYGZsh{} Example Usage}
\PYG{n}{document} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{This is a sample document with multiple sentences to demonstrate sliding window chunking.}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{chunks} \PYG{o}{=} \PYG{n}{sliding\PYGZus{}window\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{document}\PYG{p}{,} \PYG{n}{chunk\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{overlap\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{chunk} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{chunks}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Chunk }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{idx}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{chunk}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{\sphinxstylestrong{Output}:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Chunk 1: This is a sample document with multiple sentences to demonstrate

\item {} 
\sphinxAtStartPar
Chunk 2: with multiple sentences to demonstrate sliding window chunking.

\item {} 
\sphinxAtStartPar
Chunk 3: sliding window chunking.

\end{itemize}

\end{description}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Semantic Chunking}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Splits text based on natural language boundaries such as paragraphs, sentences, or specific delimiters (e.g., headings).

\item {} 
\sphinxAtStartPar
Retains semantic coherence, ideal for better retrieval and generation accuracy.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{nltk}
\PYG{n}{nltk}\PYG{o}{.}\PYG{n}{download}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{punkt\PYGZus{}tab}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{semantic\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{sentence\PYGZus{}len}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{sentences} \PYG{o}{=} \PYG{n}{nltk}\PYG{o}{.}\PYG{n}{sent\PYGZus{}tokenize}\PYG{p}{(}\PYG{n}{text}\PYG{p}{)}

    \PYG{n}{chunks} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{chunk} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{for} \PYG{n}{sentence} \PYG{o+ow}{in} \PYG{n}{sentences}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{chunk}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sentence}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{sentence\PYGZus{}len}\PYG{p}{:}
            \PYG{n}{chunk} \PYG{o}{+}\PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n}{sentence}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{chunks}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{chunk}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{chunk} \PYG{o}{=} \PYG{n}{sentence}
    \PYG{k}{if} \PYG{n}{chunk}\PYG{p}{:}
        \PYG{n}{chunks}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{chunk}\PYG{o}{.}\PYG{n}{strip}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{chunks}

\PYG{c+c1}{\PYGZsh{} Example Usage}
\PYG{n}{document} \PYG{o}{=} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{This is a sample document. It is split based on semantic boundaries. }\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Each chunk will have coherent meaning for better retrieval.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{chunks} \PYG{o}{=} \PYG{n}{semantic\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{document}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{chunk} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{chunks}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Chunk }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{idx}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{chunk}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Output}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Chunk 1: This is a sample document.

\item {} 
\sphinxAtStartPar
Chunk 2: It is split based on semantic boundaries.

\item {} 
\sphinxAtStartPar
Chunk 3: Each chunk will have coherent meaning for better retrieval.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dynamic Chunking}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Adapts chunk sizes based on content properties such as token count, content density, or specific criteria.

\item {} 
\sphinxAtStartPar
Useful when handling diverse document types with varying information density.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Example}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoTokenizer}

\PYG{k}{def} \PYG{n+nf}{dynamic\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{max\PYGZus{}tokens}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{tokenizer\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}uncased}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{tokenizer\PYGZus{}name}\PYG{p}{)}
    \PYG{n}{tokens} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{n}{text}\PYG{p}{,} \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{chunks} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{tokens}\PYG{p}{)}\PYG{p}{,} \PYG{n}{max\PYGZus{}tokens}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{chunk} \PYG{o}{=} \PYG{n}{tokens}\PYG{p}{[}\PYG{n}{i}\PYG{p}{:}\PYG{n}{i} \PYG{o}{+} \PYG{n}{max\PYGZus{}tokens}\PYG{p}{]}
        \PYG{n}{chunks}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{decode}\PYG{p}{(}\PYG{n}{chunk}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{chunks}

\PYG{c+c1}{\PYGZsh{} Example Usage}
\PYG{n}{document} \PYG{o}{=} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{This is a sample document to demonstrate dynamic chunking. }\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The tokenizer adapts the chunks based on token limits.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{chunks} \PYG{o}{=} \PYG{n}{dynamic\PYGZus{}chunking}\PYG{p}{(}\PYG{n}{document}\PYG{p}{,} \PYG{n}{max\PYGZus{}tokens}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{chunk} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{chunks}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Chunk }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{idx}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{chunk}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{description}
\sphinxlineitem{\sphinxstylestrong{Output}:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Chunk 1: this is a sample document to demonstrate dynamic chunking

\item {} 
\sphinxAtStartPar
Chunk 2: . the tokenizer adapts the chunks based on

\item {} 
\sphinxAtStartPar
Chunk 3: token limits.

\end{itemize}

\end{description}

\end{enumerate}

\sphinxAtStartPar
Comparison of Strategies


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Strategy
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Pros
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Cons
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Fixed\sphinxhyphen{}Length Chunking
&
\sphinxAtStartPar
Simple, fast
&
\sphinxAtStartPar
May split text mid\sphinxhyphen{}sentence
or lose coherence.
\\
\sphinxhline
\sphinxAtStartPar
Sliding Window Chunking
&
\sphinxAtStartPar
Preserves context
&
\sphinxAtStartPar
Overlapping increases redundancy.
\\
\sphinxhline
\sphinxAtStartPar
Semantic Chunking
&
\sphinxAtStartPar
Coherent chunks
&
\sphinxAtStartPar
Requires NLP preprocessing.
\\
\sphinxhline
\sphinxAtStartPar
Dynamic Chunking
&
\sphinxAtStartPar
Adapts to content
&
\sphinxAtStartPar
Computationally intensive.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Each strategy has its strengths and weaknesses. Select based on the task requirements, context, and available computational resources.

\sphinxAtStartPar
The optimal chunk length depends on the type of content being processed and the intended use case.
Below are recommendations for chunk lengths based on different context types, along with their rationale:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Context Type
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Chunk Length (Tokens)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Rationale
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
FAQs or Short Texts
&
\sphinxAtStartPar
100\sphinxhyphen{}200
&
\sphinxAtStartPar
Short enough to handle specific queries.
\\
\sphinxhline
\sphinxAtStartPar
Articles or Blog Posts
&
\sphinxAtStartPar
300\sphinxhyphen{}500
&
\sphinxAtStartPar
Covers logical sections while fitting multiple
chunks in the LLM context.
\\
\sphinxhline
\sphinxAtStartPar
Research Papers or Reports
&
\sphinxAtStartPar
500\sphinxhyphen{}700
&
\sphinxAtStartPar
Captures detailed sections like methodology
or results.
\\
\sphinxhline
\sphinxAtStartPar
Legal or Technical Texts
&
\sphinxAtStartPar
200\sphinxhyphen{}300
&
\sphinxAtStartPar
Maintains precision due to dense information.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The valuating Chunking Strategies for Retrieval can be found at: \sphinxurl{https://research.trychroma.com/evaluating-chunking}


\subsection{Late Chunking}
\label{\detokenize{rag:late-chunking}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{late_chunking}.png}
\caption{Naive chunking v.s. late chunking (Souce \sphinxhref{https://www.marktechpost.com/2024/08/27/jina-ai-introduced-late-chunking-a-simple-ai-approach-to-embed-short-chunks-by-leveraging-the-power-of-long-context-embedding-models/}{Jina AI})}\label{\detokenize{rag:id3}}\label{\detokenize{rag:fig-late-chunk}}\end{figure}


\subsection{Types of Indexing}
\label{\detokenize{rag:types-of-indexing}}
\sphinxAtStartPar
The embedding methods we introduced in Chapter {\hyperref[\detokenize{embedding:embedding}]{\sphinxcrossref{\DUrole{std}{\DUrole{std-ref}{Word and Sentence Embedding}}}}} can be applied here to convert each chunk
into embeddings and create indexing. These indexings(embeddings) will be used to retrieve relevant documents or information.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sparse Indexing:

\sphinxAtStartPar
Uses traditional keyword\sphinxhyphen{}based methods (e.g., TF\sphinxhyphen{}IDF, BM25).
Index stores the frequency of terms and their associations with documents.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Advantages:Easy to understand and deploy and works well for exact matches or keyword\sphinxhyphen{}heavy queries.

\item {} 
\sphinxAtStartPar
Disadvantages: Struggles with semantic understanding or paraphrased queries.

\end{itemize}

\item {} 
\sphinxAtStartPar
Dense Indexing:

\sphinxAtStartPar
Uses vector embeddings to capture semantic meaning. Documents are represented as vectors in a
high\sphinxhyphen{}dimensional space, enabling similarity search.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Advantages: Excellent for semantic search, handling synonyms, and paraphrasing.

\item {} 
\sphinxAtStartPar
Disadvantages: Requires more computational resources for storage and retrieval.

\end{itemize}

\item {} 
\sphinxAtStartPar
Hybrid Indexing:

\sphinxAtStartPar
Combines sparse and dense indexing for more robust search capabilities. For example, Elasticsearch
can integrate BM25 with vector search.

\end{itemize}


\subsection{Vector Database}
\label{\detokenize{rag:vector-database}}
\sphinxAtStartPar
Vector databases are essential for Retrieval\sphinxhyphen{}Augmented Generation (RAG) systems, enabling
efficient similarity search on dense vector embeddings. Below is a comprehensive overview
of popular vector databases for RAG workflows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{FAISS (Facebook AI Similarity Search)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} An open\sphinxhyphen{}source library developed by Facebook AI for efficient similarity search and clustering of dense vectors.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} High performance and scalability.
\sphinxhyphen{} Supports various indexing methods like \sphinxcode{\sphinxupquote{Flat}}, \sphinxcode{\sphinxupquote{IVF}}, and \sphinxcode{\sphinxupquote{HNSW}}.
\sphinxhyphen{} GPU acceleration for faster searches.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Research and prototyping.
\sphinxhyphen{} Scenarios requiring custom implementations.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Limitations}:
\sphinxhyphen{} File\sphinxhyphen{}based storage; lacks a built\sphinxhyphen{}in distributed or managed cloud solution.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://github.com/facebookresearch/faiss}{FAISS GitHub}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Pinecone}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} A fully managed vector database designed for production\sphinxhyphen{}scale workloads.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Scalable and serverless architecture.
\sphinxhyphen{} Automatic scaling and optimization of indexes.
\sphinxhyphen{} Hybrid search (combining vector and keyword search).
\sphinxhyphen{} Integrates with popular frameworks like LangChain and OpenAI.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Enterprise\sphinxhyphen{}grade applications.
\sphinxhyphen{} Handling large datasets with minimal operational overhead.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://www.pinecone.io/}{Pinecone}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Weaviate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} An open\sphinxhyphen{}source vector search engine with a strong focus on modularity and customization.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Supports hybrid search and symbolic reasoning.
\sphinxhyphen{} Schema\sphinxhyphen{}based data organization.
\sphinxhyphen{} Plugin support for pre\sphinxhyphen{}built and custom vectorization modules.
\sphinxhyphen{} Cloud\sphinxhyphen{}managed and self\sphinxhyphen{}hosted options.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Applications requiring hybrid search capabilities.
\sphinxhyphen{} Knowledge graphs and semantically rich data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://weaviate.io/}{Weaviate}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Milvus}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} An open\sphinxhyphen{}source, high\sphinxhyphen{}performance vector database designed for similarity search on large datasets.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Distributed and scalable architecture.
\sphinxhyphen{} Integration with FAISS, Annoy, and HNSW indexing techniques.
\sphinxhyphen{} Built\sphinxhyphen{}in support for time travel queries (searching historical data).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Video, audio, and image search applications.
\sphinxhyphen{} Large\sphinxhyphen{}scale datasets requiring real\sphinxhyphen{}time indexing and retrieval.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://milvus.io/}{Milvus}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Qdrant}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} An open\sphinxhyphen{}source, lightweight vector database focused on ease of use and modern developer needs.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Supports HNSW for efficient vector search.
\sphinxhyphen{} Advanced filtering capabilities for combining metadata with vector queries.
\sphinxhyphen{} REST and gRPC APIs for integration.
\sphinxhyphen{} Docker\sphinxhyphen{}ready deployment.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Scenarios requiring metadata\sphinxhyphen{}rich search.
\sphinxhyphen{} Lightweight deployments with simplicity in mind.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://qdrant.tech/}{Qdrant}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Redis (with Vector Similarity Search Module)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} A popular in\sphinxhyphen{}memory database with a module for vector similarity search.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Combines vector search with traditional key\sphinxhyphen{}value storage.
\sphinxhyphen{} Supports hybrid search and metadata filtering.
\sphinxhyphen{} High throughput and low latency due to in\sphinxhyphen{}memory architecture.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Applications requiring real\sphinxhyphen{}time, low\sphinxhyphen{}latency search.
\sphinxhyphen{} Integrating vector search with existing Redis\sphinxhyphen{}based systems.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://redis.io/docs/stack/search/}{Redis Vector Search}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Zilliz}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} A cloud\sphinxhyphen{}native vector database built on Milvus for scalable and managed vector storage.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Fully managed service for vector data.
\sphinxhyphen{} Seamless scaling and distributed indexing.
\sphinxhyphen{} Integration with machine learning pipelines.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Large\sphinxhyphen{}scale enterprise deployments.
\sphinxhyphen{} Cloud\sphinxhyphen{}native solutions with minimal infrastructure management.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://zilliz.com/}{Zilliz}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Vespa}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} A real\sphinxhyphen{}time serving engine supporting vector and hybrid search.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Combines vector search with advanced ranking and filtering.
\sphinxhyphen{} Scales to large datasets with support for distributed clusters.
\sphinxhyphen{} Powerful query configuration options.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} E\sphinxhyphen{}commerce and recommendation systems.
\sphinxhyphen{} Applications with complex ranking requirements.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://vespa.ai/}{Vespa}

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chroma}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Description}:
\sphinxhyphen{} An open\sphinxhyphen{}source, user\sphinxhyphen{}friendly vector database built for LLMs and embedding\sphinxhyphen{}based applications.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Features}:
\sphinxhyphen{} Designed specifically for RAG workflows.
\sphinxhyphen{} Simple Python API for seamless integration with AI models.
\sphinxhyphen{} Efficient and customizable vector storage for embedding data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Use Cases}:
\sphinxhyphen{} Prototyping and experimentation for LLM\sphinxhyphen{}based applications.
\sphinxhyphen{} Lightweight deployments for small to medium\sphinxhyphen{}scale RAG systems.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Official Website}: \sphinxhref{https://www.trychroma.com/}{Chroma}

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
Comparison of Vector Databases:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Database}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Open Source}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Managed Service}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Key Features}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Best For}
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
FAISS
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
High performance, GPU acceleration
&
\sphinxAtStartPar
Research, prototyping
\\
\sphinxhline
\sphinxAtStartPar
Pinecone
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
Serverless, automatic scaling
&
\sphinxAtStartPar
Enterprise\sphinxhyphen{}scale applications
\\
\sphinxhline
\sphinxAtStartPar
Weaviate
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
Hybrid search, modularity
&
\sphinxAtStartPar
Knowledge graphs
\\
\sphinxhline
\sphinxAtStartPar
Milvus
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
Distributed, high performance
&
\sphinxAtStartPar
Large\sphinxhyphen{}scale datasets
\\
\sphinxhline
\sphinxAtStartPar
Qdrant
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
Lightweight, metadata filtering
&
\sphinxAtStartPar
Small to medium\sphinxhyphen{}scale apps
\\
\sphinxhline
\sphinxAtStartPar
Redis
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
In\sphinxhyphen{}memory performance, hybrid search
&
\sphinxAtStartPar
Real\sphinxhyphen{}time apps
\\
\sphinxhline
\sphinxAtStartPar
Zilliz
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
Fully managed Milvus
&
\sphinxAtStartPar
Enterprise cloud solutions
\\
\sphinxhline
\sphinxAtStartPar
Vespa
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
Hybrid search, real\sphinxhyphen{}time ranking
&
\sphinxAtStartPar
E\sphinxhyphen{}commerce, recommendations
\\
\sphinxhline
\sphinxAtStartPar
Chroma
&
\sphinxAtStartPar
Yes
&
\sphinxAtStartPar
No
&
\sphinxAtStartPar
LLM\sphinxhyphen{}focused, simple API
&
\sphinxAtStartPar
Prototyping, lightweight apps
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Choosing a Vector Database
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{For Research or Small Projects}: FAISS, Qdrant, Milvus, or Chroma.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{For Enterprise or Cloud\sphinxhyphen{}Native Workflows}: Pinecone, Zilliz, or Weaviate.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{For Real\sphinxhyphen{}Time Use Cases}: Redis or Vespa.

\end{itemize}

\sphinxAtStartPar
Each database has unique strengths and is suited for specific RAG use cases. The choice depends on scalability, integration needs, and budget.


\section{Retrieval}
\label{\detokenize{rag:retrieval}}
\sphinxAtStartPar
The retriever selects ‚Äúchunks‚Äù of text (e.g., paragraphs or sections) relevant to the user‚Äôs query.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.5]{{retriever}.png}
\end{figure}


\subsection{Common retrieval methods}
\label{\detokenize{rag:common-retrieval-methods}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Sparse Vector Search: Traditional keyword\sphinxhyphen{}based retrieval (e.g., TF\sphinxhyphen{}IDF, BM25).

\item {} 
\sphinxAtStartPar
Dense Vector Search: Vector\sphinxhyphen{}based search using embeddings e.g.
\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{Approximate Nearest Neighbor (ANN) Search:}\begin{itemize}
\item {} 
\sphinxAtStartPar
HNSW (Hierarchical Navigable Small World): Graph\sphinxhyphen{}based approach

\item {} 
\sphinxAtStartPar
IVF (Inverted File Index): Clusters embeddings into groups and searches within relevant clusters.

\end{itemize}

\end{description}

\item {} 
\sphinxAtStartPar
Exact Nearest Neighbor Search: Computes similarities exhaustively for all vectors in the corpus

\end{itemize}

\item {} 
\sphinxAtStartPar
Hybrid Search: the combination of Sparse and Dense vector search.

\end{itemize}

\sphinxAtStartPar
Summary of Common Algorithms:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Metric/Algorithm}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Purpose}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Common Use}
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{TF\sphinxhyphen{}IDF}
&
\sphinxAtStartPar
Keyword matching with term weighting.
&
\sphinxAtStartPar
Effective for small\sphinxhyphen{}scale or structured corpora.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{BM25}
&
\sphinxAtStartPar
Advanced keyword matching with term frequency
saturation and document length normalization.
&
\sphinxAtStartPar
Widely used in sparse search; default in tools like
Elasticsearch and Solr.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Cosine Similarity}
&
\sphinxAtStartPar
Measures orientation (ignores magnitude).
&
\sphinxAtStartPar
Widely used; works well with normalized vectors.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Dot Product Similarity}
&
\sphinxAtStartPar
Measures magnitude and direction.
&
\sphinxAtStartPar
Preferred in embeddings like OpenAI‚Äôs models.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{Euclidean Distance}
&
\sphinxAtStartPar
Measures absolute distance between vectors.
&
\sphinxAtStartPar
Less common but used in some specific cases.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{HNSW (ANN)}
&
\sphinxAtStartPar
Fast and scalable nearest neighbor search.
&
\sphinxAtStartPar
Default for large\sphinxhyphen{}scale systems (e.g., FAISS).
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{IVF (ANN)}
&
\sphinxAtStartPar
Efficient clustering\sphinxhyphen{}based search.
&
\sphinxAtStartPar
Often combined with product quantization.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\subsection{Reciprocal Rank Fusion}
\label{\detokenize{rag:reciprocal-rank-fusion}}
\sphinxAtStartPar
Reciprocal Rank Fusion (RRF) is a ranking technique commonly used in information retrieval
and ensemble learning. Although it is not specific to large language models (LLMs), it can
be applied to scenarios where multiple ranking systems (or scoring mechanisms) produce
different rankings, and you want to combine them into a single, unified ranking.
\begin{description}
\sphinxlineitem{The reciprocal rank of an item in a ranked list is calculated as \(\frac{1}{k+r}\), where}\begin{itemize}
\item {} 
\sphinxAtStartPar
r is the rank of the item (1 for the top rank, 2 for the second rank, etc.).

\item {} 
\sphinxAtStartPar
k is a small constant (often set to 60 or another fixed value) to control how much weight is given to higher ranks.

\end{itemize}

\end{description}

\sphinxAtStartPar
Example:

\sphinxAtStartPar
Suppose two retrieval models give ranked lists for query responses:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Model 1 ranks documents as: {[}A,B,C,D{]}

\item {} 
\sphinxAtStartPar
Model 2 ranks documents as: {[}B,A,D,C{]}

\end{itemize}

\sphinxAtStartPar
RRF combines these rankings by assigning each document a combined score:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Document A: \(\frac{1}{60+1} +\frac{1}{60+2}=0.03252247488101534\)

\item {} 
\sphinxAtStartPar
Document B: \(\frac{1}{60+2} +\frac{1}{60+1}=0.03252247488101534\)

\item {} 
\sphinxAtStartPar
Document C: \(\frac{1}{60+3} +\frac{1}{60+4}=0.03149801587301587\)

\item {} 
\sphinxAtStartPar
Document D: \(\frac{1}{60+4} +\frac{1}{60+3}=0.03149801587301587\)

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{defaultdict}

\PYG{k}{def} \PYG{n+nf}{reciprocal\PYGZus{}rank\PYGZus{}fusion}\PYG{p}{(}\PYG{n}{ranked\PYGZus{}results}\PYG{p}{:} \PYG{n+nb}{list}\PYG{p}{[}\PYG{n+nb}{list}\PYG{p}{]}\PYG{p}{,} \PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{60}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Fuse rank from multiple retrieval systems using Reciprocal Rank Fusion.}

\PYG{l+s+sd}{    Args:}
\PYG{l+s+sd}{    ranked\PYGZus{}results: Ranked results from different retrieval system.}
\PYG{l+s+sd}{    k (int): A constant used in the RRF formula (default is 60).}

\PYG{l+s+sd}{    Returns:}
\PYG{l+s+sd}{    Tuple of list of sorted documents by score and sorted documents}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} Dictionary to store RRF mapping}
    \PYG{n}{rrf\PYGZus{}map} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Calculate RRF score for each result in each list}
    \PYG{k}{for} \PYG{n}{rank\PYGZus{}list} \PYG{o+ow}{in} \PYG{n}{ranked\PYGZus{}results}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{rank}\PYG{p}{,} \PYG{n}{item} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{rank\PYGZus{}list}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{rrf\PYGZus{}map}\PYG{p}{[}\PYG{n}{item}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{p}{(}\PYG{n}{rank} \PYG{o}{+} \PYG{n}{k}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Sort items based on their RRF scores in descending order}
    \PYG{n}{sorted\PYGZus{}items} \PYG{o}{=} \PYG{n+nb}{sorted}\PYG{p}{(}\PYG{n}{rrf\PYGZus{}map}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{key}\PYG{o}{=}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{reverse}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Return tuple of list of sorted documents by score and sorted documents}
    \PYG{k}{return} \PYG{n}{sorted\PYGZus{}items}\PYG{p}{,} \PYG{p}{[}\PYG{n}{item} \PYG{k}{for} \PYG{n}{item}\PYG{p}{,} \PYG{n}{score} \PYG{o+ow}{in} \PYG{n}{sorted\PYGZus{}items}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Example ranked lists from different sources}
\PYG{n}{ranked\PYGZus{}a} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{ranked\PYGZus{}b} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}


\PYG{c+c1}{\PYGZsh{} Combine the lists using RRF}
\PYG{n}{combined\PYGZus{}list} \PYG{o}{=} \PYG{n}{reciprocal\PYGZus{}rank\PYGZus{}fusion}\PYG{p}{(}\PYG{p}{[}\PYG{n}{ranked\PYGZus{}a}\PYG{p}{,} \PYG{n}{ranked\PYGZus{}b}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{combined\PYGZus{}list}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.03252247488101534}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.03252247488101534}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.03149801587301587}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mf}{0.03149801587301587}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Generation}
\label{\detokenize{rag:generation}}
\sphinxAtStartPar
Finally, the retrieved relevant information will be feed back into the LLMs to generate responses.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{generator}.png}
\end{figure}

\sphinxstepscope


\chapter{Fine Tuning}
\label{\detokenize{finetuning:fine-tuning}}\label{\detokenize{finetuning:finetuning}}\label{\detokenize{finetuning::doc}}
\sphinxAtStartPar
Fine\sphinxhyphen{}tuning is a machine learning technique where a pre\sphinxhyphen{}trained model (like a large
language model or neural network) is further trained on a smaller, specific dataset
to adapt it to a particular task or domain. Instead of training a model from scratch,
fine\sphinxhyphen{}tuning leverages the knowledge already embedded in the pre\sphinxhyphen{}trained model,
saving time, computational resources, and data requirements.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{fine_tuning}.png}
\caption{The three conventional feature\sphinxhyphen{}based and finetuning approaches (Souce \sphinxhref{https://magazine.sebastianraschka.com/p/finetuning-large-language-models}{Finetuning Sebastian}).}\label{\detokenize{finetuning:id4}}\label{\detokenize{finetuning:fig-fine-tuning}}\end{figure}


\section{Cutting\sphinxhyphen{}Edge Strategies for LLM Fine\sphinxhyphen{}Tuning}
\label{\detokenize{finetuning:cutting-edge-strategies-for-llm-fine-tuning}}
\sphinxAtStartPar
Over the past year, fine\sphinxhyphen{}tuning methods have made remarkable strides. Modern methods
for fine\sphinxhyphen{}tuning LLMs focus on efficiency, scalability, and resource optimization.
The following strategies are at the forefront:


\subsection{\sphinxstylestrong{LoRA (Low\sphinxhyphen{}Rank Adaptation)}}
\label{\detokenize{finetuning:lora-low-rank-adaptation}}
\sphinxAtStartPar
\sphinxstylestrong{LoRA} reduces the number of trainable parameters by introducing \sphinxstylestrong{low\sphinxhyphen{}rank decomposition} into the fine\sphinxhyphen{}tuning process.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{lora}.png}
\caption{Weight update matrix (Souce \sphinxhref{https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms}{LORA Sebastian}).}\label{\detokenize{finetuning:id5}}\label{\detokenize{finetuning:fig-lora}}\end{figure}

\sphinxAtStartPar
\sphinxstylestrong{How It Works}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Instead of updating all model weights, LoRA injects \sphinxstylestrong{low\sphinxhyphen{}rank adapters} into the model‚Äôs layers.

\item {} 
\sphinxAtStartPar
The original pre\sphinxhyphen{}trained weights remain frozen; only the low\sphinxhyphen{}rank parameters are optimized.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduces memory and computational requirements.

\item {} 
\sphinxAtStartPar
Enables fine\sphinxhyphen{}tuning on resource\sphinxhyphen{}constrained hardware.

\end{itemize}


\subsection{\sphinxstylestrong{QLoRA (Quantized Low\sphinxhyphen{}Rank Adaptation)}}
\label{\detokenize{finetuning:qlora-quantized-low-rank-adaptation}}
\sphinxAtStartPar
\sphinxstylestrong{QLoRA} combines \sphinxstylestrong{low\sphinxhyphen{}rank adaptation} with \sphinxstylestrong{4\sphinxhyphen{}bit quantization} of the pre\sphinxhyphen{}trained model.

\sphinxAtStartPar
\sphinxstylestrong{How It Works}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The LLM is quantized to \sphinxstylestrong{4\sphinxhyphen{}bit precision} to reduce memory usage.

\item {} 
\sphinxAtStartPar
LoRA adapters are applied to the quantized model for fine\sphinxhyphen{}tuning.

\item {} 
\sphinxAtStartPar
Precision is maintained using methods like \sphinxstylestrong{NF4 (Normalized Float 4)} and double backpropagation.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Further reduces memory usage compared to LoRA.

\item {} 
\sphinxAtStartPar
Enables fine\sphinxhyphen{}tuning of massive models on consumer\sphinxhyphen{}grade GPUs.

\end{itemize}


\subsection{\sphinxstylestrong{PEFT (Parameter\sphinxhyphen{}Efficient Fine\sphinxhyphen{}Tuning)}}
\label{\detokenize{finetuning:peft-parameter-efficient-fine-tuning}}
\sphinxAtStartPar
\sphinxstylestrong{PEFT} is a general framework for fine\sphinxhyphen{}tuning LLMs with minimal trainable parameters.


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\sphinxthistablewithborderlessstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxtableatstartofbodyhook
\noindent\sphinxincludegraphics[width=1.000\linewidth]{{peft_1}.png}
&
\noindent\sphinxincludegraphics[width=1.000\linewidth]{{peft_2}.png}
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Source: \sphinxcite{reference:peft}

\sphinxAtStartPar
\sphinxstylestrong{Techniques Under PEFT}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{LoRA}: Low\sphinxhyphen{}rank adaptation of weights.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Adapters}: Small trainable layers inserted into the model.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Prefix Tuning}: Fine\sphinxhyphen{}tuning input prefixes instead of weights.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Prompt Tuning}: Optimizing soft prompts in the input space.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduces the number of trainable parameters.

\item {} 
\sphinxAtStartPar
Faster training and lower hardware requirements.

\end{itemize}


\subsection{\sphinxstylestrong{SFT (Supervised Fine\sphinxhyphen{}Tuning)}}
\label{\detokenize{finetuning:sft-supervised-fine-tuning}}
\sphinxAtStartPar
\sphinxstylestrong{SFT} adapts an LLM using a labeled dataset in a fully supervised manner.

\sphinxAtStartPar
\sphinxstylestrong{How It Works}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The model is initialized with pre\sphinxhyphen{}trained weights.

\item {} 
\sphinxAtStartPar
It is fine\sphinxhyphen{}tuned on a task\sphinxhyphen{}specific dataset with a supervised loss function (e.g., cross\sphinxhyphen{}entropy).

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Achieves high performance on specific tasks.

\item {} 
\sphinxAtStartPar
Essential for aligning models with labeled datasets.

\end{itemize}


\subsection{Summary Table}
\label{\detokenize{finetuning:summary-table}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTT}
\sphinxtoprule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
\sphinxstylestrong{Method}
&
\sphinxAtStartPar
\sphinxstylestrong{Description}
&
\sphinxAtStartPar
\sphinxstylestrong{Key Benefit}
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{LoRA}
&
\sphinxAtStartPar
Low\sphinxhyphen{}rank adapters for parameter\sphinxhyphen{}efficient
tuning.
&
\sphinxAtStartPar
Reduces trainable parameters significantly.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{QLoRA}
&
\sphinxAtStartPar
LoRA with 4\sphinxhyphen{}bit quantization of the model.
&
\sphinxAtStartPar
Fine\sphinxhyphen{}tunes massive models on smaller
hardware.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{PEFT}
&
\sphinxAtStartPar
General framework for efficient fine\sphinxhyphen{}tuning.
&
\sphinxAtStartPar
Includes LoRA, Adapters, Prefix Tuning,
etc.
\\
\sphinxhline
\sphinxAtStartPar
\sphinxstylestrong{SFT}
&
\sphinxAtStartPar
Supervised fine\sphinxhyphen{}tuning with labeled data.
&
\sphinxAtStartPar
High performance on task\sphinxhyphen{}specific datasets
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
These strategies represent the forefront of \sphinxstylestrong{LLM fine\sphinxhyphen{}tuning}, offering efficient and scalable solutions for
real\sphinxhyphen{}world applications. To choose the most suitable strategy, consider the following factors:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource\sphinxhyphen{}Constrained Environments}: Use \sphinxstylestrong{LoRA} or \sphinxstylestrong{QLoRA}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Large\sphinxhyphen{}Scale Models}: \sphinxstylestrong{QLoRA} for low\sphinxhyphen{}memory fine\sphinxhyphen{}tuning.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High Performance with Labeled Data}: \sphinxstylestrong{SFT}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Minimal Setup}: \sphinxstylestrong{Zero\sphinxhyphen{}shot} or \sphinxstylestrong{Few\sphinxhyphen{}shot} learning.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{General Efficiency}: Use \sphinxstylestrong{PEFT} frameworks.

\end{itemize}


\section{Key Early Fine\sphinxhyphen{}Tuning Methods}
\label{\detokenize{finetuning:key-early-fine-tuning-methods}}
\sphinxAtStartPar
Early fine\sphinxhyphen{}tuning methods laid the foundation for current approaches. These methods
primarily focused on updating the entire model or selected components.


\subsection{\sphinxstylestrong{Full Fine\sphinxhyphen{}Tuning}}
\label{\detokenize{finetuning:full-fine-tuning}}
\sphinxAtStartPar
All the parameters of a pre\sphinxhyphen{}trained model are updated using task\sphinxhyphen{}specific data {\hyperref[\detokenize{finetuning:fig-fine-tuning}]{\sphinxcrossref{\DUrole{std}{\DUrole{std-ref}{The three conventional feature\sphinxhyphen{}based and finetuning approaches (Souce Finetuning Sebastian).}}}}} (right).

\sphinxAtStartPar
\sphinxstylestrong{How It Works}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The pre\sphinxhyphen{}trained model serves as the starting point.

\item {} 
\sphinxAtStartPar
Fine\sphinxhyphen{}tuning is conducted on a smaller, labeled dataset using a supervised loss function.

\item {} 
\sphinxAtStartPar
A low learning rate is used to prevent \sphinxstylestrong{catastrophic forgetting}.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Effective at adapting models to specific tasks.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Challenges}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Computationally expensive.

\item {} 
\sphinxAtStartPar
Risk of overfitting on small datasets.

\end{itemize}


\subsection{\sphinxstylestrong{Feature\sphinxhyphen{}Based Approach}}
\label{\detokenize{finetuning:feature-based-approach}}
\sphinxAtStartPar
The pre\sphinxhyphen{}trained model is used as a \sphinxstylestrong{feature extractor}, while only a task\sphinxhyphen{}specific head is trained {\hyperref[\detokenize{finetuning:fig-fine-tuning}]{\sphinxcrossref{\DUrole{std}{\DUrole{std-ref}{The three conventional feature\sphinxhyphen{}based and finetuning approaches (Souce Finetuning Sebastian).}}}}} (left).

\sphinxAtStartPar
\sphinxstylestrong{How It Works}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The model processes inputs and extracts features (embeddings).

\item {} 
\sphinxAtStartPar
A separate classifier (e.g., linear or MLP) is trained on top of these features.

\item {} 
\sphinxAtStartPar
The pre\sphinxhyphen{}trained model weights remain \sphinxstylestrong{frozen}.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Computationally efficient since only the task\sphinxhyphen{}specific head is trained.

\end{itemize}


\subsection{\sphinxstylestrong{Layer\sphinxhyphen{}Specific Fine\sphinxhyphen{}Tuning}}
\label{\detokenize{finetuning:layer-specific-fine-tuning}}
\sphinxAtStartPar
Only certain layers of the pre\sphinxhyphen{}trained model are fine\sphinxhyphen{}tuned while the rest remain frozen {\hyperref[\detokenize{finetuning:fig-fine-tuning}]{\sphinxcrossref{\DUrole{std}{\DUrole{std-ref}{The three conventional feature\sphinxhyphen{}based and finetuning approaches (Souce Finetuning Sebastian).}}}}} (middle).

\sphinxAtStartPar
\sphinxstylestrong{How It Works}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Earlier layers (which capture general features) are frozen.

\item {} 
\sphinxAtStartPar
Later layers (closer to the output) are fine\sphinxhyphen{}tuned on task\sphinxhyphen{}specific data.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Balances computational efficiency and task adaptation.

\end{itemize}


\subsection{\sphinxstylestrong{Task\sphinxhyphen{}Adaptive Pre\sphinxhyphen{}training}}
\label{\detokenize{finetuning:task-adaptive-pre-training}}
\sphinxAtStartPar
Before fine\sphinxhyphen{}tuning on a specific task, the model undergoes additional \sphinxstylestrong{pre\sphinxhyphen{}training} on a domain\sphinxhyphen{}specific corpus.

\sphinxAtStartPar
\sphinxstylestrong{How It Works}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
A general pre\sphinxhyphen{}trained model is further pre\sphinxhyphen{}trained (unsupervised) on domain\sphinxhyphen{}specific data.

\item {} 
\sphinxAtStartPar
Fine\sphinxhyphen{}tuning is then performed on the downstream task.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Benefits}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Provides a better starting point for domain\sphinxhyphen{}specific tasks.

\end{itemize}


\section{Embedding Model Fine\sphinxhyphen{}Tuning}
\label{\detokenize{finetuning:embedding-model-fine-tuning}}
\sphinxAtStartPar
\sphinxcite{reference:finetuneembedding}


\subsection{Results Comparison}
\label{\detokenize{finetuning:results-comparison}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Dimension
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Baseline
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Fine\sphinxhyphen{}tuned
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Improvement
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
768
&
\sphinxAtStartPar
0.75490
&
\sphinxAtStartPar
0.76503
&
\sphinxAtStartPar
1.34\%
\\
\sphinxhline
\sphinxAtStartPar
512
&
\sphinxAtStartPar
0.75492
&
\sphinxAtStartPar
0.76040
&
\sphinxAtStartPar
0.73\%
\\
\sphinxhline
\sphinxAtStartPar
256
&
\sphinxAtStartPar
0.74547
&
\sphinxAtStartPar
0.75474
&
\sphinxAtStartPar
1.24\%
\\
\sphinxhline
\sphinxAtStartPar
128
&
\sphinxAtStartPar
0.71167
&
\sphinxAtStartPar
0.72053
&
\sphinxAtStartPar
1.24\%
\\
\sphinxhline
\sphinxAtStartPar
64
&
\sphinxAtStartPar
0.64772
&
\sphinxAtStartPar
0.66091
&
\sphinxAtStartPar
2.04\%
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{LLM Fine\sphinxhyphen{}Tuning}
\label{\detokenize{finetuning:llm-fine-tuning}}
\sphinxAtStartPar
\sphinxcite{reference:finetunellm}

\sphinxstepscope


\chapter{Pre\sphinxhyphen{}training}
\label{\detokenize{pretraining:pre-training}}\label{\detokenize{pretraining:pretraining}}\label{\detokenize{pretraining::doc}}
\sphinxstepscope


\chapter{LLM Evaluation Metrics}
\label{\detokenize{evaluation:llm-evaluation-metrics}}\label{\detokenize{evaluation:evaluation}}\label{\detokenize{evaluation::doc}}

\section{Statistical Scorers}
\label{\detokenize{evaluation:statistical-scorers}}

\section{Model\sphinxhyphen{}Based Scorers}
\label{\detokenize{evaluation:model-based-scorers}}
\sphinxstepscope


\chapter{Main Reference}
\label{\detokenize{reference:main-reference}}\label{\detokenize{reference:reference}}\label{\detokenize{reference::doc}}
\begin{sphinxthebibliography}{fineTune}
\bibitem[GenAI]{reference:genai}
\sphinxAtStartPar
Wenqiang Feng, Di Zhen.
\sphinxhref{https://runawayhorse001.github.io/GenAI\_Best\_Practices}{GenAI: Best Practices}, 2024.
\bibitem[PySpark]{reference:pyspark}
\sphinxAtStartPar
Wenqiang Feng.
\sphinxhref{https://runawayhorse001.github.io/LearningApacheSpark}{Learning Apache Spark with Python}, 2017.
\bibitem[lateChunking]{reference:latechunking}
\sphinxAtStartPar
Michael Gunther etc.
\sphinxhref{https://arxiv.org/pdf/2409.04701}{Late Chunking: Contextual Chunk Embeddings Using Long\sphinxhyphen{}Context Embedding Models}, 2024.
\bibitem[PEFT]{reference:peft}
\sphinxAtStartPar
Yunho Mo etc.
\sphinxhref{https://www.mdpi.com/2227-7390/11/14/3048}{Parameter\sphinxhyphen{}Efficient Fine\sphinxhyphen{}Tuning Method for Task\sphinxhyphen{}Oriented Dialogue Systems}, 2023.
\bibitem[fineTuneEmbedding]{reference:finetuneembedding}
\sphinxAtStartPar
Philipp Schmid.
\sphinxhref{https://www.philschmid.de/fine-tune-embedding-model-for-rag}{Fine\sphinxhyphen{}tune Embedding models for Retrieval Augmented Generation (RAG)}, 2017.
\bibitem[fineTuneLLM]{reference:finetunellm}
\sphinxAtStartPar
Maxime Labonne.
\sphinxhref{https://mlabonne.github.io/blog/posts/Fine\_Tune\_Your\_Own\_Llama\_2\_Model\_in\_a\_Colab\_Notebook.html}{Fine\sphinxhyphen{}Tune Your Own Llama 2 Model in a Colab Notebook}, 2024.
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}