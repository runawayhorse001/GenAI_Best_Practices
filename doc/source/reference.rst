.. _reference:

==============
Main Reference 
==============

.. [GenAI] Wenqiang Feng, Di Zhen.
                 `GenAI: Best Practices  <https://runawayhorse001.github.io/GenAI_Best_Practices>`_, 2024.

.. [PySpark] Wenqiang Feng.
                 `Learning Apache Spark with Python  <https://runawayhorse001.github.io/LearningApacheSpark>`_, 2017.

.. [lateChunking] Michael Gunther etc.
                 `Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models
                 <https://arxiv.org/pdf/2409.04701>`_, 2024.

.. [selfRAG] Akari Asai etc.
                 `Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection
                 <https://arxiv.org/pdf/2310.11511>`_, 2023.

.. [PEFT] Yunho Mo etc.
                 `Parameter-Efficient Fine-Tuning Method for Task-Oriented Dialogue Systems
                 <https://www.mdpi.com/2227-7390/11/14/3048>`_, 2023.

.. [fineTuneEmbedding] Philipp Schmid.
                 `Fine-tune Embedding models for Retrieval Augmented Generation (RAG)
                 <https://www.philschmid.de/fine-tune-embedding-model-for-rag>`_, 2024.


.. [attentionAllYouNeed] Ashish Vaswani etc.
                 `Attention Is All You Need
                 <https://arxiv.org/pdf/1706.03762>`_, 2017.


.. [fineTuneLLM] Maxime Labonne.
                 `Fine-Tune Your Own Llama 2 Model in a Colab Notebook
                 <https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html>`_, 2024.

.. [GEval] Yang Liu.
                 `G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment
                 <https://arxiv.org/pdf/2303.16634>`_, 2023.


.. [Tri_Dao_1] Tri Dao etc.
                 `FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
                 <https://arxiv.org/abs/2205.14135>`_, 2022.          

.. [Tri_Dao_2] Tri Dao.
                 `FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
                 <https://arxiv.org/abs/2307.08691>`_, 2023.

.. [Jay_Shah] Jay Shah etc.
                    `FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision
                    <https://arxiv.org/abs/2407.08608>`_, 2024.

.. [Andrei] Andrei Ivanov etc.
                 `Data Movement Is All You Need: A Case Study on Optimizing Transformers
                 <https://arxiv.org/abs/2007.00072>`_, 2024.

.. [YiDong] Yi Dong etc.
                 `Safeguarding Large Language Models: A Survey
                 <https://arxiv.org/abs/2406.02622>`_, 2024.

.. [HakanInan] Hakan Inan etc.
                 `Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations
                 <https://arxiv.org/abs/2312.06674>`_, 2023.

.. [LucaBeurerKellner] Luca Beurer-Kellner etc.
                 `Prompting Is Programming: A Query Language for Large Language Models
                 <https://arxiv.org/abs/2212.06094>`_, 2022.

.. [LongOuyang] Long Ouyang etc.
                 `Training language models to follow instructions with human feedback
                 <https://arxiv.org/abs/2203.02155>`_, 2022.

.. [JohnSchulman] John Schulman etc.
                 `Proximal Policy Optimization Algorithms
                 <https://arxiv.org/abs/1707.06347>`_, 2017.

.. [RuiZheng] Rui Zheng etc.
                 `Secrets of RLHF in Large Language Models Part I: PPO
                 <https://arxiv.org/abs/2307.04964>`_, 2023.

.. [RafaelRafailov] Rafael Rafailov etc.
                 `Direct Preference Optimization: Your Language Model is Secretly a Reward Model
                 <https://arxiv.org/abs/2305.18290>`_, 2023.

.. [DeepSeek-V3] DeepSeek AI.
                 `DeepSeek-V3 Technical Report
                 <https://arxiv.org/abs/2412.19437>`_, 2024.

.. [DeepSeek-V2] DeepSeek AI.
                 `DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
                 <https://arxiv.org/abs/2405.04434>`_, 2024.