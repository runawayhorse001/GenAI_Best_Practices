.. _rlhf:

=================================================
Reinforcement Learning from Human Feedback (RLHF)
=================================================

The process of training a model using reinforcement learning from human
feedback (RLHF) involves three key steps, as outlined in the paper
titled “`Training language models to follow instructions with human
feedback <https://arxiv.org/abs/2203.02155>`__” by OpenAI [LongOuyang]_.

.. figure:: images/instructGPT_overview_RLHF.png
   :alt: instructGPT_overview_RLHF
   :align: center

   InstructGPT Overview (Source: `Training language models to follow instructions with human feedback <https://arxiv.org/abs/2203.02155>`__)

PPO
---

Proximal Policy Optimization (PPO) (Paper: `Proximal Policy Optimization
Algorithms <https://arxiv.org/abs/1707.06347>`__) is a key algorithm
used in RLHF to fine-tune language models based on human preferences. It
is utilized to optimize the policy of a language model by maximizing a
reward function derived from human feedback. This process helps align
the model’s outputs with human values and preferences. [JohnSchulman]_

**State, Action, and Reward in the Context of LLMs**
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the context of LLMs, the components of reinforcement learning are
defined as follows:

1. **State**: The state corresponds to the **input prompt** or context
   provided to the language model. It represents the scenario or query
   that requires a response.
2. **Action**: The action is the **output** generated by the language
   model, i.e., the response or continuation of text based on the given
   state (prompt).
3. **Reward**: The reward is a scalar value that quantifies how well the
   generated response aligns with human preferences or task objectives.
   It is typically derived from a **reward model** trained on human
   feedback.
4. **Policy**: A policy refers to the strategy or function that maps a
   given state (input prompt and context) to an action (the next token
   or sequence of tokens to generate). The policy governs how the LLM
   generates responses and is optimized to maximize a reward signal,
   such as alignment with human preferences or task-specific objectives.

**Proximal Policy Optimization (PPO)** is a reinforcement learning
algorithm designed to optimize the policy of an agent in a stable and
efficient manner. It is particularly effective in environments with
discrete or continuous action spaces. Here’s an overview of PPO along
with its objective function:

**PPO Objective Function**
~~~~~~~~~~~~~~~~~~~~~~~~~~

PPO algorithm extends the CLIP objective by incorporating additional
terms for value function optimization and entropy regularization.

.. math::


   J^{PPO}(\theta) = E[J^{CLIP}(\theta) - c_1(V_\theta(s)-V_{target})^2 + c_2 H(s,\pi_\theta(\cdot))]

where

- :math:`J^{CLIP}(\theta)` is CLIP objective in policy gradient methods.
  The use of the minimum function ensures that if the new policy’s
  probability ratio deviates too much from 1 (indicating a significant
  change), it will not receive excessive credit (or blame) for its
  performance based on the advantage estimate.

  .. math::


     J^{CLIP}(\theta) = E[\min(r(\theta)\hat{A}_{\theta_{old}}(s,a)), \text{clip}(r(\theta),1-\epsilon, 1+\epsilon) \hat{A}_{\theta_{old}}(s,a)]

- :math:`-(V_\theta(s) - V_{target})^2` is the negative mean squared
  error (MSE), which we aim to maximize. It minimizes the difference
  between the predicted value function :math:`V_\theta(s)` and the
  target value :math:`V_{target}`. The coefficient :math:`c_2` controls
  the tradeoff between policy optimization and value function fitting.

- :math:`H(s,\pi_\theta(\cdot))` represents the entropy of the policy.
  Maximizing entropy encourages exploration by preventing premature
  convergence to deterministic policies. The coefficient :math:`c_2`
  determines the weight of this entropy term.

Below is a pseudocode of PPO-Clip Algorithm 

.. figure:: images/ppo_clip_algo.png
   :alt: ppo_clip_algo
   :align: center

   PPO Clip Algorithm (Source: `OpenAI Spinning Up - Proximal Policy Optimization <https://spinningup.openai.com/en/latest/algorithms/ppo.html>`__)

**Steps of RLHF Using PPO**
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The RLHF process using PPO involves three main stages:

1. **Training a Reward Model**: A reward model is trained to predict
   human preferences based on labeled data. Human annotators rank
   multiple responses for each prompt, and this ranking data is used to
   train the reward model in a supervised manner. The reward model
   learns to assign higher scores to responses that align better with
   human preferences.

2. **Fine-Tuning the LLM with PPO**: After training the reward model,
   PPO is used to fine-tune the LLM. The steps are as follows:

   1. **Initialize Policies**: Start with a pre-trained LLM as both the
      **policy model** (actor) and optionally as the critic for value
      estimation.

      - The **actor** is the language model that generates responses
        (actions) based on input prompts (states).

        For example: Input: “Explain quantum mechanics.” Output:
        “Quantum mechanics is a branch of physics that studies particles
        at atomic and subatomic scales.”

      - The **critic** is typically implemented as a **value function**,
        which predicts how good a particular response (action) is in
        terms of achieving long-term objectives. This model predicts a
        scalar value for each token or sequence, representing its
        expected reward or usefulness.

        For example:

        Input: “Explain quantum mechanics.” → “Quantum mechanics is…”
        Output: A value score indicating how well this response aligns
        with human preferences or task objectives.

      - Both the actor and critic can be initialized from the same
        pre-trained LLM weights to leverage shared knowledge from
        pretraining. However, their roles diverge during fine-tuning:
        The actor focuses on generating responses. The critic focuses on
        evaluating those responses.

   2. **Collect Rollouts**: Interact with the environment by sampling
      prompts from a dataset. Generate responses (actions) using the
      current policy. Compute rewards for these responses using the
      trained reward model.

   3. **Compute Advantage Estimates**: Use rewards from the reward model
      and value estimates from the critic to compute advantages:

      .. math::


         \hat{A}(s, a) = R_t + \gamma V(s_{t+1}) - V(s_t),

      where $ R_t $ is the reward from the reward model.

   4. **Optimize Policy with PPO Objective**: Optimize the policy using
      PPO’s clipped surrogate objective:

      .. math::


         J^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r(\theta)\hat{A}(s, a), \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)\hat{A}(s, a)\right)\right],

      where $ r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}
      $ is the probability ratio between new and old policies.

   5. **Update Value Function**: Simultaneously update the value
      function by minimizing mean squared error between predicted values
      and rewards:

      .. math::


         \mathcal{L}_{\text{value}} = \mathbb{E}\left[(V_\theta(s) - R_t)^2\right].

   6. **Repeat**: Iterate over multiple epochs until convergence,
      ensuring stable updates by clipping policy changes.

3. **Evaluation**: Evaluate the fine-tuned LLM on unseen prompts to
   ensure it generates outputs aligned with human preferences.
   Optionally, collect additional human feedback to further refine both
   the reward model and policy.

The following diagrams summarizes the high-level RLHF process with PPO,
from preference data creation, to training a reward model, and using
reward model in an RL loop to fine tune LLM.

.. figure:: images/PPO_RLHF_flowchart.png
   :alt: PPO_RLHF_flowchart
   :align: center

   Flowchart of PPO in RLHF

The following workflow chart illustrates the more detailed training
process of RLHF with PPO. [RuiZheng]_ 

.. figure:: images/RLHF_training_realworld.png
   :alt: RLHF_training_realworld
   :align: center

   RLHF Training Workflow (Source: `Secrets of RLHF in Large Language Models Part I PPO <https://arxiv.org/abs/2307.04964>`__)

RLHF Training Tricks
~~~~~~~~~~~~~~~~~~~~

There are practical challenges that arise during RLHF training. These
challenges stem from the inherent complexities of RL, especially when
applied to aligning LLMs with human preferences. Therefore, tricks are
essential for addressing the practical limitations of RLHF, ensuring the
training process remains efficient, stable, and aligned with human
preferences while minimizing the impact of inherent challenges in RL
systems. 

.. figure:: images/RLHF_training_tricks.png
   :alt: RLHF_training_tricks
   :align: center

   RLHF Training Tricks (Source: `Secrets of RLHF in Large Language Models Part I 
   PPO <https://arxiv.org/abs/2307.04964>`__)

DPO
---

The main reason why RLHF with PPO is hard is that it takes a lot of
redundant effort. Policy Model is all we need, all other efforts are not
necessary. **DPO (Direct Preference Optimization)** is a novel
alternative to traditional RLHF for fine-tuning LLMs. It simplifies the
RLHF process by eliminating the need for complex reward models and RL
algorithms. Instead, DPO reframes the problem of aligning LLMs with
human preferences as a classification problem using human-labeled
preference data. [RafaelRafailov]_

The main idea is DPO and difference between DPO and PPO are shown in the
figure below 

.. figure:: images/DPO_idea.png
   :alt: DPO_idea
   :align: center

   DPO Idea in the Paper (Source: `Direct Preference Optimization Your Language Model is Secretly a Reward Model <https://arxiv.org/abs/2305.18290>`__)

DPO Objective
~~~~~~~~~~~~~

**RLHF objective** is defined as follows. Keep in mind that no matter
whether DPO or PPO is used, the objective is always like this.

.. math::


   \max_{\pi_\theta} E_{x \sim D, y \sim \pi_\theta(y|x)}\Big[r_{\phi}(x,y) - \beta D_{KL}\big[\pi_\theta(y|x) || \pi_{ref}(y|x)\big]\Big]

where :math:`\beta D_{KL}\big[\pi_\theta(y|x) || \pi_{ref}(y|x)\big]` is
a regularization term. When applying RL to NLP, regularization is often
needed. Otherwise RL would explore every possible situation and find out
hidden tricks which deviate from a language model.

**DPO’s objective function** is derived by incoroprating the probability
of preference from reward function of optimal policy. DPO paper has
provided detailed steps of deriving the gradient of the DPO objective:[RafaelRafailov]_

.. math::


   L_{DPO}(\pi_\theta; \pi_{ref}) = -E_{(x,y_w,y_l) \sim D} \Big[\log \sigma \Big(\beta \log {\pi_{\theta}(y_w|x)\over \pi_{ref}(y_w|x)} - \beta \log {\pi_{\theta}(y_l|x)\over \pi_{ref}(y_l|x)}\Big)\Big)\Big]

**Key ideas of DPO objective**:

- DPO’s objective aims to increase the likelihood of generating
  preferred responses over less preferred ones. By focusing directly on
  preference data, DPO eliminates the need to first fit a reward model
  that predicts scalar rewards based on human preferences. This
  simplifies the training pipeline and reduces computational overhead.
- Value functions exist to help reduce the variance of the reward model.
  In DPO, the value function is not involved because DPO does not rely
  on a traditional RL framework, such as Actor-Critic methods. Instead,
  DPO directly optimizes the policy using human preference data as a
  **classification task**, skipping the intermediate steps of training a
  reward model or estimating value functions.
- DPO was originally designed to work with **pairwise** preference data,
  however, recent advancements and adaptations have extended its
  applicability to ranking preference data as well (e.g RankDPO).

.. code:: python

   import torch.nn.functional as F

   def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):
       """
       pi_logps: policy logprobs, shape (B,)
       ref_logps: reference model logprobs, shape (B,)
       yw_idxs: preferred completion indices in [0, B-1], shape (T,)
       yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)
       beta: temperature controlling strength of KL penalty

       Each pair of (yw_idxs[i], yl_idxs[i]) represents the
       indices of a single preference pair.
       """

       pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]
       ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]

       pi_logratios = pi_yw_logps - pi_yl_logps
       ref_logratios = ref_yw_logps - ref_yl_logps

       losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))
       rewards = beta * (pi_logps - ref_logps).detach()

       return losses, rewards

Steps of RLHF Using DPO
~~~~~~~~~~~~~~~~~~~~~~~

**1. Initial Setup and Supervised Fine-Tuning (SFT)**: Begin by
fine-tuning a pre-trained LLM using supervised learning on a dataset
that is representative of the tasks the model will perform. This step
ensures the model has a strong foundation in the relevant domain,
preparing it for preference-based optimization.

**2. Collect Preference Data**: Gather human feedback in the form of
pairwise preferences or rankings. Annotators evaluate responses
generated by the model and indicate which ones they prefer. Construct a
dataset of prompts and corresponding preferred and less-preferred
responses.

**3. Iterative Rounds of DPO**

- **Sampling and Annotation**: In each round, sample a set of responses
  from the model for given prompts. Collect new preference annotations
  based on these samples, allowing for dynamic updates to the preference
  dataset. (Public preference data works as well. Off-policy and
  on-policy data both work).

- **Preference Optimization**: Use DPO to adjust the model’s outputs
  based on collected preference data:

- **Model Update**: Fine-tune the model using this loss function to
  increase the likelihood of generating preferred responses.

**4. Evaluation and Iteration**

- **Performance Assessment**: After each round, evaluate the model’s
  performance on new prompts to ensure it aligns with human preferences.
  Use feedback from these evaluations to inform subsequent rounds of
  sampling and optimization.

- **Iterative Refinement**: Continue this loop process over multiple
  rounds, iteratively refining the model’s alignment with human
  preferences through continuous sampling and preference optimization.

DPO Variants
~~~~~~~~~~~~

The key area of research involves developing variants of DPO and
conducting theoretical analyses to understand its limitations and
potential improvements. This includes exploring different loss functions
or optimization strategies that can be applied within the DPO framework.

- One significant area of research focuses on refining the loss function
  used in DPO. This includes exploring ways to eliminate the need for a
  reference model, which can simplify the optimization process.

  Examples:

  - `ORPO: Monolithic Preference Optimization without Reference
    Model <https://arxiv.org/abs/2403.07691>`__

  - `SimPO: Simple Preference Optimization with a Reference-Free
    Reward <https://arxiv.org/abs/2405.14734>`__

- Another key direction involves leveraging existing supervised
  fine-tuning data as preference data for DPO. This strategy aims to
  enhance the quality of preference data by utilizing high-quality
  labeled datasets that may already exist from previous SFT processes.

  Examples:

  - `Refined Direct Preference Optimization with Synthetic Data for
    Behavioral Alignment of LLMs <https://arxiv.org/abs/2402.08005v1>`__

Main Difficulties in RLHF
-------------------------

**Data Collection**
~~~~~~~~~~~~~~~~~~~

In practice, people noticed that the collection of human feedback in the
form of the preference dataset is a slow manual process that needs to be
repeated whenever alignment criteria change. And there is increasing
difficulty in annotating preference data as models become more advanced,
particularly because distinguishing between outputs becomes more nuanced
and subjective.

- The paper “`CDR: Customizable Density Ratios of Strong-over-weak LLMs
  for Preference Annotation <https://arxiv.org/abs/2411.02481>`__”
  explains that as models become more advanced, it becomes harder to
  identify which output is better due to subtle differences in quality.
  This makes preference data annotation increasingly difficult and
  subjective.
- Another paper, “`Improving Context-Aware Preference Modeling for
  Language Models <https://arxiv.org/abs/2407.14916>`__,” discusses how
  the underspecified nature of natural language and multidimensional
  criteria make direct preference feedback difficult to interpret. This
  highlights the challenge of providing consistent annotations when
  outputs are highly sophisticated and nuanced.
- “`Less for More: Enhancing Preference Learning in Generative Language
  Models <https://www.arxiv.org/abs/2408.12799>`__” also notes that
  ambiguity among annotators leads to inconsistently annotated datasets,
  which becomes a greater issue as model outputs grow more complex.

**Reward Hacking**
~~~~~~~~~~~~~~~~~~

Reward hacking is a common problem in reinforcement learning, where the
agent learns to exploit the system by maximizing its reward through
actions that deviate from the intended goal. In the context of RLHF,
reward hacking occurs when training settles in an unintended region of
the loss landscape. In this scenario, the model generates responses that
achieve high reward scores, but these responses may fail to be
meaningful or useful to the user.

In PPO, reward hacking occurs when the model exploits flaws or
ambiguities in the **reward model** to achieve high rewards without
genuinely aligning with human intentions. This is because PPO relies on
a learned reward model to guide policy updates, and any inaccuracies or
biases in this model can lead to unintended behaviors being rewarded.
PPO is particularly vulnerable to reward hacking if the reward model is
not robustly designed or if it fails to capture the true objectives of
human feedback. The iterative nature of PPO, which involves continuous
policy updates based on reward signals, can exacerbate this issue if not
carefully managed.

DPO avoids explicit reward modeling by directly optimizing policy based
on preference data. However, it can still encounter issues similar to
reward hacking if the preference data is **biased** or if the
optimization process leads to **overfitting** specific patterns in the
data that do not generalize well. While DPO does not suffer from reward
hacking in the traditional sense (since it lacks a separate reward
model), it can still find biased solutions that exploit
**out-of-distribution responses** or deviate from intended behavior due
to distribution shifts between training and deployment contexts.

- The article “`Reward Hacking in Reinforcement
  Learning <https://lilianweng.github.io/posts/2024-11-28-reward-hacking/>`__”
  by Lilian Weng discusses how reward hacking occurs when a RL agent
  exploits flaws or ambiguities in the reward function to achieve high
  rewards without genuinely learning the intended task. It highlights
  that in RLHF for language models, reward hacking is a critical
  challenge, as models might learn to exploit unit tests or mimic biases
  to achieve high rewards, which can hinder real-world deployment.
- The research “`Scaling Laws for Reward Model
  Overoptimization <https://arxiv.org/abs/2210.10760>`__” explores how
  optimizing against reward models trained to predict human preferences
  can lead to overoptimization, hindering the actual objective.

  1. **Impact of Policy Model Size**: Holding the RM size constant,
     experiments showed that larger policy models exhibited similar
     overoptimization trends as smaller models, despite achieving higher
     initial gold scores. This implies that their higher performance on
     gold rewards does not lead to excessive optimization pressure on
     the RM.
  2. **Relationship with RM Data Size**: Data size had a notable effect
     on RM performance and overoptimization. Models trained on fewer
     than ~2,000 comparison labels showed near-chance performance, with
     limited improvement in gold scores. Beyond this threshold, all RMs,
     regardless of size, benefited from increased data, with larger RMs
     showing greater improvements in gold rewards compared to smaller
     ones.
  3. **Scaling Laws for RM Parameters and Data Size**: Overoptimization
     patterns scaled smoothly with both RM parameter count and data
     size. Larger RMs demonstrated better alignment with gold rewards
     and less susceptibility to overoptimization when trained on
     sufficient data, indicating improved robustness.
  4. **Proxy vs. Gold Reward Trends**: For small data sizes, proxy
     reward scores deviated significantly from gold reward scores,
     highlighting overoptimization risks. As data size increased, the
     gap between proxy and gold rewards narrowed, reducing
     overoptimization effects.

Note that the KL divergence term in the RLHF objective is intended to
prevent the policy from deviating too much from a reference model,
thereby maintaining stability during training. However, it does not
fully prevent reward hacking. Reward hacking occurs when an agent
exploits flaws or ambiguities in the reward model to achieve high
rewards without genuinely aligning with human intentions. The KL
divergence penalty does not correct these flaws in the reward model
itself, meaning that if the reward model is misaligned, the agent can
still find ways to exploit it. KL does not directly address whether the
actions align with the true objectives or desired outcomes.
