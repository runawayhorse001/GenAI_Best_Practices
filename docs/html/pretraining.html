

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>7. Pre-training &mdash; GenAI: Best Practices 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />

  
    <link rel="shortcut icon" href="_static/icon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="8. LLM Evaluation Metrics" href="evaluation.html" />
    <link rel="prev" title="6. Fine Tuning" href="finetuning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            GenAI: Best Practices
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preface.html">1. Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="prelim.html">2. Preliminary</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedding.html">3. Word and Sentence Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompt.html">4. Prompt Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="rag.html">5. Retrieval-Augmented Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="finetuning.html">6. Fine Tuning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">7. Pre-training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#transformer-architecture">7.1. Transformer Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#attention-is-all-you-need">7.1.1. Attention Is All You Need</a></li>
<li class="toctree-l3"><a class="reference internal" href="#encoder-decoder">7.1.2. Encoder-Decoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="#positional-encoding">7.1.3. Positional Encoding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sinusoidal-positional-encodings">7.1.3.1. Sinusoidal Positional Encodings</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rotary-positional-embeddings-rope">7.1.3.2. Rotary Positional Embeddings (RoPE)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#learnable-positional-encodings">7.1.3.3. <strong>Learnable Positional Encodings</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#summary">7.1.3.4. Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#embedding-matrix">7.1.4. Embedding Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attention-mechanism">7.1.5. Attention Mechanism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#self-attention">7.1.5.1. Self-Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cross-attention">7.1.5.2. Cross Attention</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#layer-normalization">7.1.6. Layer Normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#residual-connections">7.1.7. Residual Connections</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feed-forward-networks">7.1.8. Feed-Forward Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#label-smoothing">7.1.9. Label Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmax-and-temperature">7.1.10. Softmax and Temperature</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unembedding-matrix">7.1.11. Unembedding Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decoding">7.1.12. Decoding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#greedy-decoding">7.1.12.1. Greedy Decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="#beam-search">7.1.12.2. Beam Search</a></li>
<li class="toctree-l4"><a class="reference internal" href="#top-k-sampling">7.1.12.3. Top-k Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#top-p-nucleus-sampling">7.1.12.4. Top-p (Nucleus) Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#temperature-scaling">7.1.12.5. Temperature Scaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#summary-1">7.1.12.6. Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#modern-transformer-techniques">7.2. Modern Transformer Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#kv-cache">7.2.1. KV Cache</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-query-attention">7.2.2. Multi-Query Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="#grouped-query-attention">7.2.3. Grouped-Query Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flash-attention">7.2.4. Flash Attention</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">8. LLM Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="guardrails.html">9. LLM Guardrails</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">10. Main Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GenAI: Best Practices</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><span class="section-number">7. </span>Pre-training</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pre-training">
<span id="pretraining"></span><h1><span class="section-number">7. </span>Pre-training<a class="headerlink" href="#pre-training" title="Link to this heading"></a></h1>
<div class="admonition-ilya-sutskever-at-neurips-2024 admonition">
<p class="admonition-title">Ilya Sutskever at Neurips 2024</p>
<figure class="align-center" id="fig-ilya">
<img alt="_images/ilya_sutskever.png" src="_images/ilya_sutskever.png" />
</figure>
</div>
<p>In industry, most companies focus primarily on prompt engineering, RAG, and fine-tuning,
while advanced techniques like pre-training from scratch or deep model customization
remain less common due to the significant resources and expertise required.</p>
<p>LLMs, like GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder
Representations from Transformers), and others, are large-scale models built using
the transformer architecture. These models are trained on vast amounts of text data to
learn patterns in language, enabling them to generate human-like text, answer questions,
summarize information, and perform other natural language processing tasks.</p>
<p>This chapter delves into transformer models, drawing on insights from
<a class="reference external" href="https://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a> and <a class="reference external" href="https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c">Tracing the Transformer in Diagrams</a>, to explore their
underlying architecture and practical applications.</p>
<section id="transformer-architecture">
<h2><span class="section-number">7.1. </span>Transformer Architecture<a class="headerlink" href="#transformer-architecture" title="Link to this heading"></a></h2>
<figure class="align-center" id="id8">
<img alt="transformer_architecture" src="_images/transformer_architecture.png" />
<figcaption>
<p><span class="caption-text">Transformer Architecture (source: <a class="reference internal" href="reference.html#attentionallyouneed" id="id1"><span>[attentionAllYouNeed]</span></a>)</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="attention-is-all-you-need">
<h3><span class="section-number">7.1.1. </span>Attention Is All You Need<a class="headerlink" href="#attention-is-all-you-need" title="Link to this heading"></a></h3>
<p>The Transformer is a deep learning model designed to handle sequential
data, such as text, by relying entirely on attention mechanisms rather
than recurrence or convolution. It consists of an <strong>encoder-decoder
structure</strong>, where the <strong>encoder</strong> transforms an input sequence into a
set of rich contextual representations, and the <strong>decoder</strong> generates
the output sequence by attending to these representations and previously
generated tokens. Both encoder and decoder are composed of stacked
layers, each featuring <strong>multi-head self-attention</strong> (to capture
relationships between tokens), <strong>feedforward neural networks</strong> (for
non-linear transformations), and <strong>residual connections with layer
normalization</strong> (to improve training stability). Positional encodings
are added to token embeddings to retain sequence order information, and
the architecture’s parallelism and scalability make it highly efficient
for tasks like machine translation, summarization, and language
modeling.</p>
<p>When the Transformer architecture was introduced in the paper
<em>“Attention Is All You Need”</em> (Vaswani et al., 2017), the primary task
it aimed to address was <strong>machine translation</strong>. The researchers wanted
to develop a model that could translate text from one language to
another more efficiently and effectively than the existing
sequence-to-sequence (Seq2Seq) models, which relied heavily on recurrent
neural networks (RNNs) or long short-term memory (LSTM) networks. RNNs /
LSTMs suffer from slow training and inference, short-term memory, and
vanishing / exploding gradients challenges, due to their sequentual
nature and long-range dependencies. The Transformer with self-attention
mechanism achieved to eliminate the sequential bottleneck of RNNs while
retaining the ability to capture dependencies across the entire input
sequence.</p>
</section>
<section id="encoder-decoder">
<h3><span class="section-number">7.1.2. </span>Encoder-Decoder<a class="headerlink" href="#encoder-decoder" title="Link to this heading"></a></h3>
<p>Transformer has an encoding component, a decoding component, and
connections between them. The encoding component is a stack of encoders
- usually 6-12 layers, though it can go higher (e.g. T5-large has 24
encoder layers). The decoder component is a stack of decoders, usually
in the same number of layers for balance.</p>
<ul class="simple">
<li><p>Each <strong>encoder</strong> layer includes multi-head self-attention, feedforward
neural network (FNN), add &amp; norm, and positional encoding. It reads
the input sequence (e.g., a sentence in Chinese) and produces a
context-aware representation.</p></li>
<li><p>Each <strong>decoder</strong> layer includes masked multi-head self-attention,
encoder-decoder attention, feedforward neural network (FFN), add &amp;
norm, and positional encoding. It generates the output sequence (e.g.,
a translation in English) using the encoder’s output and previously
generated tokens.</p></li>
</ul>
<p>The encoder-decoder structure was inspired by earlier Seq2Seq models
(Sutskever et al., 2014), which used separate RNNs or LSTMs for encoding
the input sequence and decoding the output sequence. The innovation of
the Transformer was replacing the recurrent nature of those models with
an attention-based approach. The Transformer revolutionized not just
machine translation but also the entire field of natural language
processing (NLP). Its encoder-decoder structure provided a blueprint for
subsequent models:</p>
<ul class="simple">
<li><p><strong>Encoder-only models</strong> (e.g., BERT, RoBERTa, DistilBERT) for
understanding tasks such as classification, sentiment analysis, named
entity recognition, and question answering.</p>
<ul>
<li><p>Unlike encoder-decoder or decoder-only models, encoder-only models
don’t generate new sequences. Its architecture and training
objectives are optimized for extracting contextual representations
from input sequences. They focus solely on understanding and
representing the input.</p></li>
<li><p>Encoder-only models typically use <strong>bidirectional self-attention</strong>,
meaning each token can attend to all other tokens in the sequence
(both before and after it). This contrasts with decoder-only models,
which use causal masking and can only attend to past tokens.
Bidirectionality provides a more holistic understanding of the
input.</p></li>
<li><p>Encoder-only models are often pretrained with tasks like <strong>masked
language modeling (MLM)</strong>, where random tokens in the input are
masked and the model learns to predict them based on context.</p></li>
</ul>
</li>
<li><p><strong>Decoder-only models</strong> (e.g., GPT series, Transformer-XL) for text
generation tasks.</p>
<ul>
<li><p>Decoder-only models are trained with an <strong>autoregressive
objective</strong>, meaning they predict the next token in a sequence based
on the tokens seen so far. This makes them inherently suited for
producing coherent, contextually relevant continuations.</p></li>
<li><p>The self-attention mechanism in decoder-only models is <strong>causal</strong>,
meaning each token attends only to previous tokens (including
itself). They are pretrained with <strong>causal language modeling
(CLM)</strong>, where they learn to predict the next token given the
previous ones.</p></li>
<li><p>Decoder-only models are not constrained to fixed-length outputs and
can generate sequences of arbitrary lengths, making them ideal for
open-ended tasks such as story writing, dialogue generation, and
summarization.</p></li>
</ul>
</li>
<li><p><strong>Encoder-decoder models</strong> (e.g., Original Transformer, BART, T5) for
sequence-to-sequence tasks such as machine translation, summarization,
and text generation.</p>
<ul>
<li><p>Encoder and decoder are designed to handle different parts of the
task - creating a contextual representation and generating output
sequence. This decoupling of encoding and decoding allows the model
to flexibly handle inputs and outputs of different lengths.</p></li>
<li><p>The <strong>encoder-decoder attention mechanism</strong> in the decoder allows
the model to focus on specific parts of the encoded input sequence
while generating the output sequence. This <strong>cross-attention</strong>
mechanism helps maintain the relationship between the input and
output sequences.</p></li>
<li><p>In many encoder-decoder models (such as those based on
Transformers), the encoder processes the input sequence
<strong>bidirectionally</strong>, meaning it can attend to both preceding and
succeeding tokens when creating the representations. This ensures a
comprehensive understanding of the input sequence before it is
passed to the decoder.</p></li>
<li><p>During training, the encoder-decoder model is typically provided
with a sequence of <strong>input-output pairs</strong> (e.g., a Chinese sentence
and its English translation). This paired structure makes the model
highly suited for tasks like translation, where the goal is to map
input sequences in one language to corresponding output sequences in
another language.</p></li>
</ul>
</li>
</ul>
</section>
<section id="positional-encoding">
<h3><span class="section-number">7.1.3. </span>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading"></a></h3>
<p><strong>Positional encoding</strong> is a mechanism used in transformers to provide
information about the order of tokens in a sequence. Unlike recurrent
neural networks (RNNs), transformers process all tokens in parallel, and
therefore lack a built-in way to capture sequential information.
Positional encoding solves this by injecting position-dependent
information into the input embeddings.</p>
<section id="sinusoidal-positional-encodings">
<h4><span class="section-number">7.1.3.1. </span>Sinusoidal Positional Encodings<a class="headerlink" href="#sinusoidal-positional-encodings" title="Link to this heading"></a></h4>
<p>Sinusoidal positional encoding adds a vector to the embedding of each
token, with the vector values derived using <strong>sinusoidal functions</strong>.
For a token at position <span class="math notranslate nohighlight">\(pos\)</span> in the sequence and a specific
dimension <span class="math notranslate nohighlight">\(i\)</span> of the embedding:</p>
<div class="math notranslate nohighlight">
\[\begin{split}PE(pos,2i) = \sin\Big({pos\over 10000^{2i/d}}\Big)\\
PE(pos,2i+1) = \cos\Big({pos\over 10000^{2i/d}}\Big)\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(pos\)</span>: Position of the token in the sequence.</p></li>
<li><p><span class="math notranslate nohighlight">\(i\)</span>: Index of the embedding dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(d\)</span>: Total dimension of the embedding vector.</p></li>
</ul>
<p>The positional encodings are added directly to the token embeddings:</p>
<div class="math notranslate nohighlight">
\[\text{Input to Transformer} = \text{Token Embedding} + \text{Positional Encoding}\]</div>
<figure class="align-center" id="id9">
<img alt="position_embedding" src="_images/position_embedding.png" />
<figcaption>
<p><span class="caption-text">Positional Embedding</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="rotary-positional-embeddings-rope">
<h4><span class="section-number">7.1.3.2. </span>Rotary Positional Embeddings (RoPE)<a class="headerlink" href="#rotary-positional-embeddings-rope" title="Link to this heading"></a></h4>
<p>Rotary positional embedding is a modern variant that introduces
positional information through rotation in a complex vector space. It
encodes positional information by rotating the query and key vectors in
the attention mechanism using a transformation in a complex vector
space. RoPE mitigates the limitations of absolute positional encodings
by focusing on relative relationships, enabling smooth transitions and
better handling of long sequences. This makes it particularly
advantageous in large-scale language models like GPT-4, LLaMA, where
long-range dependencies and adaptability are crucial.</p>
<p>Given a token vector <span class="math notranslate nohighlight">\(x\)</span> with positional encoding, RoPE applies a
rotation:</p>
<div class="math notranslate nohighlight">
\[\text{RoPE} = R(pos)\cdot x\]</div>
<p>where <span class="math notranslate nohighlight">\(R(pos)\)</span> is the rotation matrix determined by the token’s
position.</p>
<p>Specifically, for a rotation by an angle <span class="math notranslate nohighlight">\(\theta\)</span>, the 2D rotation
matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}R(\theta) = \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta)\end{bmatrix}\end{split}\]</div>
<p>For each pair of dimensions <span class="math notranslate nohighlight">\((x_{even}, x_{odd})\)</span>, the rotation is
performed as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}x'_{even} \\x'_{odd} \end{bmatrix} = \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta)\end{bmatrix} \cdot \begin{bmatrix}x_{even} \\x_{odd} \end{bmatrix}\end{split}\]</div>
</section>
<section id="learnable-positional-encodings">
<h4><span class="section-number">7.1.3.3. </span><strong>Learnable Positional Encodings</strong><a class="headerlink" href="#learnable-positional-encodings" title="Link to this heading"></a></h4>
<p>Learnable Positional Encodings are a type of positional encoding used in
transformer-based models where the positional information is not fixed
(like in <strong>sinusoidal</strong> encoding) but is <strong>learned during training</strong>.
These encodings are treated as trainable parameters and are updated
through backpropagation, just like other parameters in the model.</p>
</section>
<section id="summary">
<h4><span class="section-number">7.1.3.4. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Sinusoidal
Positional Encoding</p></th>
<th class="head"><p>Rotary Positional
Embeddings (RoPE)</p></th>
<th class="head"><p>Learnable
Positional
Encodings</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Type</p></td>
<td><p>Absolute</p></td>
<td><p>Relative</p></td>
<td><p>Absolute</p></td>
</tr>
<tr class="row-odd"><td><p>Learnable</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p>Advantages</p></td>
<td><p>Fixed, no trainable
parameters;
Generalizes to
unseen sequence
lengths;
Computationally
simple.</p></td>
<td><p>Encodes relative
positional
relationships;
Scales efficiently
to long sequences;
Smooth handling of
long-range
dependencies.</p></td>
<td><p>Flexible for
task-specific
adaptation;
Optimized during
training.</p></td>
</tr>
<tr class="row-odd"><td><p>Disadvantages</p></td>
<td><p>Fixed, cannot adapt
to data; Encodes
only absolute
positions; Less
flexible for
relative tasks.</p></td>
<td><p>More complex to
implement;
Relatively new,
less widespread for
general tasks.</p></td>
<td><p>Limited to a fixed
maximum sequence
length; No inherent
relative
positioning;
Requires more
parameters.</p></td>
</tr>
<tr class="row-even"><td><p>Usage</p></td>
<td><p>Early models (e.g.,
original
Transformer);
S
equence-to-sequence
tasks like
translation.</p></td>
<td><p>Modern LLMs (e.g.,
GPT-4, LLaMA) with
long context
lengths; Tasks
requiring
long-range
dependencies.</p></td>
<td><p>Popular in earlier
models like GPT-2,
BERT; Tasks with
shorter sequences.</p></td>
</tr>
<tr class="row-odd"><td><p>Best For</p></td>
<td><p>Simplicity,
generalization to
unseen data.</p></td>
<td><p>Long-context tasks,
relative
dependencies,
efficient scaling.</p></td>
<td><p>Task-specific
optimization,
shorter context
tasks.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="embedding-matrix">
<h3><span class="section-number">7.1.4. </span>Embedding Matrix<a class="headerlink" href="#embedding-matrix" title="Link to this heading"></a></h3>
<p><strong>Embedding</strong> refers to the process of converting <strong>discrete tokens
(words, subwords, or characters)</strong> into <strong>continuous vector
representations</strong> in a high-dimensional space. These vectors capture the
semantic and syntactic properties of tokens, allowing the model to
process and understand language more effectively. Embedding layer is a
necessary component because:</p>
<ul class="simple">
<li><p>Discrete symbols are not directly understandable by the model.
Embeddings transform these discrete tokens into continuous vectors.
Neural networks process continuous numbers more effectively than
discrete symbols.</p></li>
<li><p>Embeddings help the model learn relationships between words. By
learning the <strong>semantic properties</strong> of tokens during training, words
with similar meanings (e.g. “king” and “queen”) should have similar
vector representations.</p></li>
<li><p>In Transformer based models, embeddings are not just static
representations but can be adjusted as the model learns from the
context of a sentence to capture subtle semantic nuances and
dependencies between words.</p></li>
</ul>
<figure class="align-center" id="id10">
<img alt="word_embedding-modified" src="_images/word_embedding_matrix.png" />
<figcaption>
<p><span class="caption-text">Word Embedding</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Take an example of embedding matrix <span class="math notranslate nohighlight">\(W_E\)</span> with ~50k vocabulary
size, each token in the vocabulary has a corresponding vector, typically
initialized <strong>randomly</strong> at the beginning of training. Embedding matrix
does not only represent individual words. They also encode the
information about the position of the word. And through training process
(passing through self-attention and multiple layers), these embeddings
are transformed into <strong>contextual embeddings</strong>, encoding not only the
individual word but also its relationship to other words in the
sequence.</p>
<p>The reason why a model predicting the next word requires efficient
context incorporation, is that the meaning of a word is clearly informed
by its surroundings, sometimes this includes context from a long
distance away. For example, with contextual embeddings, the dot products
of pieces of this sentence “<em>Harry Potter attends Hogwarts School of
Witchcraft and Wizardry, retrieves the Philosopher’s Stone, battles a
basilisk, and ultimately leads a final battle at Hogwarts, defeating
Voldemort and bringing peace to the wizarding world</em>” results in the
following projections in embedding space:</p>
<figure class="align-center" id="id11">
<img alt="contextual_embedding" src="_images/contextual_embedding.png" />
<figcaption>
<p><span class="caption-text">Contextual Embedding</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Embedding matrix contains vectors of all words in the vocabulary. It’s
the first pile of weights in our model. If the vocabulary size is
<span class="math notranslate nohighlight">\(V\)</span> and the embedding dimension is <span class="math notranslate nohighlight">\(d\)</span>, the embedding matrix
<span class="math notranslate nohighlight">\(W_E\)</span> has dimensions <span class="math notranslate nohighlight">\(d \times V\)</span>. The total number of
parameters in this embedding matrix is calculated by <span class="math notranslate nohighlight">\(d \times V\)</span>.</p>
</section>
<section id="attention-mechanism">
<h3><span class="section-number">7.1.5. </span>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Link to this heading"></a></h3>
<figure class="align-center" id="id12">
<img alt="self_attention_hendrik" src="_images/self_attention_hendrik.png" />
<figcaption>
<p><span class="caption-text">Self Attention (source: <a class="reference external" href="https://www.hendrik-erz.de/post/the-transformer-architecture-a-visual-guide-pdf-download">The Transformer Architecture A Visual Guide</a>)</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="self-attention">
<h4><span class="section-number">7.1.5.1. </span>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading"></a></h4>
<p>A <strong>self-attention</strong> is called single-head attention, which enables the
model to effectively capture relationships and dependencies between
different tokens within the same input sequence. Multi-headed attention
has multiple self-attentions running in parallel. The goal of
self-attention is to produce a refined embedding where each word has
ingested contextual meanings from other words by a series of
computations. For example, in the input of “The brave wizard cast a
powerful spell”, the refined embedding E3’ of ‘wizard’ should contain
the meaning of ‘brave’, and the refined embedding E7’ of ‘spell’ should
contain the meaning of ‘powerful’.</p>
<figure class="align-center">
<img alt="selfattention_goal" src="_images/selfattention_goal.png" />
</figure>
<p>The computation involved in self-attention in transformers consists of
several key steps: generating query, key, and value representations,
calculating attention scores, applying softmax, and computing a weighted
sum of the values.</p>
<ol class="arabic">
<li><p><strong>Linear Projection to Query space</strong></p>
<p>Given an input represention with dimension of <span class="math notranslate nohighlight">\((d \times N)\)</span>
where <span class="math notranslate nohighlight">\(d\)</span> is the embedding dimension and <span class="math notranslate nohighlight">\(N\)</span> is the token
number. Query matrix <span class="math notranslate nohighlight">\(W_Q\)</span> with dimension of
<span class="math notranslate nohighlight">\((N \times d_q)\)</span> (<span class="math notranslate nohighlight">\(d_q\)</span> is usually small e.g. 128)
contains learnable parameters. It is used to project input
representation <span class="math notranslate nohighlight">\(W_E\)</span> to the smaller query space <span class="math notranslate nohighlight">\(Q\)</span> by
matrix multiplication.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Q &amp;= W_E W_Q\\
(N\times d)(d\times d_q) &amp;\rightarrow (N \times d_q)\end{split}\]</div>
<p>Conceptually, the query matrix aims to ask each word a question
regarding what kinds of relationship it has with each of the other
words.</p>
<figure class="align-center" id="id13">
<img alt="query_projection" src="_images/query_projection.png" />
<figcaption>
<p><span class="caption-text">Query Projection</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
<li><p><strong>Linear Projection to Key space</strong></p>
<p>Key matrix <span class="math notranslate nohighlight">\(W_k\)</span> with dimension of <span class="math notranslate nohighlight">\((N \times d_k)\)</span>
contains learnable parameters. It is used to project input
representation <span class="math notranslate nohighlight">\(W_E\)</span> to the smaller key space <span class="math notranslate nohighlight">\(K\)</span> by
matrix multiplication.</p>
<div class="math notranslate nohighlight">
\[\begin{split}K &amp;= W_E W_K \\
(N \times d) (d \times d_k) &amp;\rightarrow (N \times d_k)\end{split}\]</div>
<p>Conceptually, the keys are answering the queries by matching the
queries whenever they closely align with each other. In our example
of “The brave wizard cast a powerful spell”, the key metrix maps the
word ‘brave’ to vectors that are closely aligned with the query
produced by the word ‘wizard’.</p>
<figure class="align-center" id="id14">
<img alt="key_projection" src="_images/key_projection.png" />
<figcaption>
<p><span class="caption-text">Key Projection</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
<li><p><strong>Compute Attention Scores</strong></p>
<p>Attention scores are calculated by taking the <strong>dot product</strong> of the
query vectors with the key vectors. These scores as a measurement of
relationship represent how well each key matches each query. They can
be values from negative infinity to positive infinity.</p>
<div class="math notranslate nohighlight">
\[\text{Attention Score} = QK^T\]</div>
<p>In our example, the attention score produced by <span class="math notranslate nohighlight">\(K_2 \cdot Q_3\)</span>
is expected to be a large positive value because ‘brave’ is an
adjective to ‘wizard’. In other words, the embedding of ‘brave’
<strong>attends to</strong> the embedding of ‘wizard’.</p>
<figure class="align-center" id="id15">
<img alt="attention_score" src="_images/attention_score.png" />
<figcaption>
<p><span class="caption-text">Attention Score</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
<li><p><strong>Scaling and softmax normalization</strong></p>
<p>To prevent large values in the attention scores (which could lead to
very small gradients), the scores are often scaled by the square root
of the dimension of the key vectors <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>. This scaling
helps stabilize the softmax function used in the next step.</p>
<div class="math notranslate nohighlight">
\[\text{Scaled Attention Score} = {QK^T \over \sqrt{d_k}}\]</div>
<p>The attention scores are passed through a <strong>softmax</strong> function, which
normalizes them into a probability distribution. This ensures that
each column of the attention matrix sums to 1, so each token has a
clear distribution of “attention” over all tokens.</p>
<div class="math notranslate nohighlight">
\[\text{Attention Weights} = \text{softmax}\Big({QK^T\over{\sqrt{d_k}}}\Big)\]</div>
<p>Note that for a <strong>masked</strong> self attention, the bottom left triangle
of attention scores are set to negative infinity before softmax
normalization. The purpose is to mask those information as latter
words are not allowed to influence earlier words. After softmax
normalization, those masked attention information becomes zero and
the columns stay normalized. This process is called <strong>masking</strong>.</p>
</li>
<li><p><strong>Computing weighted sum of values</strong></p>
<p>In the attention score matrix with dimension of <span class="math notranslate nohighlight">\(N \times N\)</span>,
each column is giving weights according to how relevant the word in
key space (on the left in the figure) is to the correpsonding word in
query space (on the top in the figure). This matrix is also called
<strong>attention pattern</strong>.</p>
<p>The size of attention pattern is the square of the context size,
therefore, context size is a huge bottleneck for LLMs. Recent years,
some variations of attention mechanism are developed such as Sparse
Attention Mechanism, Blockwise Attention, Linformer, Reformer,
Longformer, etc, aiming to make context more scalable.</p>
</li>
<li><p><strong>Linear Projection to Value space</strong></p>
<p>Value matrix <span class="math notranslate nohighlight">\(W_v\)</span> with dimension of <span class="math notranslate nohighlight">\((N \times d_v)\)</span>
contains learnable parameters. It is used to project input
representation <span class="math notranslate nohighlight">\(W_E\)</span> to the smaller value space <span class="math notranslate nohighlight">\(V\)</span> by
matrix multiplication.</p>
<div class="math notranslate nohighlight">
\[\begin{split}V &amp;= W_E W_V \\
(N \times d) (d \times d_v) &amp;\rightarrow (N \times d_v)\end{split}\]</div>
<p>Conceptually, by maping the embedding of a word to the value space,
it’s trying to figure out what should be added to the embedding of
other words, if this word is relevant to adjusting the meaning of
other words.</p>
</li>
<li><p><strong>Compute Weighted Sum of Values</strong></p>
<p>Each token’s output is computed by taking a <strong>weighted sum</strong> of the
value vectors, where the weights come from the attention distribution
obtained in the previous step.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Output} &amp;= \text{Attention Weights} \times V\\
(N \times N) (N \times d_v) &amp;\rightarrow (N \times d_v)\end{split}\]</div>
<p>This results in a matrix of size <span class="math notranslate nohighlight">\(N \times d_v\)</span> where for each
word there is a weighted sum of the value vectors <span class="math notranslate nohighlight">\(\Delta E\)</span>
based on the attention distribution. Conceptually, this is the change
going to be added to the original embedding, resulting in a more
refined vector, encoding contextually rich meaning.</p>
<figure class="align-center" id="id16">
<img alt="value_projection_weighted_sum" src="_images/value_projection_weighted_sum.png" />
<figcaption>
<p><span class="caption-text">Value Projection and Weighted Sum</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
</li>
</ol>
<p>To sum up, given <span class="math notranslate nohighlight">\(W_E\)</span> input matrix (<span class="math notranslate nohighlight">\(N \times d\)</span>),
<span class="math notranslate nohighlight">\(W_Q, W_K, W_V\)</span> as weight matrices
(<span class="math notranslate nohighlight">\(d\times d_q, d\times d_k, d\times d_v\)</span>), the matrix form of the
full self-attention process can be written as:</p>
<div class="math notranslate nohighlight">
\[\text{Output} = \text{softmax}\Big({(W_EW_Q)(W_EW_K)^T \over \sqrt{d_k}}\Big) \times (W_EW_V)\]</div>
<p>where the final output matrix is <span class="math notranslate nohighlight">\(N \times d_v\)</span>.</p>
<p>A full attention block inside a transformer consists of <strong>multi-head
attention</strong>, where self-attention operations run in parallel, each with
its own distinct Key, Query, Value matrices.</p>
<p>To update embedding matrix, the weighted sum of values is passed through
a linear transformation (via <span class="math notranslate nohighlight">\(W_O\)</span>), and then added to the
original input embeddings via a residual connection.</p>
<div class="math notranslate nohighlight">
\[\text{Final output} = \text{Output} \times W_o\]</div>
<p>The number of parameters involved in Attention Mechanism:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p># Parameters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Embedding Matrix</p></td>
<td><p>d_embed * n_vocab</p></td>
</tr>
<tr class="row-odd"><td><p>Key Matrix</p></td>
<td><p>d_key * d_embed * n_heads * n_layers</p></td>
</tr>
<tr class="row-even"><td><p>Query Matrix</p></td>
<td><p>d_query * d_embed * n_heads * n_layers</p></td>
</tr>
<tr class="row-odd"><td><p>Value Matrix</p></td>
<td><p>d_value * d_embed * n_heads * n_layers</p></td>
</tr>
<tr class="row-even"><td><p>Output Matrix</p></td>
<td><p>d_embed * d_value * n_heads * n_layers</p></td>
</tr>
<tr class="row-odd"><td><p>Unembedding Matrix</p></td>
<td><p>n_vocab * d_embed</p></td>
</tr>
</tbody>
</table>
</section>
<section id="cross-attention">
<h4><span class="section-number">7.1.5.2. </span>Cross Attention<a class="headerlink" href="#cross-attention" title="Link to this heading"></a></h4>
<p><strong>Cross-attention</strong> is a mechanism in transformers where the queries
(<span class="math notranslate nohighlight">\(Q\)</span>) come from one sequence (e.g., the decoder), while the keys
(<span class="math notranslate nohighlight">\(K\)</span>) and values (<span class="math notranslate nohighlight">\(V\)</span>) come from another sequence (e.g., the
encoder). It allows the model to align and focus on relevant parts of a
second sequence when processing the current sequence.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Self-Attention</p></th>
<th class="head"><p>Cross-Attention</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Source
of
Queries</p></td>
<td><p>Queries (<span class="math notranslate nohighlight">\(Q\)</span>) come
from the same sequence.</p></td>
<td><p>Queries (<span class="math notranslate nohighlight">\(Q\)</span>) come
from one sequence (e.g.,
decoder).</p></td>
</tr>
<tr class="row-odd"><td><p>Source
of
Keys
/Values</p></td>
<td><p>Keys (<span class="math notranslate nohighlight">\(K\)</span>) and Values
(<span class="math notranslate nohighlight">\(V\)</span>) come from the
same sequence.</p></td>
<td><p>Keys (<span class="math notranslate nohighlight">\(K\)</span>) and Values
(<span class="math notranslate nohighlight">\(V\)</span>) come from a
different sequence (e.g.,
encoder).</p></td>
</tr>
<tr class="row-even"><td><p>Purpose</p></td>
<td><p>Captures relationships
within the same sequence.</p></td>
<td><p>Aligns and integrates
information between two
sequences.</p></td>
</tr>
<tr class="row-odd"><td><p>Example
Usage</p></td>
<td><p>Used in both encoder and
decoder to process input or
output tokens.</p></td>
<td><p>Used in encoder-decoder
models (e.g., translation)
to let the decoder focus on
encoder outputs.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="layer-normalization">
<h3><span class="section-number">7.1.6. </span>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading"></a></h3>
<p>Layer Normalization is crucial in transformers because it helps
stabilize and accelerate the training of deep neural networks by
normalizing the activations across the layers. The transformer
architecture, which consists of many layers and complex operations,
benefits significantly from this technique for several reasons:</p>
<ol class="arabic simple">
<li><p><strong>Internal Covariate Shift</strong>:</p>
<ul class="simple">
<li><p>Deep models like transformers often suffer from <strong>internal
covariate shift</strong>, where the distribution of activations changes
during training due to the update of model parameters. This can
make training slower and less stable.</p></li>
<li><p>Layer normalization helps mitigate this by ensuring that the output
of each layer has a consistent distribution, which leads to faster
convergence and more stable training.</p></li>
</ul>
</li>
<li><p><strong>Gradient Flow</strong>:</p>
<ul class="simple">
<li><p>In deep models, the gradients can become either very small
(vanishing gradient problem) or very large (exploding gradient
problem) as they propagate through the layers. Layer normalization
helps keep the gradients within a reasonable range, ensuring
<strong>efficient gradient flow</strong> and preventing these issues.</p></li>
</ul>
</li>
<li><p><strong>Improved Convergence</strong>:</p>
<ul class="simple">
<li><p>By normalizing the activations, layer normalization allows the
model to use <strong>larger learning rates</strong>, which speeds up training
and leads to better convergence.</p></li>
</ul>
</li>
<li><p><strong>Works Across Batch Sizes</strong>:</p>
<ul class="simple">
<li><p>Unlike <strong>Batch Normalization</strong>, which normalizes activations across
the batch dimension, <strong>Layer Normalization</strong> normalizes across the
feature dimension for each individual example, making it more
suitable for tasks like <strong>sequence modeling</strong>, where the batch size
may vary and the model deals with sequences of different lengths.</p></li>
</ul>
</li>
</ol>
<p>The process can be broken down into the following steps:</p>
<ol class="arabic">
<li><p>Compute the Mean and Variance: for a given input
<span class="math notranslate nohighlight">\(x = [x_1, ..., x_d]\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mu &amp;= {1\over d} \sum^d_{i=1}x_i\\
\sigma^2 &amp;= {1\over d} \sum^d_{i=1} \sum^d_{i=1} (x_i-\mu)^2\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the mean and <span class="math notranslate nohighlight">\(\sigma^2\)</span> is the variance of
the input.</p>
</li>
<li><p>Normalize the input: subtracting the mean and dividing by the
standard deviation:</p>
<div class="math notranslate nohighlight">
\[\hat{x_i} = { x_i - \mu \over \sqrt{\sigma^2 + \epsilon}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant added to the variance to
avoid division by zero.</p>
</li>
<li><p>Scale and shift: after normalization, the output is scaled and
shifted by <strong>learnable parameters</strong> <span class="math notranslate nohighlight">\(\gamma\)</span> (scale) and
<span class="math notranslate nohighlight">\(\beta\)</span> (shift), which allow the model to restore the original
distribution if needed:</p>
<div class="math notranslate nohighlight">
\[y_i = \gamma \cdot \hat{x_i} + \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameters
learned during the training process.</p>
</li>
</ol>
</section>
<section id="residual-connections">
<h3><span class="section-number">7.1.7. </span>Residual Connections<a class="headerlink" href="#residual-connections" title="Link to this heading"></a></h3>
<p>In the transformer architecture, <strong>residual connections</strong> are used after
each key operation, such as:</p>
<ul class="simple">
<li><p><strong>After Self-Attention</strong>: The input to the attention layer is added
back to the output of the self-attention mechanism.</p></li>
<li><p><strong>After Feed-Forward Networks</strong>: Similarly, after the output of the
feed-forward network is computed, the input to the feed-forward block
is added back to the result.</p></li>
</ul>
<p>In both cases, the sum is typically passed through a <strong>Layer
Normalization</strong> operation, which stabilizes the training process
further.</p>
<p>Residual connection has the following advantages:</p>
<ol class="arabic simple">
<li><p><strong>Skip Connection</strong>: The original input to the layer is <strong>skipped
over</strong> and added directly to the output of the layer. This allows the
model to preserve the information from earlier layers, helping it
learn faster and more efficiently.</p></li>
<li><p><strong>Enabling Easier Gradient Flow</strong>: In deep neural networks, as layers
become deeper, gradients can either vanish or explode, making
training difficult. Residual connections mitigate the vanishing
gradient problem by allowing gradients to flow more easily through
the network during backpropagation.</p></li>
<li><p><strong>Helping with Identity Mapping</strong>: Residual connections allow the
network to learn <strong>identity mappings</strong>. If a certain layer doesn’t
need to make any modifications to the input, the network can simply
learn to output the input directly, ensuring that deeper layers don’t
hurt the performance of the network. This helps the network avoid
situations where deeper layers perform worse than shallow layers.</p></li>
<li><p><strong>Stabilizing Training</strong>: The direct path from the input to the
output, via the residual connection, helps stabilize the training by
providing an additional gradient flow, making the learning process
more robust to initialization and hyperparameters.</p></li>
</ol>
</section>
<section id="feed-forward-networks">
<h3><span class="section-number">7.1.8. </span>Feed-Forward Networks<a class="headerlink" href="#feed-forward-networks" title="Link to this heading"></a></h3>
<p>In the Transformer architecture, <strong>Feed-Forward Networks (FFNs)</strong> are a
key component within each layer of the encoder and decoder. FFNs are
applied independently to each token in the sequence, after the attention
mechanism (self-attention or cross-attention). They process the
information passed through the attention mechanism to refine the
representations of each token.</p>
<p>The characteristics and roles of FFN:</p>
<ol class="arabic simple">
<li><p><strong>Position-Independent</strong>: FFNs operate <strong>independently</strong> on each
token’s embedding, without considering the sequence structure. Each
token is treated individually.</p></li>
<li><p><strong>Non-Linearity</strong>: The <strong>activation function</strong> (like ReLU or GELU)
introduces <strong>non-linearity</strong> into the model, which is crucial for
allowing the network to learn complex patterns in the data</p></li>
<li><p><strong>Parameter Sharing</strong>: The same FFN is applied to each token in the
sequence independently. The parameters are shared across all tokens,
which is computationally efficient and reduces the number of
parameters in the model.</p></li>
<li><p><strong>Dimensionality Expansion</strong>: The hidden layer size <span class="math notranslate nohighlight">\(d_{ff}\)</span> is
typically <strong>larger</strong> than the model dimension
<span class="math notranslate nohighlight">\(d_{\text{model}}\)</span> (often by a factor of 4), allowing the
network to learn richer representations in the intermediate space.</p></li>
<li><p><strong>Local Information Processing</strong>: FFNs only process <strong>local</strong>
information about each token’s embedding, as opposed to the
self-attention mechanism, which captures <strong>global dependencies</strong>
across all tokens in the sequence.</p></li>
<li><p><strong>Residual Connection</strong>: FFNs in transformers use <strong>residual
connections</strong>, where the input to the FFN is added to the output.
This helps <strong>prevent vanishing gradient issues</strong> and makes training
deep models more efficient.</p></li>
<li><p><strong>Parallelization</strong>: Since FFNs are applied independently to each
token, they can be <strong>parallelized</strong> effectively, leading to faster
training and inference.</p></li>
</ol>
<p>The network can only process a fixed number of vectors at a time, known
as its <strong>context size</strong>. The context size can be 4096 (GPT-3) up to 2M
tokens (LongRoPE).</p>
</section>
<section id="label-smoothing">
<h3><span class="section-number">7.1.9. </span>Label Smoothing<a class="headerlink" href="#label-smoothing" title="Link to this heading"></a></h3>
<p>In transformer models, <strong>label smoothing</strong> is commonly applied during
the training phase to improve the model’s generalization by modifying
the target labels used for training. This technique is typically used in
tasks like <strong>machine translation</strong>, <strong>language modeling</strong>, and other
sequence-to-sequence tasks.</p>
<p>Label smoothing is applied after the decoder generates a probability
distribution over the vocabulary in the final layer. The output of the
decoder is a vector of logits (raw predictions), which are transformed
into a probability distribution using <strong>softmax</strong>. After applying
softmax, the predicted probabilities are compared to the smoothed target
distribution to calculate the loss.</p>
<p>The target distribution is originally an one-hot vector. After <strong>label
smoothing</strong>, the one-hot encoding is adjusted so that the correct token
has a reduced probability, and the incorrect tokens share a small amount
of probability mass. For example, if the origianl one-hot vector is
<span class="math notranslate nohighlight">\([0, 1, 0, 0]\)</span>, then label smoothing would convert this vector
into something like <span class="math notranslate nohighlight">\([0.05, 0.9, 0.05, 0.05]\)</span>.</p>
<p>During training, the model computes the <strong>cross-entropy loss</strong> between
the predicted probabilities and the smoothed target distribution. The
loss function is modified as follows:</p>
<div class="math notranslate nohighlight">
\[L = -\sum_i{\hat{y_i} \log(p_i)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y_i}\)</span> is the smoothed target probability for class
<span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(p_i\)</span> is the predicted probability for class
<span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>The model’s output probabilities are then adjusted during training by
backpropagating the modified loss. This encourages the model to
distribute some probability to alternative tokens, making it less likely
to become overly confident in its predictions.</p>
<p>Label smoothing is important in transformers because</p>
<ul class="simple">
<li><p><strong>Prevents Overfitting</strong>: Label smoothing forces the model to spread
some probability mass over other tokens, making it <strong>less
overconfident</strong> and more likely to generalize well to unseen data.</p></li>
<li><p><strong>Encourages Robustness</strong>: By smoothing the target labels, the
transformer is encouraged to explore alternative possibilities for
each token rather than memorizing the exact sequence of tokens in the
training data.</p></li>
<li><p><strong>Improved Calibration</strong>: The model learns to <strong>distribute probability
more evenly</strong> across all tokens, which often results in
<strong>better-calibrated probabilities</strong> that improve performance in tasks
such as <strong>classification</strong> and <strong>sequence generation</strong>.</p></li>
<li><p><strong>Training Stability</strong>: Label smoothing reduces the effect of outliers
and noisy labels in the training data, improving the overall stability
of training and leading to faster convergence.</p></li>
</ul>
</section>
<section id="softmax-and-temperature">
<h3><span class="section-number">7.1.10. </span>Softmax and Temperature<a class="headerlink" href="#softmax-and-temperature" title="Link to this heading"></a></h3>
<p>The <strong>softmax function</strong> is a mathematical operation used to transform a
vector of raw scores (<strong>logits</strong>) into a vector of <strong>probabilities</strong>. It
takes a vector of real numbers, <span class="math notranslate nohighlight">\(z = [z_1, z_2, \dots, z_n]\)</span>, and
maps it to a probability distribution, where each element is in the
range [0, 1], and the sum of all elements equals 1. Mathematically,</p>
<div class="math notranslate nohighlight">
\[p_i=\text{softmax}(z_i) = {e^{z_i}\over \sum^n_{j=1}e^{z_j}}\]</div>
<p>The softmax function has been used in GPT in two ways:</p>
<ul class="simple">
<li><p><strong>Probability Distribution</strong>: It converts raw scores into
probabilities that sum to 1. Next token as prediction will be the
token with the highest probability.</p></li>
<li><p><strong>Attention Weights</strong>: In attention mechanism, softmax is applied to
the score of all tokens in the sequence to normalize them into
attention weights.</p></li>
</ul>
<p>Properties of Softmax:</p>
<ul class="simple">
<li><p><strong>Exponentiation</strong>: Amplifies the difference between higher and lower
scores, making the largest score dominate.</p></li>
<li><p><strong>Normalization</strong>: Ensures that the output probabilities sum to 1.</p></li>
<li><p><strong>Differentiable</strong>: Enables backpropagation for training the model.</p></li>
</ul>
<p>The <strong>temperature</strong> parameter is used in the softmax function to control
the sharpness or smoothness of the probability distribution over the
logits, affecting how confident or diverse the model’s predictions are.
When using a temperature <span class="math notranslate nohighlight">\(T &gt; 0\)</span>, the logits are scaled by
<span class="math notranslate nohighlight">\(\frac{1}{T}\)</span> before applying softmax:</p>
<div class="math notranslate nohighlight">
\[p_i = \text{softmax}(z_i) = {\exp(z_i/T)\over \sum^n_{j=1}\exp(z_j/T)}\]</div>
<p>When <span class="math notranslate nohighlight">\(T\)</span> is larger, more weight is given to the lower values, then
the distribution is more uniform. If <span class="math notranslate nohighlight">\(T\)</span> is smaller, the biggest
logit score will dominate more aggresively. Setting <span class="math notranslate nohighlight">\(T=0\)</span> gives
all the weights to the maximum value resulting a ~100% probability. This
means higher temperature leads to creative but potentially incoherent
outputs, and lower temperature leads to safe and predictable outputs.</p>
</section>
<section id="unembedding-matrix">
<h3><span class="section-number">7.1.11. </span>Unembedding Matrix<a class="headerlink" href="#unembedding-matrix" title="Link to this heading"></a></h3>
<p>The <strong>unembedding matrix</strong> in the final layer of GPT is the counterpart
to the <strong>embedding matrix</strong> used at the input layer. GPT’s final hidden
layer outputs continuous vectors for each token position in the input
sequence. The unembedding matrix projects these vectors into a space
where each dimension corresponds to a token in the vocabulary, producing
logits for all vocabulary tokens.</p>
<p>The unembedding matrix is not randomly initialized, instead, it’s
initialized as the transpose of the embedding matrix
<span class="math notranslate nohighlight">\(W_U = W_E^T\)</span>. If the vocabulary size is <span class="math notranslate nohighlight">\(V\)</span> and the hidden
layer size is <span class="math notranslate nohighlight">\(d\)</span>, the unembedding matrix <span class="math notranslate nohighlight">\(W_U\)</span> has
dimensions <span class="math notranslate nohighlight">\(V \times d\)</span>. In the final layer, GPT produces a hidden
state <span class="math notranslate nohighlight">\(h\)</span> with size <span class="math notranslate nohighlight">\(d\)</span> for each token position. The
unembedding matrix is applied as follows.</p>
<div class="math notranslate nohighlight">
\[\text{Logits} = h \cdot W_U^T\]</div>
<p>The logits are passed through the <strong>softmax function</strong> to generate
probabilities over the vocabulary. The token with the highest
probability (or sampled stochastically) is chosen as the next token.</p>
<p>Using a learned unembedding matrix to compute logits in the final layer
of GPT offers critical advantages over directly computing logits from
the final hidden vector without this additional projection step:</p>
<ul class="simple">
<li><p>The embedding and unembedding matrices establish a connection between
the input and output token spaces. Without an unembedding matrix,
there would be no learned mechanism to align the model’s internal
representation to the specific vocabulary used for prediction.</p></li>
<li><p>The model’s hidden states are designed to represent rich features of
the input sequence rather than being explicitly tied to the vocabulary
size. The unembedding matrix translates the compressed hidden state
(e.g. 768 or 1024 size) into a vocabulary distribution (e.g. ~50k
tokens), ensuring the model can scale to larger vocabularies or output
spaces.</p></li>
<li><p>The unembedding matrix learns how to transform these rich
representations into logits that accurately reflect token
probabilities in the specific vocabulary. It provides a structured way
for gradients from the loss function (e.g., cross-entropy loss) to
update both the model’s hidden representations and the vocabulary
mappings.</p></li>
</ul>
</section>
<section id="decoding">
<h3><span class="section-number">7.1.12. </span>Decoding<a class="headerlink" href="#decoding" title="Link to this heading"></a></h3>
<p>In transformer models, <strong>decoding</strong> refers to the process of generating
output sequences from a model’s learned representations. Decoder takes
the hidden state generated by encoder from input representations as well
as previously generated tokens (or a start token) and progressively
generates the output sequence one by one based on the probability
distribution over all possible words in the vocabulary for the next
token.</p>
<p>Depending on the specific task and goals (e.g., translation, generation,
or summarization), different decoding strategies like <strong>beam search</strong>,
<strong>top-k sampling</strong>, <strong>top-p sampling</strong>, and <strong>temperature sampling</strong> can
be used to strike the right balance between creativity and accuracy.</p>
<section id="greedy-decoding">
<h4><span class="section-number">7.1.12.1. </span>Greedy Decoding<a class="headerlink" href="#greedy-decoding" title="Link to this heading"></a></h4>
<p>Greedy decoding is the simplest and most straightforward method. At each
time step, the model chooses the token with the highest probability from
the predicted distribution and adds it to the output sequence.</p>
</section>
<section id="beam-search">
<h4><span class="section-number">7.1.12.2. </span>Beam Search<a class="headerlink" href="#beam-search" title="Link to this heading"></a></h4>
<p>Beam search is a more advanced method than greedy decoding. It keeps
track of multiple hypotheses at each decoding step (instead of just the
most probable one) and selects the top-k most likely sequences (called
the “beam width”).</p>
<p>At each decoding step, beam search explores the top-k candidate
sequences (instead of just one) and chooses the one with the highest
cumulative probability. A hyperparameter, <strong>beam width</strong>, controls how
many candidate sequences are considered at each step.</p>
<figure class="align-center" id="id17">
<img alt="beam_search" src="_images/beam_search.png" />
<figcaption>
<p><span class="caption-text">Beam Search</span><a class="headerlink" href="#id17" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="top-k-sampling">
<h4><span class="section-number">7.1.12.3. </span>Top-k Sampling<a class="headerlink" href="#top-k-sampling" title="Link to this heading"></a></h4>
<p>After the model outputs a probability distribution over the entire
vocabulary (e.g., 50,000 tokens for GPT-style models). Only the top
<span class="math notranslate nohighlight">\(k\)</span> tokens with the highest probabilities are retained. All other
tokens are discarded. The probabilities of the remaining <span class="math notranslate nohighlight">\(k\)</span>
tokens are renormalized to sum to 1. A token is randomly selected from
the <span class="math notranslate nohighlight">\(k\)</span>-token subset based on the renormalized probabilities.</p>
<p>When <span class="math notranslate nohighlight">\(k=1\)</span>, top-k sampling is the same as greedy decoding, where
the token with the highest probability is chosen. Higher <span class="math notranslate nohighlight">\(k\)</span>
allows more variety by considering more tokens.</p>
<p>Top-k sampling is considered <strong>static</strong> and <strong>predefined</strong> because once
a contant <span class="math notranslate nohighlight">\(k\)</span> is specified, at each decoding step, only the top
<span class="math notranslate nohighlight">\(k\)</span> tokens are considered for sampling. Regardless the shape of
distribution, the size of the candidate pool <span class="math notranslate nohighlight">\(k\)</span> does not change.
If the probability distribution is “flat”(many tokens with similar
probabilities), top-k might still discard important tokens outside the
top <span class="math notranslate nohighlight">\(k\)</span>. If the distribution is “peaked” (one or a few tokens
dominate), top-k might include unlikely tokens unnecessarily.</p>
</section>
<section id="top-p-nucleus-sampling">
<h4><span class="section-number">7.1.12.4. </span>Top-p (Nucleus) Sampling<a class="headerlink" href="#top-p-nucleus-sampling" title="Link to this heading"></a></h4>
<p>After the model outputs a probability distribution over the vocabulary.
Tokens are sorted in descending order of probability. A cumulative sum
of probabilities is calculated for the sorted tokens. The smallest set
of tokens whose cumulative probability exceeds or equals <span class="math notranslate nohighlight">\(p\)</span> are
retained. The probabilities of the selected tokens are renormalized to
sum to 1. A token is randomly selected from this dynamic subset.</p>
<p>When <span class="math notranslate nohighlight">\(p=1\)</span>, all tokens are included, then top-p sampling is
equivalent to pure sampling. Lower <span class="math notranslate nohighlight">\(p\)</span> focuses on fewer tokens,
ensuring higher-quality predictions while retaining some randomness.</p>
<p>Top-p sampling is considered <strong>dynamic</strong> and <strong>adaptive</strong> because the
number of tokens in the pool varies depending on the shape of the
probability distribution. If the distribution is “peaked,” top-p will
include fewer tokens because the most probable tokens quickly satisfy
the cumulative threshold <span class="math notranslate nohighlight">\(p\)</span>. If the distribution is “flat,” top-p
will include more tokens to ensure the cumulative probability reaches
<span class="math notranslate nohighlight">\(p\)</span>.</p>
</section>
<section id="temperature-scaling">
<h4><span class="section-number">7.1.12.5. </span>Temperature Scaling<a class="headerlink" href="#temperature-scaling" title="Link to this heading"></a></h4>
<p>As mentioned in the section “Softmax and Temperature”, temperature
scaling is applied to the logits right before sampling or selection
(e.g., during top-k or top-p sampling). It modifies the softmax function
with a parameter <span class="math notranslate nohighlight">\(T\)</span> added to adjust the shape of the resulting
probability distribution from logits. Temperature scaling is used in
tasks requiring stochastic decoding methods like top-k sampling or
nucleus sampling.</p>
<p><strong>Temperature (:math:`T`) + Top-k</strong>:</p>
<ul class="simple">
<li><p>“High <span class="math notranslate nohighlight">\(T\)</span> + high <span class="math notranslate nohighlight">\(k\)</span>” results in extremely diverse and
creative outputs. It may produce incoherent or irrelevant text because
too many unlikely tokens are considered. It’s used when generating
highly imaginative or exploratory text, such as in creative writing.</p></li>
<li><p>“High <span class="math notranslate nohighlight">\(T\)</span> + low <span class="math notranslate nohighlight">\(k\)</span>” balances diversity with some level of
coherence. Even with low <span class="math notranslate nohighlight">\(k\)</span>, high <span class="math notranslate nohighlight">\(T\)</span> may introduce
unexpected word choices. It’s used when creative tasks where some
randomness is desired, but the context must still be respected.</p></li>
<li><p>“Low <span class="math notranslate nohighlight">\(T\)</span> + high <span class="math notranslate nohighlight">\(k\)</span>” produces coherent and focused outputs
because <span class="math notranslate nohighlight">\(T\)</span> emphasizes the most probable tokens. The effect of
high <span class="math notranslate nohighlight">\(k\)</span> is mitigated because the scaled probabilities naturally
limit diversity.</p></li>
<li><p>“Low <span class="math notranslate nohighlight">\(T\)</span> + low <span class="math notranslate nohighlight">\(k\)</span>” produces highly deterministic outputs.
Text may seem repetitive. It’s used when tasks requiring consistency,
such as factual responses or concise answers.</p></li>
</ul>
<p><strong>Temperature (:math:`T`) + Top-p</strong>:</p>
<ul class="simple">
<li><p>“High <span class="math notranslate nohighlight">\(T\)</span> + high <span class="math notranslate nohighlight">\(p\)</span>” produces diverse outputs, but the
context may still be loosely followed. It may produce incoherent or
irrelevant text because too many unlikely tokens are considered. It’s
used when generating exploratory or brainstorming text.</p></li>
<li><p>“High <span class="math notranslate nohighlight">\(T\)</span> + low <span class="math notranslate nohighlight">\(p\)</span>” produces constrained output despite
high <span class="math notranslate nohighlight">\(T\)</span>, as only the most probable tokens within the
<span class="math notranslate nohighlight">\(p\)</span>-threshold are considered. Even with low <span class="math notranslate nohighlight">\(k\)</span>, high
<span class="math notranslate nohighlight">\(T\)</span> may introduce unexpected word choices. It’s used for
slightly creative tasks with some emphasis on coherence.</p></li>
<li><p>“Low <span class="math notranslate nohighlight">\(T\)</span> + high <span class="math notranslate nohighlight">\(p\)</span>” produces coherent and slightly
diverse text. It’s used in balanced tasks, such as assistant chatbots
or domain-specific content generation.</p></li>
<li><p>“Low <span class="math notranslate nohighlight">\(T\)</span> + low <span class="math notranslate nohighlight">\(p\)</span>” produces very deterministic and rigid
outputs. it’s used when generating formal or technical content
requiring precision, such as legal or scientific writing.</p></li>
</ul>
</section>
<section id="summary-1">
<span id="id2"></span><h4><span class="section-number">7.1.12.6. </span>Summary<a class="headerlink" href="#summary-1" title="Link to this heading"></a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Advantages</p></th>
<th class="head"><p>Disadvantages</p></th>
<th class="head"><p>Use Cases</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Greedy
Decoding</p></td>
<td><p>Simple, fast,
deterministic</p></td>
<td><p>May produce
repetitive or
suboptimal
sequences</p></td>
<td><p>When speed is
important, low
diversity tasks</p></td>
</tr>
<tr class="row-odd"><td><p>Beam
Search</p></td>
<td><p>Produces
higher-quality
sequences, less
repetitive</p></td>
<td><p>Computationally
expensive,
limited by beam
width</p></td>
<td><p>Machine
translation,
summarization</p></td>
</tr>
<tr class="row-even"><td><p>Top-k
Sampling</p></td>
<td><p>Adds diversity,
avoids repetitive
output</p></td>
<td><p>May reduce
coherence in
some cases</p></td>
<td><p>Creative text
generation,
storytelling</p></td>
</tr>
<tr class="row-odd"><td><p>Top-p
Sampling</p></td>
<td><p>Dynamically adjusts
for diversity, more
natural</p></td>
<td><p>May still
produce
incoherent
outputs</p></td>
<td><p>Creative text
generation,
dialogue systems</p></td>
</tr>
<tr class="row-even"><td><p>Temperature
Sampling</p></td>
<td><p>Fine control over
and diversity
randomness, balance
between coherence</p></td>
<td><p>Requires tuning
for optimal
results</p></td>
<td><p>Creative text
randomness
generation,
fine-tuning output</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="modern-transformer-techniques">
<h2><span class="section-number">7.2. </span>Modern Transformer Techniques<a class="headerlink" href="#modern-transformer-techniques" title="Link to this heading"></a></h2>
<section id="kv-cache">
<h3><span class="section-number">7.2.1. </span>KV Cache<a class="headerlink" href="#kv-cache" title="Link to this heading"></a></h3>
<p>The primary purpose of the KV cache is to <strong>speed up the inference
process</strong> and make it more efficient. Specifically, during
autoregressive generation (such as generating text one token at a time),
the transformer model processes the input tokens sequentially, which
means that for each new token, it needs to compute the attention scores
between the current token and all previous tokens.</p>
<p>Instead of recalculating the <strong>key (K)</strong> and <strong>value (V)</strong> vectors for
the entire sequence at each step (which would be computationally
expensive), the KV cache allows the model to <strong>reuse the keys and
values</strong> from previous tokens, thus reducing redundant computations.</p>
<p>As demonstrated in the diagram below, during the training process,
attention scores are calculated by this formula without KV Cache:</p>
<div class="math notranslate nohighlight">
\[\text{Attention Weights} = \text{softmax}\Big({QK^T\over{\sqrt{d_k}}}\Big)\]</div>
<p><img alt="qkv_attention_pattern" src="_images/qkv_attention_pattern.png" /></p>
<p>When generating the next token during inference, the model doesn’t need
to recompute the keys and values for the tokens it has already
processed. Instead, it simply retrieves the stored keys and values from
the cache for all previously generated tokens. Only the new token’s key
and value are computed for the current timestep and added to the cache.</p>
<p>During the attention computation for each new token, the model uses both
the new key and value (for the current token) and the cached keys and
values (for all previous tokens). This way, the attention mechanism can
still compute the correct attention scores and weighted sums without
recalculating everything from scratch.</p>
<p><strong>The attention formula with Cache:</strong> for a new token <span class="math notranslate nohighlight">\(t\)</span>,</p>
<div class="math notranslate nohighlight">
\[\text{Attention Output} = \text{softmax} \Big({Q_t \cdot [K_{\text{cache}}, K_t]^T\over \sqrt{d_k}}\Big) \cdot [V_{\text{cache}}, V_t]\]</div>
<p><img alt="kv_cache" src="_images/kv_cache.png" /></p>
<p><strong>Why Not Cache Queries:</strong> <strong>Queries</strong> are specific to the token being
processed at the current step of generation. For every new token in
autoregressive decoding, the query vector needs to be freshly computed
because it is derived from the embedding of the current token. Keys and
values, on the other hand, represent the context of the previous tokens,
which remains the same across multiple steps until the sequence is
extended.</p>
<p><strong>Space complexity of KV Cache is huge without optimization</strong>: The space
complexity is calculated by number of layers * number of batch size * number
of attention heads * attention head size * sequence length.</p>
<p>Space complexity can be optimized by reducing “number of attention
heads” without too much penalty on performance.</p>
</section>
<section id="multi-query-attention">
<h3><span class="section-number">7.2.2. </span>Multi-Query Attention<a class="headerlink" href="#multi-query-attention" title="Link to this heading"></a></h3>
<p><strong>Multi-Query Attention (MQA)</strong> is a variant of the attention mechanism
introduced to improve the efficiency of transformer models, particularly
in scenarios where decoding speed and memory usage are critical. It
modifies the standard multi-head attention by using multiple query heads
but sharing the key and value matrices across all the heads. There are
still multiple independent query heads (<span class="math notranslate nohighlight">\(Q\)</span>), but the <strong>key
(:math:`K`) and value (:math:`V`) matrices are shared</strong> across all the
heads.</p>
<p>Each query head <span class="math notranslate nohighlight">\(i\)</span> computes its attention scores with the shared
key matrix:</p>
<div class="math notranslate nohighlight">
\[\text{Attention}_i = \text{softmax} \Big({Q_i K^T \over \sqrt{d_k}}\Big)V\]</div>
<figure class="align-center" id="id18">
<img alt="multiquery_attention" src="_images/multiquery_attention.png" />
<figcaption>
<p><span class="caption-text">Multi-Query Attention</span><a class="headerlink" href="#id18" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Advantages of MQA:</strong></p>
<ul class="simple">
<li><p><strong>Efficiency in Memory Usage</strong>: By sharing the <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span>
matrices across heads, the memory footprint is reduced, particularly
for the KV cache used during autoregressive generation in large
models. This is especially valuable for serving large-scale language
models with limited GPU/TPU memory.</p></li>
<li><p><strong>Faster Decoding</strong>: During autoregressive decoding (e.g., in GPT-like
models), each query needs to attend to the cached keys and values. In
standard multi-head attention, this involves accessing multiple
<span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span> matrices, which can slow down decoding. In
MQA, since only one shared <span class="math notranslate nohighlight">\(K\)</span> and <span class="math notranslate nohighlight">\(V\)</span> matrix is used, the
decoding process is faster and more streamlined</p></li>
<li><p><strong>Minimal Performance Tradeoff</strong>: Despite simplifying the model, MQA
often achieves comparable performance to standard multi-head attention
in many tasks, particularly in large-scale language models.</p></li>
</ul>
</section>
<section id="grouped-query-attention">
<h3><span class="section-number">7.2.3. </span>Grouped-Query Attention<a class="headerlink" href="#grouped-query-attention" title="Link to this heading"></a></h3>
<p><strong>Grouped-Query Attention (GQA)</strong> is a hybrid approach between
<strong>Multi-Head Attention (MHA)</strong> and <strong>Multi-Query Attention (MQA)</strong> that
balances computational efficiency and expressivity. In GQA, multiple
query heads are grouped together, and each group shares a set of
<strong>keys</strong> and <strong>values</strong>. This design seeks to retain some of the
flexibility of MHA while reducing the memory and computational overhead,
similar to MQA.</p>
<p>Mathematically, if there are <span class="math notranslate nohighlight">\(G\)</span> groups, each with <span class="math notranslate nohighlight">\(H / G\)</span>
heads, the queries are processed independently for each group but share
keys and values within the group:</p>
<div class="math notranslate nohighlight">
\[\text{Attention}_i = \text{softmax} \Big({Q_i K^T_{\text{group,i}}\over \sqrt{d_k}}\Big) V_{group,i}\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> is the query head within a group.</p>
<figure class="align-center" id="id19">
<img alt="grouped_query_attention" src="_images/grouped_query_attention.png" />
<figcaption>
<p><span class="caption-text">Grouped Query Attention</span><a class="headerlink" href="#id19" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Advantages of GQA:</strong></p>
<ul class="simple">
<li><p><strong>Efficiency</strong>:</p>
<ul>
<li><p>Reduced KV Cache Size: GQA requires fewer key and value matrices
compared to MHA. This reduces memory usage, especially during
autoregressive decoding when keys and values for all previous tokens
are stored in a cache.</p></li>
<li><p>Faster Inference: By reducing the number of keys and values to
process, GQA speeds up attention computations during decoding,
particularly in long-sequence tasks.</p></li>
</ul>
</li>
<li><p><strong>Balance Between Flexibility and Efficiency</strong>:</p>
<ul>
<li><p>More Expressivity Than MQA: Unlike MQA, where all heads share the
same keys and values, GQA allows multiple groups of keys and values,
enabling more flexibility for the attention mechanism to learn
diverse patterns.</p></li>
<li><p>Simpler Than MHA: GQA is less computationally expensive and
memory-intensive than MHA, as fewer sets of keys and values are
used.</p></li>
</ul>
</li>
<li><p><strong>Scalability</strong>:</p>
<ul>
<li><p>GQA is well-suited for very large models and long-sequence tasks
where standard MHA becomes computationally and memory prohibitive.</p></li>
</ul>
</li>
</ul>
</section>
<section id="flash-attention">
<h3><span class="section-number">7.2.4. </span>Flash Attention<a class="headerlink" href="#flash-attention" title="Link to this heading"></a></h3>
<p>FlashAttention <a class="reference internal" href="reference.html#tri-dao-1" id="id3"><span>[Tri_Dao_1]</span></a> is a novel and
efficient algorithm designed to address the computational and memory
challenges of self-attention in Transformers, particularly for long
sequences. It’s designed to solve two challenges of traditional
Transformer implementation:</p>
<ul class="simple">
<li><p>Self-attention mechanisms in transformers are computationally
expensive with quadratic time (<span class="math notranslate nohighlight">\(n^2\)</span>) and memory complexity
concerning sequence length (<span class="math notranslate nohighlight">\(n\)</span>), making them inefficient for
long sequences.</p></li>
<li><p>It’s been revealed in “Data Movement is All You Need” <a class="reference internal" href="reference.html#andrei" id="id4"><span>[Andrei]</span></a> that the
key bottleneck during training a Transformer is data movement (reading
and writing data) rather than computation. The paper highlights that
many transformer operations are <strong>memory-bandwidth-bound</strong>, meaning
that the speed of data transfer to and from HBM often becomes a
bottleneck rather than the GPU’s raw computational power. This finding
shows that existing implementations of Transformers do not efficiently
utilize GPUs.</p></li>
</ul>
<figure class="align-center" id="id20">
<img alt="flashattention" src="_images/flashattention_paper.png" />
<figcaption>
<p><span class="caption-text">Flash Attention (source: <a class="reference external" href="https://arxiv.org/abs/2205.14135">Flash Attention</a>)</span><a class="headerlink" href="#id20" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The idea of Flash Attention is <strong>computing by blocks</strong> to reduce HBM
reads and writes. Their implementation is a <strong>fused CUDA kernel</strong> for
fine-grained control of memory accesses with two techniques:</p>
<ul>
<li><p><strong>Tiling</strong>: Tiling works by decomposing large softmax into smaller
ones by scaling. It firstly loads inputs by blocks from HBM to SRAM
for fast computation, computes attention output with respect to that
block in SRAM, then updates output in HBM by scaling.</p>
<p>The method decomposes softmax as follows as an example.
<span class="math notranslate nohighlight">\([x_1, x_2]\)</span> represents the concatenation of two partitions
(blocks) of input scores. Softmax is independently computed one block
at a time. This block-wise operations reduce memory and computational
overhead compared to processing the entire sequence at once.
<span class="math notranslate nohighlight">\(m(x)\)</span> represents the maximum value within a block of the
attention matrix. It’s used as a max-shifting step during the softmax
calculation, which improves numerical stability. <span class="math notranslate nohighlight">\(\ell(x)\)</span> is a
normalization factor used to convert the exponentials into probability
distributions. The combination of scaling factors ensures that the
results match the global Softmax computation if it were performed over
the full sequence.</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;m(x) = m(\begin{bmatrix}x_1 &amp; x_2\end{bmatrix}) = \max(m(x_1), m(x_2))\\
&amp;f(x) = \begin{bmatrix} e^{m(x_1)-m(x)}f(x_1) &amp; e^{m(x_2)-m(x)}f(x_2)\end{bmatrix}\\
&amp;\ell(x) = \ell(\begin{bmatrix}x_1 &amp; x_2\end{bmatrix}) = e^{m(x_1)-m(x)}f(x_1)+e^{m(x_2)-m(x)}f(x_2)\\
&amp;\text{softmax}(x) = {f(x)\over \ell(x)}\end{split}\]</div>
</li>
<li><p><strong>Recomputation</strong>: the idea is to store the output
<span class="math notranslate nohighlight">\(\text{softmax}(PQ^T)V\)</span> and softmax normalization factors
<span class="math notranslate nohighlight">\(m(x), \ell(x)\)</span> rather than storing the attention matrix from
forward in HBM, then recompute the attention matrix in the backward in
SRAM.</p>
<p>Recomputation allows the model to discard intermediate activations
during the forward pass, only keeping the most essential data for
backpropagation. This frees up memory, enabling the model to process
much longer sequences or use larger batch sizes. It essentially trades
<strong>additional computation</strong> for <strong>reduced memory usage</strong>, making the
process scalable. This is a tradeoff that is often acceptable,
especially with hardware accelerators (GPUs/TPUs) where computation
power is abundant but memory capacity is limited.</p>
</li>
</ul>
<p>Both <strong>tiling</strong> and <strong>recomputation</strong> aim to address memory and
computational challenges when working with large models or long
sequences, each improving efficiency in different ways:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Benefit</strong></p></th>
<th class="head"><p><strong>Tiling</strong></p></th>
<th class="head"><p><strong>Recomputation</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Memory
Efficiency</p></td>
<td><p>Reduces memory usage by
processing smaller tiles
instead of the whole
sequence at once.</p></td>
<td><p>Saves memory by not
storing intermediate
results; recomputes when
needed.</p></td>
</tr>
<tr class="row-odd"><td><p>Computational
Speed</p></td>
<td><p>Enables parallel
processing of smaller
tiles, improving
computation time.</p></td>
<td><p>Reduces memory footprint,
potentially increasing
throughput by minimizing
the need to store large
intermediate values.</p></td>
</tr>
<tr class="row-even"><td><p>Handling
Long
Sequences</p></td>
<td><p>Makes it feasible to
process long sequences
that otherwise wouldn’t
fit in memory.</p></td>
<td><p>Allows for computation of
large models with limited
memory by recomputing
expensive intermediate
steps.</p></td>
</tr>
<tr class="row-odd"><td><p>Hardware
Utilization</p></td>
<td><p>Optimizes the use of
limited memory resources
(e.g., GPU/TPU) by
limiting the amount of
data in memory.</p></td>
<td><p>Helps avoid running out
of memory by not
requiring large storage
for intermediate states.</p></td>
</tr>
<tr class="row-even"><td><p>Scalability</p></td>
<td><p>Enables handling of
larger datasets and
longer sequences without
overwhelming memory.</p></td>
<td><p>Makes it possible to work
with large models and
datasets by not storing
every intermediate
result.</p></td>
</tr>
<tr class="row-odd"><td><p>Reduced
Memory
Bandwidth</p></td>
<td><p>Lowers memory bandwidth
requirements by only
loading small parts of
data at a time.</p></td>
<td><p>Minimizes the need for
frequent memory
writes/reads, improving
memory access efficiency.</p></td>
</tr>
<tr class="row-even"><td><p>Reduces
Redundant
Computation</p></td>
<td><p>Focuses on smaller
sub-problems, reducing
redundant operations.</p></td>
<td><p>Recomputes intermediate
steps only when
necessary, avoiding
unnecessary storage and
computation.</p></td>
</tr>
</tbody>
</table>
<p><strong>Flash Attention 2</strong>:</p>
<p>FlashAttention-2 <a class="reference internal" href="reference.html#tri-dao-2" id="id6"><span>[Tri_Dao_2]</span></a> builds upon
FlashAttention by addressing suboptimal work partitioning between
different thread blocks and warps on the GPU. It reduces the number of
non-matrix multiplication (matmul) FLOPs, which are slower to perform on
GPUs. It also parallelizes the attention computation across the sequence
length dimension, in addition to the batch and number of heads
dimensions. This increases occupancy (utilization of GPU resources),
especially when the sequence is long and the batch size is small. Within
each thread block, FlashAttention-2 distributes the work between warps
to reduce communication through shared memory. FlashAttention-2 also
uses a minor tweak to the backward pass, using the row-wise logsumexp
instead of both the row-wise max and row-wise sum of exponentials in the
softmax. It incorporates techniques like swapping the order of loops and
parallelization over the sequence length, which were first suggested in
the Triton implementation. Furthermore, it can also efficiently handle
multi-query attention (MQA) and grouped-query attention (GQA) by
manipulating indices instead of duplicating key and value heads.</p>
<p><strong>FlashAttention-3</strong>:</p>
<p>FlashAttention-3 <a class="reference internal" href="reference.html#jay-shah" id="id7"><span>[Jay_Shah]</span></a> further improves
performance, especially on newer GPUs like the H100. It achieves this by
exploiting asynchrony and low-precision computations. It uses a
<strong>warp-specialized software pipelining</strong> scheme that splits the
producers and consumers of data into separate warps, overlapping overall
computation and data movement. This hides memory and instruction issue
latencies. FlashAttention-3 overlaps non-GEMM operations involved in
softmax with the asynchronous WGMMA instructions for GEMM. This is done
by interleaving block-wise matmul and softmax operations, and by
reworking the FlashAttention-2 algorithm to circumvent sequential
dependencies between softmax and GEMMs. It implements <strong>block
quantization and incoherent processing</strong> that leverages hardware support
for FP8 low-precision to achieve further speedup. FP8 FlashAttention-3
is also more accurate than a baseline FP8 attention by 2.6x, due to its
block quantization and incoherent processing, especially in cases with
outlier features. It uses primitives from CUTLASS, such as WGMMA and TMA
abstractions. Like FlashAttention and FlashAttention-2, it is also able
to handle multi-query attention (MQA) and grouped-query attention (GQA).</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="finetuning.html" class="btn btn-neutral float-left" title="6. Fine Tuning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="evaluation.html" class="btn btn-neutral float-right" title="8. LLM Evaluation Metrics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Wenqiang Feng, Di Zhen and Wenyun Wang.
      <span class="lastupdated">Last updated on Dec 30, 2024.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>